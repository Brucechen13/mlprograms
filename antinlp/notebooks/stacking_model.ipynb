{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenchi/anaconda3/envs/py27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2993060671581812978\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6431991067815918392\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13557213458687819206\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7648542720\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3199392792380803727\n",
      "physical_device_desc: \"device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tflearn.data_utils import pad_sequences\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras import initializers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import gc\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "VOCAB_LENGTH = 3000\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_cnn_output(input_x1, params):\n",
    "    # cnn0模块，kernel_size = 2\n",
    "    conv0_1 = Convolution1D(256, 2, padding='same')(input_x1)\n",
    "    bn0_1 = BatchNormalization()(conv0_1)\n",
    "    relu0_1 = Activation('relu')(bn0_1)\n",
    "    conv0_2 = Convolution1D(128, 2, padding='same')(relu0_1)\n",
    "    bn0_2 = BatchNormalization()(conv0_2)\n",
    "    relu0_2 = Activation('relu')(bn0_2)\n",
    "    cnn0 = MaxPool1D(pool_size=4)(relu0_2)\n",
    "    # cnn1模块，kernel_size = 3\n",
    "    conv1_1 = Convolution1D(256, 3, padding='same')(input_x1)\n",
    "    bn1_1 = BatchNormalization()(conv1_1)\n",
    "    relu1_1 = Activation('relu')(bn1_1)\n",
    "    conv1_2 = Convolution1D(128, 3, padding='same')(relu1_1)\n",
    "    bn1_2 = BatchNormalization()(conv1_2)\n",
    "    relu1_2 = Activation('relu')(bn1_2)\n",
    "    cnn1 = MaxPool1D(pool_size=4)(relu1_2)\n",
    "    # cnn2模块，kernel_size = 4\n",
    "    conv2_1 = Convolution1D(256, 4, padding='same')(input_x1)\n",
    "    bn2_1 = BatchNormalization()(conv2_1)\n",
    "    relu2_1 = Activation('relu')(bn2_1)\n",
    "    conv2_2 = Convolution1D(128, 4, padding='same')(relu2_1)\n",
    "    bn2_2 = BatchNormalization()(conv2_2)\n",
    "    relu2_2 = Activation('relu')(bn2_2)\n",
    "    cnn2 = MaxPool1D(pool_size=4)(relu2_2)\n",
    "    # cnn3模块，kernel_size = 5\n",
    "    conv3_1 = Convolution1D(256, 5, padding='same')(input_x1)\n",
    "    bn3_1 = BatchNormalization()(conv3_1)\n",
    "    relu3_1 = Activation('relu')(bn3_1)\n",
    "    conv3_2 = Convolution1D(128, 5, padding='same')(relu3_1)\n",
    "    bn3_2 = BatchNormalization()(conv3_2)\n",
    "    relu3_2 = Activation('relu')(bn3_2)\n",
    "    cnn3 = MaxPool1D(pool_size=4)(relu3_2)\n",
    "    # 拼接三个模块\n",
    "    cnn = concatenate([cnn0,cnn1,cnn2,cnn3], axis=-1)\n",
    "    return cnn\n",
    "\n",
    "def create_model_dssm(params):\n",
    "    # dssm是一个简单的双塔模型\n",
    "    input1 = keras.Input(shape=(MAX_SEQUENCE_LENGTH_WORD, ), dtype='int32')\n",
    "    input2 = keras.Input(shape=(MAX_SEQUENCE_LENGTH_WORD, ), dtype='int32')\n",
    "    input1c = keras.Input(shape=(MAX_SEQUENCE_LENGTH_CHAR, ), dtype='int32')\n",
    "    input2c = keras.Input(shape=(MAX_SEQUENCE_LENGTH_CHAR, ), dtype='int32')\n",
    "    \n",
    "    embedder = Embedding(5002, 300, input_length = MAX_SEQUENCE_LENGTH_WORD, \n",
    "                         weights = [embedding_matrix_word])# , mask_zero=True, trainable = False\n",
    "    # CuDNN\n",
    "    lstm0 = LSTM(params['num_lstm'],\n",
    "        dropout=params['lstm_dropout_rate'],\n",
    "        recurrent_dropout=params['lstm_dropout_rate'],\n",
    "                 return_sequences = True)\n",
    "    lstm1 = Bidirectional(LSTM(params['num_lstm'],\n",
    "        dropout=params['lstm_dropout_rate'],\n",
    "        recurrent_dropout=params['lstm_dropout_rate']))\n",
    "    lstm2 = LSTM(params['num_lstm'])\n",
    "    den = Dense(64,activation = 'tanh')\n",
    "\n",
    "    att1 = Lambda(lambda x: K.max(x,axis = 1))\n",
    "    v1 = embedder(input1)\n",
    "    v2 = embedder(input2)\n",
    "    v11 = lstm1(v1)\n",
    "    v22 = lstm1(v2)\n",
    "    v1ls = lstm2(lstm0(v1))\n",
    "    v2ls = lstm2(lstm0(v2))\n",
    "    v1 = Concatenate(axis=1)([att1(v1),v11])\n",
    "    v2 = Concatenate(axis=1)([att1(v2),v22])\n",
    "    \n",
    "    embedder = Embedding(2087, 300, input_length = MAX_SEQUENCE_LENGTH_CHAR, \n",
    "                         weights = [embedding_matrix_char])# , mask_zero=True, trainable = False\n",
    "    lstm1c = Bidirectional(LSTM(params['num_lstm']))\n",
    "    att1c = Lambda(lambda x: K.max(x,axis = 1))\n",
    "    v1c = embedder(input1c)\n",
    "    v2c = embedder(input2c)\n",
    "    v11c = lstm1c(v1c)\n",
    "    v22c = lstm1c(v2c)\n",
    "    v1c = Concatenate(axis=1)([att1c(v1c),v11c])\n",
    "    v2c = Concatenate(axis=1)([att1c(v2c),v22c])\n",
    "\n",
    "\n",
    "    mul = Multiply()([v1,v2])\n",
    "    sub = Lambda(lambda x: K.abs(x))(Subtract()([v1,v2]))\n",
    "    maximum = Maximum()([Multiply()([v1,v1]),Multiply()([v2,v2])])\n",
    "    mulc = Multiply()([v1c,v2c])\n",
    "    subc = Lambda(lambda x: K.abs(x))(Subtract()([v1c,v2c]))\n",
    "    maximumc = Maximum()([Multiply()([v1c,v1c]),Multiply()([v2c,v2c])])\n",
    "    sub2 = Lambda(lambda x: K.abs(x))(Subtract()([v1ls,v2ls]))\n",
    "    matchlist = Concatenate(axis=1)([mul,sub,mulc,subc,maximum,maximumc,sub2])\n",
    "    matchlist = Dropout(0.2)(matchlist)\n",
    "    \n",
    "\n",
    "    merged = Concatenate(axis=1)([Dense(32,activation = 'relu')(matchlist),\n",
    "                                     Dense(48,activation = 'sigmoid')(matchlist)])\n",
    "    \n",
    "    merged = keras.layers.Dense(params['num_dense'], activation='relu')(merged)\n",
    "    # merged = keras.layers.Dropout(params['dense_dropout_rate'])(merged)\n",
    "    merged = keras.layers.BatchNormalization()(merged)\n",
    "    res = Dense(1, activation = 'sigmoid')(merged)\n",
    "\n",
    "\n",
    "    model = keras.Model(inputs=[input1c, input2c, input1, input2], outputs=res)\n",
    "    model.compile(optimizer= keras.optimizers.Adam(lr = 0.001), loss=\"binary_crossentropy\", \n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "### CNN\n",
    "def create_model_cnn(params):\n",
    "    input1 = keras.Input(shape=(MAX_SEQUENCE_LENGTH_WORD, ), dtype='int32')\n",
    "    input2 = keras.Input(shape=(MAX_SEQUENCE_LENGTH_WORD, ), dtype='int32')\n",
    "    input1c = keras.Input(shape=(MAX_SEQUENCE_LENGTH_CHAR, ), dtype='int32')\n",
    "    input2c = keras.Input(shape=(MAX_SEQUENCE_LENGTH_CHAR, ), dtype='int32')\n",
    "    \n",
    "    embedder = Embedding(5002, 300, input_length = MAX_SEQUENCE_LENGTH_WORD, \n",
    "                         weights = [embedding_matrix_word], trainable = False)#mask_zero=True, \n",
    "    v1 = embedder(input1)\n",
    "    v2 = embedder(input2)\n",
    "\n",
    "    att1 = Lambda(lambda x: K.max(x,axis = 1))\n",
    "    \n",
    "    v11 = get_cnn_output(v1, params)\n",
    "    v22 = get_cnn_output(v2, params)\n",
    "    \n",
    "    v1 = Concatenate(axis=1)([att1(v1),att1(v11)])\n",
    "    v2 = Concatenate(axis=1)([att1(v2),att1(v22)])\n",
    "    \n",
    "    embedder = Embedding(2087, 300, input_length = MAX_SEQUENCE_LENGTH_CHAR, \n",
    "                         weights = [embedding_matrix_char], trainable = False)# , mask_zero=True\n",
    "    att1c = Lambda(lambda x: K.max(x,axis = 1))\n",
    "    v1c = embedder(input1c)\n",
    "    v2c = embedder(input2c)\n",
    "    v11c = get_cnn_output(v1c, params)\n",
    "    v22c = get_cnn_output(v2c, params)\n",
    "    v1c = Concatenate(axis=1)([att1c(v1c),att1(v11c)])\n",
    "    v2c = Concatenate(axis=1)([att1c(v2c),att1(v22c)])\n",
    "    \n",
    "    mul = Multiply()([v1,v2])\n",
    "    sub = Lambda(lambda x: K.abs(x))(Subtract()([v1,v2]))\n",
    "    maximum = Maximum()([Multiply()([v1,v1]),Multiply()([v2,v2])])\n",
    "    mulc = Multiply()([v1c,v2c])\n",
    "    subc = Lambda(lambda x: K.abs(x))(Subtract()([v1c,v2c]))\n",
    "    maximumc = Maximum()([Multiply()([v1c,v1c]),Multiply()([v2c,v2c])])\n",
    "    matchlist = Concatenate(axis=1)([mul,sub,mulc,subc,maximum,maximumc])\n",
    "    matchlist = Dropout(0.2)(matchlist)\n",
    "\n",
    "    merged = Concatenate(axis=1)([Dense(32,activation = 'relu')(matchlist),\n",
    "                                     Dense(48,activation = 'sigmoid')(matchlist)])\n",
    "    \n",
    "    merged = keras.layers.Dense(params['num_dense'], activation='relu')(merged)\n",
    "    merged = keras.layers.Dropout(params['dense_dropout_rate'])(merged)\n",
    "    merged = keras.layers.BatchNormalization()(merged)\n",
    "    res = Dense(1, activation = 'sigmoid')(merged)\n",
    "\n",
    "\n",
    "    model = keras.Model(inputs=[input1c, input2c, input1, input2], outputs=res)\n",
    "    model.compile(optimizer= keras.optimizers.Adam(lr = 0.001), loss=\"binary_crossentropy\", \n",
    "        metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_word = pd.read_csv('../data/aux/train_word_indexvec.csv')\n",
    "train_data_char = pd.read_csv('../data/aux/train_char_indexvec.csv')\n",
    "word_squence_ques1_char = list(train_data_char.iloc[:, 1])\n",
    "word_squence_ques1_char = [[int(im) for im in item.split(' ')] for item in word_squence_ques1_char]\n",
    "word_squence_ques2_char = list(train_data_char.iloc[:, 2])\n",
    "word_squence_ques2_char = [[int(im) for im in item.split(' ')] for item in word_squence_ques2_char]\n",
    "\n",
    "word_squence_ques1_word = list(train_data_word.iloc[:, 1])\n",
    "word_squence_ques1_word = [[int(im) for im in item.split(' ')] for item in word_squence_ques1_word]\n",
    "word_squence_ques2_word = list(train_data_word.iloc[:, 2])\n",
    "word_squence_ques2_word = [[int(im) for im in item.split(' ')] for item in word_squence_ques2_word]\n",
    "\n",
    "MAX_SEQUENCE_LENGTH_WORD = 50 # char 40 word 30\n",
    "MAX_SEQUENCE_LENGTH_CHAR = 60 # char 40 word 30\n",
    "word_squence_ques1_char_len = [len(item) for item in word_squence_ques1_char]\n",
    "word_squence_ques2_char_len = [len(item) for item in word_squence_ques2_char]\n",
    "word_squence_ques1_word_len = [len(item) for item in word_squence_ques1_word]\n",
    "word_squence_ques2_word_len = [len(item) for item in word_squence_ques2_word]\n",
    "word_squence_ques1_char = pad_sequences(word_squence_ques1_char, maxlen=MAX_SEQUENCE_LENGTH_CHAR)\n",
    "word_squence_ques2_char = pad_sequences(word_squence_ques2_char, maxlen=MAX_SEQUENCE_LENGTH_CHAR)\n",
    "word_squence_ques1_word = pad_sequences(word_squence_ques1_word, maxlen=MAX_SEQUENCE_LENGTH_WORD)\n",
    "word_squence_ques2_word = pad_sequences(word_squence_ques2_word, maxlen=MAX_SEQUENCE_LENGTH_WORD)\n",
    "\n",
    "\n",
    "\n",
    "embedding_matrix_char = np.load('../data/aux/vec_char.npy')\n",
    "\n",
    "embedding_matrix_word = np.load('../data/aux/vec_word.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98976, 2)\n",
      "(98976, 16)\n",
      "(98976, 5)\n",
      "(98976, 2)\n",
      "(98976, 16)\n",
      "(98976, 5)\n",
      "(98976, 4)\n",
      "(98976, 27)\n"
     ]
    }
   ],
   "source": [
    "lda_feas_char = pd.read_csv('../lda_features_char.csv')\n",
    "lda_feas_char = lda_feas_char.values\n",
    "print(lda_feas_char.shape)\n",
    "ngram_feas_char = pd.read_csv('../ngram_features_char.csv')\n",
    "ngram_feas_char = ngram_feas_char.values\n",
    "print(ngram_feas_char.shape)\n",
    "simsummary_feas_char = pd.read_csv('../simsummary_features_char.csv')\n",
    "simsummary_feas_char = simsummary_feas_char.values\n",
    "print(simsummary_feas_char.shape)\n",
    "lda_feas_word = pd.read_csv('../lda_features_word.csv')\n",
    "lda_feas_word = lda_feas_word.values\n",
    "print(lda_feas_word.shape)\n",
    "ngram_feas_word = pd.read_csv('../ngram_features_word.csv')\n",
    "ngram_feas_word = ngram_feas_word.values\n",
    "print(ngram_feas_word.shape)\n",
    "simsummary_feas_word = pd.read_csv('../simsummary_features_word.csv')\n",
    "simsummary_feas_word = simsummary_feas_word.values\n",
    "print(simsummary_feas_word.shape)\n",
    "tfidf_feas = pd.read_csv('../tfidf_features.csv')\n",
    "tfidf_feas = tfidf_feas.values\n",
    "print(tfidf_feas.shape)\n",
    "all_feas_char = np.concatenate([ngram_feas_char,lda_feas_char,simsummary_feas_char], axis=1)\n",
    "all_feas_word = np.concatenate([ngram_feas_word, lda_feas_word, simsummary_feas_word], axis=1)\n",
    "# , tfidf_feas\n",
    "all_feas = np.concatenate([all_feas_word, tfidf_feas], axis=1)\n",
    "print all_feas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenchi/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  0.37521274949713757\n",
      "weights:  0.3723700495049505\n",
      "weights:  0.3766884200940963\n",
      "weights:  0.37189891027127286\n",
      "weights:  0.3794212218649518\n",
      "weights:  0.3489984004874705\n",
      "weights:  0.3496300816108611\n",
      "weights:  0.3490580487057742\n",
      "weights:  0.3436352509179927\n",
      "weights:  0.35096774193548386\n",
      "weights:  0.3974818571303663\n",
      "weights:  0.40378051982147545\n",
      "weights:  0.40455740578439964\n",
      "weights:  0.4006353688669255\n",
      "weights:  0.4038128249566724\n",
      "weights:  0.3370007535795026\n",
      "weights:  0.3399484926526284\n",
      "weights:  0.33963396339633967\n",
      "weights:  0.3385537654552267\n",
      "weights:  0.33866705813270703\n",
      "weights:  0.33136719781670926\n",
      "weights:  0.332860184528034\n",
      "weights:  0.33968502566779774\n",
      "weights:  0.32959463039830433\n",
      "weights:  0.33044695180617506\n",
      "weights:  0.33928943892094837\n",
      "weights:  0.3360961458765023\n",
      "weights:  0.34210158108297184\n",
      "weights:  0.3383160675797817\n",
      "weights:  0.34126603113641035\n",
      "weights:  0.3759075418580531\n",
      "weights:  0.38115277673404974\n",
      "weights:  0.3849384717412129\n",
      "weights:  0.37946001618480096\n",
      "weights:  0.3806633106307374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n",
    "\n",
    "class Metrics(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[:-3])))#\n",
    "        val_predict = [1 if item > 0.5 else 0 for item in val_predict]\n",
    "        val_targ = self.validation_data[-3]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        print \"epoch end, f1-score: \", _val_f1\n",
    "metrics = Metrics()\n",
    "class_weight = {0: 1.,\n",
    "                1: 5.,}\n",
    "\n",
    "y_label = train_data_char.iloc[:, 3]\n",
    "\n",
    "def stacking_model(X_train, X_val, y_train, y_val):\n",
    "    clf = LogisticRegression(class_weight={0: 1.,1: 5,})\n",
    "    clf.fit(X_train, y_train)\n",
    "    f1 = f1_score(y_val, clf.predict(X_val))\n",
    "    print 'weights: ', f1\n",
    "    return clf.predict_proba(X_val)[:, 1].reshape(-1,1)\n",
    "            \n",
    "\n",
    "NUM_FOLDS = 5\n",
    "kfold = StratifiedKFold(\n",
    "    y_label,\n",
    "    n_folds=NUM_FOLDS,\n",
    "    shuffle=True,\n",
    "    random_state=30\n",
    ")\n",
    "\n",
    "stacking_res = []\n",
    "\n",
    "for X in [ngram_feas_char,lda_feas_char,simsummary_feas_char, \n",
    "        ngram_feas_word, lda_feas_word, simsummary_feas_word,\n",
    "        tfidf_feas]:\n",
    "    train_oofp = np.zeros((X.shape[0], 1))\n",
    "    for fold_num, (ix_train, ix_val) in enumerate(kfold):\n",
    "        X_train = X[ix_train]\n",
    "        X_val = X[ix_val]\n",
    "        y_train = y_label[ix_train]\n",
    "        y_val = y_label[ix_val]\n",
    "        train_oofp[ix_val] = (stacking_model(X_train, X_val, y_train, y_val))\n",
    "    stacking_res.append(train_oofp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 158360 samples, validate on 19796 samples\n",
      "Epoch 1/100\n",
      "158360/158360 [==============================] - 629s 4ms/step - loss: 0.9194 - acc: 0.7065 - val_loss: 0.3887 - val_acc: 0.8315\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38865, saving model to ../data/checkpoints/stack/0create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.5031277926720286\n",
      "Epoch 2/100\n",
      "158360/158360 [==============================] - 618s 4ms/step - loss: 0.6441 - acc: 0.8195 - val_loss: 0.5671 - val_acc: 0.7384\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.5009154861713405\n",
      "Epoch 3/100\n",
      "158360/158360 [==============================] - 615s 4ms/step - loss: 0.4257 - acc: 0.8873 - val_loss: 0.4722 - val_acc: 0.8238\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.49280418665503706\n",
      "Epoch 4/100\n",
      "158360/158360 [==============================] - 620s 4ms/step - loss: 0.3045 - acc: 0.9221 - val_loss: 0.7218 - val_acc: 0.7410\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.5039659508609016\n",
      "Epoch 5/100\n",
      "158360/158360 [==============================] - 621s 4ms/step - loss: 0.2404 - acc: 0.9402 - val_loss: 0.7608 - val_acc: 0.7523\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.5029393877964727\n",
      "Epoch 6/100\n",
      "158360/158360 [==============================] - 523s 3ms/step - loss: 0.1918 - acc: 0.9530 - val_loss: 0.6356 - val_acc: 0.8099\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.4927887855506133\n",
      "Epoch 7/100\n",
      "158360/158360 [==============================] - 531s 3ms/step - loss: 0.1266 - acc: 0.9693 - val_loss: 0.6465 - val_acc: 0.8190\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.49364227182820003\n",
      "Epoch 8/100\n",
      "158360/158360 [==============================] - 527s 3ms/step - loss: 0.1025 - acc: 0.9760 - val_loss: 0.6806 - val_acc: 0.8201\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.4893922018348623\n",
      "Epoch 9/100\n",
      "158360/158360 [==============================] - 528s 3ms/step - loss: 0.0855 - acc: 0.9800 - val_loss: 0.7390 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.49657027572293205\n",
      "Epoch 10/100\n",
      "158360/158360 [==============================] - 532s 3ms/step - loss: 0.0781 - acc: 0.9817 - val_loss: 0.7613 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.4987104655897923\n",
      "Epoch 11/100\n",
      "158360/158360 [==============================] - 531s 3ms/step - loss: 0.0715 - acc: 0.9836 - val_loss: 0.7924 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.4996658200775297\n",
      "Epoch 12/100\n",
      "158360/158360 [==============================] - 529s 3ms/step - loss: 0.0641 - acc: 0.9849 - val_loss: 0.7632 - val_acc: 0.8227\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.4877408056042032\n",
      "Epoch 13/100\n",
      "158360/158360 [==============================] - 530s 3ms/step - loss: 0.0629 - acc: 0.9860 - val_loss: 0.7780 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.4952462040584646\n",
      "Epoch 14/100\n",
      "158360/158360 [==============================] - 528s 3ms/step - loss: 0.0609 - acc: 0.9861 - val_loss: 0.7812 - val_acc: 0.8189\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.4945023963913166\n",
      "Epoch 15/100\n",
      "158360/158360 [==============================] - 533s 3ms/step - loss: 0.0603 - acc: 0.9865 - val_loss: 0.7746 - val_acc: 0.8216\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.4891524443158809\n",
      "Epoch 16/100\n",
      "158360/158360 [==============================] - 532s 3ms/step - loss: 0.0602 - acc: 0.9864 - val_loss: 0.7795 - val_acc: 0.8223\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.4875418913011802\n",
      "Epoch 17/100\n",
      "158360/158360 [==============================] - 531s 3ms/step - loss: 0.0600 - acc: 0.9867 - val_loss: 0.7796 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.48979000724112964\n",
      "Epoch 18/100\n",
      "158360/158360 [==============================] - 534s 3ms/step - loss: 0.0590 - acc: 0.9868 - val_loss: 0.7788 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.48839907192575405\n",
      "Epoch 19/100\n",
      "158360/158360 [==============================] - 531s 3ms/step - loss: 0.0598 - acc: 0.9868 - val_loss: 0.7830 - val_acc: 0.8215\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.49143515186411396\n",
      "Epoch 20/100\n",
      "158360/158360 [==============================] - 530s 3ms/step - loss: 0.0590 - acc: 0.9869 - val_loss: 0.7805 - val_acc: 0.8215\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.4895246351683282\n",
      "Epoch 21/100\n",
      "158360/158360 [==============================] - 521s 3ms/step - loss: 0.0595 - acc: 0.9866 - val_loss: 0.7807 - val_acc: 0.8212\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.38865\n",
      "epoch end, f1-score:  0.49071808893365954\n",
      "Epoch 00021: early stopping\n",
      "Train on 158362 samples, validate on 19795 samples\n",
      "Epoch 1/100\n",
      "158362/158362 [==============================] - 518s 3ms/step - loss: 0.9209 - acc: 0.7064 - val_loss: 0.3965 - val_acc: 0.8411\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.39651, saving model to ../data/checkpoints/stack/1create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.36193954148914587\n",
      "Epoch 2/100\n",
      "158362/158362 [==============================] - 510s 3ms/step - loss: 0.6387 - acc: 0.8196 - val_loss: 0.4635 - val_acc: 0.8030\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.5350029815146095\n",
      "Epoch 3/100\n",
      "158362/158362 [==============================] - 514s 3ms/step - loss: 0.4200 - acc: 0.8882 - val_loss: 0.7119 - val_acc: 0.7201\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.5007209805335257\n",
      "Epoch 4/100\n",
      "158362/158362 [==============================] - 508s 3ms/step - loss: 0.3010 - acc: 0.9232 - val_loss: 0.5439 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.4949523275378575\n",
      "Epoch 5/100\n",
      "158362/158362 [==============================] - 507s 3ms/step - loss: 0.2346 - acc: 0.9410 - val_loss: 0.6495 - val_acc: 0.7988\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.5220208808352333\n",
      "Epoch 6/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.1971 - acc: 0.9518 - val_loss: 0.6459 - val_acc: 0.8006\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.5055110220440882\n",
      "Epoch 7/100\n",
      "158362/158362 [==============================] - 507s 3ms/step - loss: 0.1266 - acc: 0.9695 - val_loss: 0.6497 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.4951666426201125\n",
      "Epoch 8/100\n",
      "158362/158362 [==============================] - 511s 3ms/step - loss: 0.1007 - acc: 0.9762 - val_loss: 0.6998 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.5064864864864865\n",
      "Epoch 9/100\n",
      "158362/158362 [==============================] - 508s 3ms/step - loss: 0.0854 - acc: 0.9801 - val_loss: 0.6980 - val_acc: 0.8295\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.4778843179709248\n",
      "Epoch 10/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.0770 - acc: 0.9823 - val_loss: 0.7519 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.4910740415569213\n",
      "Epoch 11/100\n",
      "158362/158362 [==============================] - 509s 3ms/step - loss: 0.0674 - acc: 0.9844 - val_loss: 0.7730 - val_acc: 0.8259\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.48055471811878203\n",
      "Epoch 12/100\n",
      "158362/158362 [==============================] - 504s 3ms/step - loss: 0.0610 - acc: 0.9857 - val_loss: 0.7713 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.48832397280520246\n",
      "Epoch 13/100\n",
      "158362/158362 [==============================] - 507s 3ms/step - loss: 0.0619 - acc: 0.9862 - val_loss: 0.7787 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.39651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch end, f1-score:  0.49458327314747935\n",
      "Epoch 14/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.0591 - acc: 0.9866 - val_loss: 0.7826 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.49134545454545453\n",
      "Epoch 15/100\n",
      "158362/158362 [==============================] - 508s 3ms/step - loss: 0.0605 - acc: 0.9863 - val_loss: 0.7853 - val_acc: 0.8236\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.4908136482939633\n",
      "Epoch 16/100\n",
      "158362/158362 [==============================] - 507s 3ms/step - loss: 0.0582 - acc: 0.9869 - val_loss: 0.7892 - val_acc: 0.8226\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.49263218722912455\n",
      "Epoch 17/100\n",
      "158362/158362 [==============================] - 507s 3ms/step - loss: 0.0567 - acc: 0.9870 - val_loss: 0.7877 - val_acc: 0.8234\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.49252214316828813\n",
      "Epoch 18/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.0579 - acc: 0.9869 - val_loss: 0.7883 - val_acc: 0.8236\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.4899211218229623\n",
      "Epoch 19/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.0560 - acc: 0.9868 - val_loss: 0.7892 - val_acc: 0.8238\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.4895330112721417\n",
      "Epoch 20/100\n",
      "158362/158362 [==============================] - 508s 3ms/step - loss: 0.0557 - acc: 0.9873 - val_loss: 0.7879 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.48967032967032964\n",
      "Epoch 21/100\n",
      "158362/158362 [==============================] - 504s 3ms/step - loss: 0.0560 - acc: 0.9872 - val_loss: 0.7903 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.39651\n",
      "epoch end, f1-score:  0.48929932571093526\n",
      "Epoch 00021: early stopping\n",
      "Train on 158362 samples, validate on 19795 samples\n",
      "Epoch 1/100\n",
      "158362/158362 [==============================] - 514s 3ms/step - loss: 0.9211 - acc: 0.7048 - val_loss: 0.4421 - val_acc: 0.7941\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44209, saving model to ../data/checkpoints/stack/2create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.5390792896731139\n",
      "Epoch 2/100\n",
      "158362/158362 [==============================] - 506s 3ms/step - loss: 0.6420 - acc: 0.8200 - val_loss: 0.4399 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44209 to 0.43992, saving model to ../data/checkpoints/stack/2create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.5315521628498727\n",
      "Epoch 3/100\n",
      "158362/158362 [==============================] - 503s 3ms/step - loss: 0.4242 - acc: 0.8880 - val_loss: 0.4887 - val_acc: 0.8286\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.4964370546318289\n",
      "Epoch 4/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.3035 - acc: 0.9224 - val_loss: 0.5876 - val_acc: 0.7881\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.5127192472993379\n",
      "Epoch 5/100\n",
      "158362/158362 [==============================] - 503s 3ms/step - loss: 0.2375 - acc: 0.9402 - val_loss: 0.5964 - val_acc: 0.8196\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.5031306525671351\n",
      "Epoch 6/100\n",
      "158362/158362 [==============================] - 504s 3ms/step - loss: 0.1996 - acc: 0.9503 - val_loss: 0.5963 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.4946594534609516\n",
      "Epoch 7/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.1669 - acc: 0.9588 - val_loss: 0.7290 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.38956266078647556\n",
      "Epoch 8/100\n",
      "158362/158362 [==============================] - 506s 3ms/step - loss: 0.1124 - acc: 0.9744 - val_loss: 0.6894 - val_acc: 0.8207\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.49950641658440276\n",
      "Epoch 9/100\n",
      "158362/158362 [==============================] - 506s 3ms/step - loss: 0.0859 - acc: 0.9800 - val_loss: 0.6919 - val_acc: 0.8261\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.48680876434640036\n",
      "Epoch 10/100\n",
      "158362/158362 [==============================] - 508s 3ms/step - loss: 0.0736 - acc: 0.9834 - val_loss: 0.7523 - val_acc: 0.8163\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.5010289477294553\n",
      "Epoch 11/100\n",
      "158362/158362 [==============================] - 506s 3ms/step - loss: 0.0660 - acc: 0.9845 - val_loss: 0.7654 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.49575551782682514\n",
      "Epoch 12/100\n",
      "158362/158362 [==============================] - 508s 3ms/step - loss: 0.0623 - acc: 0.9856 - val_loss: 0.7738 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.49076517150395776\n",
      "Epoch 13/100\n",
      "158362/158362 [==============================] - 504s 3ms/step - loss: 0.0540 - acc: 0.9879 - val_loss: 0.7770 - val_acc: 0.8254\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.4912435614422369\n",
      "Epoch 14/100\n",
      "158362/158362 [==============================] - 507s 3ms/step - loss: 0.0543 - acc: 0.9877 - val_loss: 0.7848 - val_acc: 0.8224\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.49460819554277496\n",
      "Epoch 15/100\n",
      "158362/158362 [==============================] - 509s 3ms/step - loss: 0.0516 - acc: 0.9886 - val_loss: 0.7876 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.4926985981308411\n",
      "Epoch 16/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.0523 - acc: 0.9884 - val_loss: 0.7966 - val_acc: 0.8219\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.4969316397887826\n",
      "Epoch 17/100\n",
      "158362/158362 [==============================] - 507s 3ms/step - loss: 0.0512 - acc: 0.9884 - val_loss: 0.7933 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.49524084222670894\n",
      "Epoch 18/100\n",
      "158362/158362 [==============================] - 508s 3ms/step - loss: 0.0506 - acc: 0.9884 - val_loss: 0.7979 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.4945118428653957\n",
      "Epoch 19/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.0508 - acc: 0.9884 - val_loss: 0.7966 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.4939950803067574\n",
      "Epoch 20/100\n",
      "158362/158362 [==============================] - 509s 3ms/step - loss: 0.0502 - acc: 0.9889 - val_loss: 0.7942 - val_acc: 0.8242\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.49337410805300713\n",
      "Epoch 21/100\n",
      "158362/158362 [==============================] - 507s 3ms/step - loss: 0.0493 - acc: 0.9890 - val_loss: 0.7922 - val_acc: 0.8244\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.49358974358974356\n",
      "Epoch 22/100\n",
      "158362/158362 [==============================] - 504s 3ms/step - loss: 0.0512 - acc: 0.9886 - val_loss: 0.7970 - val_acc: 0.8234\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.43992\n",
      "epoch end, f1-score:  0.49377353026353893\n",
      "Epoch 00022: early stopping\n",
      "Train on 158362 samples, validate on 19795 samples\n",
      "Epoch 1/100\n",
      "158362/158362 [==============================] - 520s 3ms/step - loss: 0.9187 - acc: 0.7044 - val_loss: 0.4435 - val_acc: 0.7991\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44351, saving model to ../data/checkpoints/stack/3create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.5502657469184666\n",
      "Epoch 2/100\n",
      "158362/158362 [==============================] - 506s 3ms/step - loss: 0.6468 - acc: 0.8167 - val_loss: 0.6356 - val_acc: 0.7251\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5087560931576097\n",
      "Epoch 3/100\n",
      "158362/158362 [==============================] - 506s 3ms/step - loss: 0.4230 - acc: 0.8860 - val_loss: 0.4982 - val_acc: 0.8034\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5326609029779058\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158362/158362 [==============================] - 504s 3ms/step - loss: 0.3063 - acc: 0.9208 - val_loss: 0.5381 - val_acc: 0.8336\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.4896947156361382\n",
      "Epoch 5/100\n",
      "158362/158362 [==============================] - 507s 3ms/step - loss: 0.2377 - acc: 0.9397 - val_loss: 0.6368 - val_acc: 0.8024\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5254216721271691\n",
      "Epoch 6/100\n",
      "158362/158362 [==============================] - 504s 3ms/step - loss: 0.1986 - acc: 0.9504 - val_loss: 0.6374 - val_acc: 0.8190\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5219479653102067\n",
      "Epoch 7/100\n",
      "158362/158362 [==============================] - 503s 3ms/step - loss: 0.1315 - acc: 0.9674 - val_loss: 0.6234 - val_acc: 0.8306\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.49594229035166815\n",
      "Epoch 8/100\n",
      "158362/158362 [==============================] - 501s 3ms/step - loss: 0.1014 - acc: 0.9759 - val_loss: 0.6980 - val_acc: 0.8185\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5222709746044409\n",
      "Epoch 9/100\n",
      "158362/158362 [==============================] - 503s 3ms/step - loss: 0.0880 - acc: 0.9791 - val_loss: 0.7190 - val_acc: 0.8207\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5240713423628806\n",
      "Epoch 10/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.0785 - acc: 0.9812 - val_loss: 0.7525 - val_acc: 0.8206\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5224522721161602\n",
      "Epoch 11/100\n",
      "158362/158362 [==============================] - 506s 3ms/step - loss: 0.0706 - acc: 0.9834 - val_loss: 0.7419 - val_acc: 0.8309\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.48986434994665445\n",
      "Epoch 12/100\n",
      "158362/158362 [==============================] - 503s 3ms/step - loss: 0.0637 - acc: 0.9850 - val_loss: 0.7521 - val_acc: 0.8271\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5046323103647944\n",
      "Epoch 13/100\n",
      "158362/158362 [==============================] - 502s 3ms/step - loss: 0.0620 - acc: 0.9857 - val_loss: 0.7576 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5069124423963134\n",
      "Epoch 14/100\n",
      "158362/158362 [==============================] - 504s 3ms/step - loss: 0.0629 - acc: 0.9855 - val_loss: 0.7633 - val_acc: 0.8253\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5057175528873642\n",
      "Epoch 15/100\n",
      "158362/158362 [==============================] - 501s 3ms/step - loss: 0.0599 - acc: 0.9862 - val_loss: 0.7674 - val_acc: 0.8253\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5082480091012515\n",
      "Epoch 16/100\n",
      "158362/158362 [==============================] - 504s 3ms/step - loss: 0.0582 - acc: 0.9865 - val_loss: 0.7679 - val_acc: 0.8266\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5007997673404101\n",
      "Epoch 17/100\n",
      "158362/158362 [==============================] - 502s 3ms/step - loss: 0.0593 - acc: 0.9864 - val_loss: 0.7719 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5020289855072465\n",
      "Epoch 18/100\n",
      "158362/158362 [==============================] - 503s 3ms/step - loss: 0.0588 - acc: 0.9866 - val_loss: 0.7725 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5036137612026598\n",
      "Epoch 19/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.0615 - acc: 0.9864 - val_loss: 0.7740 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5028935185185185\n",
      "Epoch 20/100\n",
      "158362/158362 [==============================] - 502s 3ms/step - loss: 0.0595 - acc: 0.9863 - val_loss: 0.7704 - val_acc: 0.8266\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5014526438117375\n",
      "Epoch 21/100\n",
      "158362/158362 [==============================] - 504s 3ms/step - loss: 0.0573 - acc: 0.9871 - val_loss: 0.7737 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.44351\n",
      "epoch end, f1-score:  0.5028935185185185\n",
      "Epoch 00021: early stopping\n",
      "Train on 158362 samples, validate on 19795 samples\n",
      "Epoch 1/100\n",
      "158362/158362 [==============================] - 514s 3ms/step - loss: 0.9204 - acc: 0.7049 - val_loss: 0.4256 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42561, saving model to ../data/checkpoints/stack/4create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.5378091872791518\n",
      "Epoch 2/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.6424 - acc: 0.8191 - val_loss: 0.5027 - val_acc: 0.7707\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.5297834869988606\n",
      "Epoch 3/100\n",
      "158362/158362 [==============================] - 506s 3ms/step - loss: 0.4213 - acc: 0.8886 - val_loss: 0.5365 - val_acc: 0.7827\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.5273106934828002\n",
      "Epoch 4/100\n",
      "158362/158362 [==============================] - 504s 3ms/step - loss: 0.3040 - acc: 0.9229 - val_loss: 0.5219 - val_acc: 0.8332\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.4721867007672635\n",
      "Epoch 5/100\n",
      "158362/158362 [==============================] - 507s 3ms/step - loss: 0.2385 - acc: 0.9404 - val_loss: 0.9253 - val_acc: 0.6961\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.48817423855708697\n",
      "Epoch 6/100\n",
      "158362/158362 [==============================] - 500s 3ms/step - loss: 0.1943 - acc: 0.9518 - val_loss: 0.6236 - val_acc: 0.8177\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.5125641718454472\n",
      "Epoch 7/100\n",
      "158362/158362 [==============================] - 504s 3ms/step - loss: 0.1294 - acc: 0.9685 - val_loss: 0.6439 - val_acc: 0.8305\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.49058894960534305\n",
      "Epoch 8/100\n",
      "158362/158362 [==============================] - 504s 3ms/step - loss: 0.0969 - acc: 0.9773 - val_loss: 0.7276 - val_acc: 0.8122\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.5145618388402767\n",
      "Epoch 9/100\n",
      "158362/158362 [==============================] - 502s 3ms/step - loss: 0.0846 - acc: 0.9803 - val_loss: 0.7122 - val_acc: 0.8206\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.5097985095224953\n",
      "Epoch 10/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.0779 - acc: 0.9817 - val_loss: 0.7181 - val_acc: 0.8257\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.4998550304436069\n",
      "Epoch 11/100\n",
      "158362/158362 [==============================] - 500s 3ms/step - loss: 0.0684 - acc: 0.9838 - val_loss: 0.7430 - val_acc: 0.8234\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.502419584400797\n",
      "Epoch 12/100\n",
      "158362/158362 [==============================] - 507s 3ms/step - loss: 0.0638 - acc: 0.9852 - val_loss: 0.7485 - val_acc: 0.8249\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.49253294289897515\n",
      "Epoch 13/100\n",
      "158362/158362 [==============================] - 501s 3ms/step - loss: 0.0612 - acc: 0.9857 - val_loss: 0.7494 - val_acc: 0.8263\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.49306989088764375\n",
      "Epoch 14/100\n",
      "158362/158362 [==============================] - 508s 3ms/step - loss: 0.0626 - acc: 0.9854 - val_loss: 0.7484 - val_acc: 0.8271\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.4864945978391356\n",
      "Epoch 15/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.0609 - acc: 0.9861 - val_loss: 0.7571 - val_acc: 0.8258\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.4956114686951434\n",
      "Epoch 16/100\n",
      "158362/158362 [==============================] - 503s 3ms/step - loss: 0.0601 - acc: 0.9866 - val_loss: 0.7595 - val_acc: 0.8246\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.49549549549549554\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158362/158362 [==============================] - 503s 3ms/step - loss: 0.0592 - acc: 0.9860 - val_loss: 0.7572 - val_acc: 0.8264\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.4961149391584812\n",
      "Epoch 18/100\n",
      "158362/158362 [==============================] - 505s 3ms/step - loss: 0.0594 - acc: 0.9859 - val_loss: 0.7571 - val_acc: 0.8269\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.494169741697417\n",
      "Epoch 19/100\n",
      "158362/158362 [==============================] - 499s 3ms/step - loss: 0.0582 - acc: 0.9866 - val_loss: 0.7572 - val_acc: 0.8268\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.4954371504268473\n",
      "Epoch 20/100\n",
      "158362/158362 [==============================] - 506s 3ms/step - loss: 0.0584 - acc: 0.9870 - val_loss: 0.7567 - val_acc: 0.8271\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.4953560371517028\n",
      "Epoch 21/100\n",
      "158362/158362 [==============================] - 501s 3ms/step - loss: 0.0584 - acc: 0.9867 - val_loss: 0.7592 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.42561\n",
      "epoch end, f1-score:  0.49655576725780454\n",
      "Epoch 00021: early stopping\n",
      "Train on 158360 samples, validate on 19796 samples\n",
      "Epoch 1/100\n",
      "158360/158360 [==============================] - 106s 669us/step - loss: 1.2101 - acc: 0.4752 - val_loss: 0.6840 - val_acc: 0.5732\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68400, saving model to ../data/checkpoints/stack/0create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.3621262458471761\n",
      "Epoch 2/100\n",
      "158360/158360 [==============================] - 96s 608us/step - loss: 1.0957 - acc: 0.5909 - val_loss: 0.6283 - val_acc: 0.6446\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.68400 to 0.62831, saving model to ../data/checkpoints/stack/0create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.4216065115514265\n",
      "Epoch 3/100\n",
      "158360/158360 [==============================] - 97s 613us/step - loss: 1.0026 - acc: 0.6567 - val_loss: 0.6865 - val_acc: 0.5790\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62831\n",
      "epoch end, f1-score:  0.42122074855912783\n",
      "Epoch 4/100\n",
      "158360/158360 [==============================] - 96s 609us/step - loss: 0.9419 - acc: 0.6858 - val_loss: 0.6631 - val_acc: 0.6266\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62831\n",
      "epoch end, f1-score:  0.44387601564851037\n",
      "Epoch 5/100\n",
      "158360/158360 [==============================] - 96s 609us/step - loss: 0.8931 - acc: 0.7057 - val_loss: 0.5752 - val_acc: 0.6768\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62831 to 0.57522, saving model to ../data/checkpoints/stack/0create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.4679860302677532\n",
      "Epoch 6/100\n",
      "158360/158360 [==============================] - 97s 610us/step - loss: 0.8513 - acc: 0.7216 - val_loss: 0.7028 - val_acc: 0.6022\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.57522\n",
      "epoch end, f1-score:  0.44463252927070107\n",
      "Epoch 7/100\n",
      "158360/158360 [==============================] - 97s 610us/step - loss: 0.8131 - acc: 0.7375 - val_loss: 0.5085 - val_acc: 0.7366\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.57522 to 0.50846, saving model to ../data/checkpoints/stack/0create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.49865384615384617\n",
      "Epoch 8/100\n",
      "158360/158360 [==============================] - 96s 606us/step - loss: 0.7703 - acc: 0.7491 - val_loss: 0.6062 - val_acc: 0.6732\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.50846\n",
      "epoch end, f1-score:  0.4723536127874735\n",
      "Epoch 9/100\n",
      "158360/158360 [==============================] - 96s 607us/step - loss: 0.7268 - acc: 0.7686 - val_loss: 0.4285 - val_acc: 0.7981\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50846 to 0.42848, saving model to ../data/checkpoints/stack/0create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.4974849094567405\n",
      "Epoch 10/100\n",
      "158360/158360 [==============================] - 96s 606us/step - loss: 0.6854 - acc: 0.7806 - val_loss: 0.5416 - val_acc: 0.7196\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.48960823983814605\n",
      "Epoch 11/100\n",
      "158360/158360 [==============================] - 96s 606us/step - loss: 0.6399 - acc: 0.7979 - val_loss: 0.4349 - val_acc: 0.8128\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5004718889038695\n",
      "Epoch 12/100\n",
      "158360/158360 [==============================] - 96s 608us/step - loss: 0.5879 - acc: 0.8170 - val_loss: 0.4857 - val_acc: 0.7659\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5091602245049244\n",
      "Epoch 13/100\n",
      "158360/158360 [==============================] - 96s 608us/step - loss: 0.5491 - acc: 0.8312 - val_loss: 0.5826 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.4999514232973866\n",
      "Epoch 14/100\n",
      "158360/158360 [==============================] - 96s 606us/step - loss: 0.5070 - acc: 0.8474 - val_loss: 0.5414 - val_acc: 0.7784\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5051895306859207\n",
      "Epoch 15/100\n",
      "158360/158360 [==============================] - 96s 606us/step - loss: 0.3772 - acc: 0.8912 - val_loss: 0.5314 - val_acc: 0.7999\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5095320623916811\n",
      "Epoch 16/100\n",
      "158360/158360 [==============================] - 96s 607us/step - loss: 0.3131 - acc: 0.9134 - val_loss: 0.5749 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5122596153846154\n",
      "Epoch 17/100\n",
      "158360/158360 [==============================] - 96s 609us/step - loss: 0.2772 - acc: 0.9249 - val_loss: 0.5834 - val_acc: 0.8070\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.510316544918621\n",
      "Epoch 18/100\n",
      "158360/158360 [==============================] - 96s 609us/step - loss: 0.2556 - acc: 0.9317 - val_loss: 0.6033 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5128991060025542\n",
      "Epoch 19/100\n",
      "158360/158360 [==============================] - 96s 605us/step - loss: 0.2332 - acc: 0.9383 - val_loss: 0.6424 - val_acc: 0.7992\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5134043334557473\n",
      "Epoch 20/100\n",
      "158360/158360 [==============================] - 96s 606us/step - loss: 0.2099 - acc: 0.9452 - val_loss: 0.6361 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.508384349214799\n",
      "Epoch 21/100\n",
      "158360/158360 [==============================] - 96s 607us/step - loss: 0.2082 - acc: 0.9461 - val_loss: 0.6388 - val_acc: 0.8154\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5075471698113209\n",
      "Epoch 22/100\n",
      "158360/158360 [==============================] - 96s 609us/step - loss: 0.2043 - acc: 0.9477 - val_loss: 0.6439 - val_acc: 0.8126\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5089368462862438\n",
      "Epoch 23/100\n",
      "158360/158360 [==============================] - 96s 609us/step - loss: 0.2021 - acc: 0.9475 - val_loss: 0.6442 - val_acc: 0.8149\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5084518379393616\n",
      "Epoch 24/100\n",
      "158360/158360 [==============================] - 96s 608us/step - loss: 0.1983 - acc: 0.9487 - val_loss: 0.6478 - val_acc: 0.8154\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5084745762711865\n",
      "Epoch 25/100\n",
      "158360/158360 [==============================] - 96s 603us/step - loss: 0.1958 - acc: 0.9500 - val_loss: 0.6493 - val_acc: 0.8148\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5081153588195841\n",
      "Epoch 26/100\n",
      "158360/158360 [==============================] - 96s 607us/step - loss: 0.1952 - acc: 0.9498 - val_loss: 0.6498 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5084791386271871\n",
      "Epoch 27/100\n",
      "158360/158360 [==============================] - 97s 611us/step - loss: 0.1944 - acc: 0.9502 - val_loss: 0.6482 - val_acc: 0.8154\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5086067778375472\n",
      "Epoch 28/100\n",
      "158360/158360 [==============================] - 97s 609us/step - loss: 0.1963 - acc: 0.9493 - val_loss: 0.6511 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5087414920592552\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158360/158360 [==============================] - 96s 609us/step - loss: 0.1967 - acc: 0.9496 - val_loss: 0.6496 - val_acc: 0.8148\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.42848\n",
      "epoch end, f1-score:  0.5083154506437769\n",
      "Epoch 00029: early stopping\n",
      "Train on 158362 samples, validate on 19795 samples\n",
      "Epoch 1/100\n",
      "158362/158362 [==============================] - 106s 671us/step - loss: 1.2031 - acc: 0.5347 - val_loss: 0.6802 - val_acc: 0.5887\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68023, saving model to ../data/checkpoints/stack/1create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.3730458221024259\n",
      "Epoch 2/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 1.0858 - acc: 0.6346 - val_loss: 0.8299 - val_acc: 0.4303\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.68023\n",
      "epoch end, f1-score:  0.36982397317686505\n",
      "Epoch 3/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.9974 - acc: 0.6765 - val_loss: 0.8314 - val_acc: 0.4693\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.68023\n",
      "epoch end, f1-score:  0.38968281631230395\n",
      "Epoch 4/100\n",
      "158362/158362 [==============================] - 96s 605us/step - loss: 0.9299 - acc: 0.7016 - val_loss: 0.4623 - val_acc: 0.7806\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.68023 to 0.46233, saving model to ../data/checkpoints/stack/1create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.5036004114755972\n",
      "Epoch 5/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.8806 - acc: 0.7175 - val_loss: 0.5291 - val_acc: 0.7230\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.46233\n",
      "epoch end, f1-score:  0.49432813796919667\n",
      "Epoch 6/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.8365 - acc: 0.7351 - val_loss: 0.5235 - val_acc: 0.7241\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.46233\n",
      "epoch end, f1-score:  0.4992205410362219\n",
      "Epoch 7/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.7918 - acc: 0.7481 - val_loss: 0.4359 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.46233 to 0.43592, saving model to ../data/checkpoints/stack/1create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.5124571968355178\n",
      "Epoch 8/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.7551 - acc: 0.7616 - val_loss: 0.4824 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5195663397005679\n",
      "Epoch 9/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.7120 - acc: 0.7744 - val_loss: 0.6141 - val_acc: 0.6888\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.4854660875375877\n",
      "Epoch 10/100\n",
      "158362/158362 [==============================] - 96s 607us/step - loss: 0.6610 - acc: 0.7926 - val_loss: 0.6053 - val_acc: 0.7005\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.48966942148760323\n",
      "Epoch 11/100\n",
      "158362/158362 [==============================] - 96s 607us/step - loss: 0.6180 - acc: 0.8083 - val_loss: 0.4904 - val_acc: 0.7757\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5156505616752101\n",
      "Epoch 12/100\n",
      "158362/158362 [==============================] - 97s 609us/step - loss: 0.5725 - acc: 0.8244 - val_loss: 0.5221 - val_acc: 0.7575\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5116988809766022\n",
      "Epoch 13/100\n",
      "158362/158362 [==============================] - 97s 612us/step - loss: 0.4358 - acc: 0.8715 - val_loss: 0.5065 - val_acc: 0.7991\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5268292682926828\n",
      "Epoch 14/100\n",
      "158362/158362 [==============================] - 96s 607us/step - loss: 0.3691 - acc: 0.8954 - val_loss: 0.5448 - val_acc: 0.7880\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5256018989487962\n",
      "Epoch 15/100\n",
      "158362/158362 [==============================] - 96s 607us/step - loss: 0.3351 - acc: 0.9051 - val_loss: 0.5796 - val_acc: 0.7840\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5208426714477812\n",
      "Epoch 16/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.3059 - acc: 0.9148 - val_loss: 0.5674 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5197319434102755\n",
      "Epoch 17/100\n",
      "158362/158362 [==============================] - 96s 607us/step - loss: 0.2843 - acc: 0.9223 - val_loss: 0.5837 - val_acc: 0.8072\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5138198955547064\n",
      "Epoch 18/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.2551 - acc: 0.9315 - val_loss: 0.5905 - val_acc: 0.8081\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.510693120329812\n",
      "Epoch 19/100\n",
      "158362/158362 [==============================] - 97s 611us/step - loss: 0.2486 - acc: 0.9339 - val_loss: 0.5954 - val_acc: 0.8085\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5098901098901099\n",
      "Epoch 20/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.2454 - acc: 0.9349 - val_loss: 0.5941 - val_acc: 0.8127\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5108179419525066\n",
      "Epoch 21/100\n",
      "158362/158362 [==============================] - 97s 611us/step - loss: 0.2439 - acc: 0.9350 - val_loss: 0.5996 - val_acc: 0.8091\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5097326758370101\n",
      "Epoch 22/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.2397 - acc: 0.9360 - val_loss: 0.6020 - val_acc: 0.8102\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5085676913015043\n",
      "Epoch 23/100\n",
      "158362/158362 [==============================] - 96s 605us/step - loss: 0.2393 - acc: 0.9369 - val_loss: 0.6042 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5089437263350307\n",
      "Epoch 24/100\n",
      "158362/158362 [==============================] - 96s 607us/step - loss: 0.2335 - acc: 0.9380 - val_loss: 0.6036 - val_acc: 0.8115\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5094637223974764\n",
      "Epoch 25/100\n",
      "158362/158362 [==============================] - 96s 606us/step - loss: 0.2386 - acc: 0.9369 - val_loss: 0.6038 - val_acc: 0.8107\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5094907710433303\n",
      "Epoch 26/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 0.2353 - acc: 0.9380 - val_loss: 0.6030 - val_acc: 0.8114\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5100406877542986\n",
      "Epoch 27/100\n",
      "158362/158362 [==============================] - 96s 607us/step - loss: 0.2360 - acc: 0.9374 - val_loss: 0.6033 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.43592\n",
      "epoch end, f1-score:  0.5094339622641509\n",
      "Epoch 00027: early stopping\n",
      "Train on 158362 samples, validate on 19795 samples\n",
      "Epoch 1/100\n",
      "158362/158362 [==============================] - 105s 661us/step - loss: 1.2084 - acc: 0.4940 - val_loss: 0.7923 - val_acc: 0.2609\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.79227, saving model to ../data/checkpoints/stack/2create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.3184422602133507\n",
      "Epoch 2/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 1.1104 - acc: 0.6011 - val_loss: 0.6493 - val_acc: 0.6911\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.79227 to 0.64930, saving model to ../data/checkpoints/stack/2create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.4221865255598602\n",
      "Epoch 3/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 1.0324 - acc: 0.6408 - val_loss: 0.7499 - val_acc: 0.4810\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.64930\n",
      "epoch end, f1-score:  0.391157470515024\n",
      "Epoch 4/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.9667 - acc: 0.6729 - val_loss: 0.4602 - val_acc: 0.7907\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.64930 to 0.46021, saving model to ../data/checkpoints/stack/2create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.49891172914147514\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158362/158362 [==============================] - 97s 611us/step - loss: 0.9199 - acc: 0.6973 - val_loss: 0.6029 - val_acc: 0.6740\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.46021\n",
      "epoch end, f1-score:  0.4796000645057249\n",
      "Epoch 6/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 0.8759 - acc: 0.7148 - val_loss: 0.6653 - val_acc: 0.6100\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.46021\n",
      "epoch end, f1-score:  0.4480983700314556\n",
      "Epoch 7/100\n",
      "158362/158362 [==============================] - 97s 611us/step - loss: 0.8340 - acc: 0.7284 - val_loss: 0.3972 - val_acc: 0.8178\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.46021 to 0.39718, saving model to ../data/checkpoints/stack/2create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.5093877551020408\n",
      "Epoch 8/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.7945 - acc: 0.7430 - val_loss: 0.4272 - val_acc: 0.7916\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5224036123654047\n",
      "Epoch 9/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 0.7524 - acc: 0.7568 - val_loss: 0.5074 - val_acc: 0.7343\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5071689626089401\n",
      "Epoch 10/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.7142 - acc: 0.7719 - val_loss: 0.4867 - val_acc: 0.7618\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5240209931368591\n",
      "Epoch 11/100\n",
      "158362/158362 [==============================] - 96s 606us/step - loss: 0.6705 - acc: 0.7874 - val_loss: 0.4601 - val_acc: 0.7777\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.520174482006543\n",
      "Epoch 12/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 0.6272 - acc: 0.8014 - val_loss: 0.4707 - val_acc: 0.7880\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.524634726469589\n",
      "Epoch 13/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 0.4921 - acc: 0.8518 - val_loss: 0.4947 - val_acc: 0.7883\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5349017867051382\n",
      "Epoch 14/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.4241 - acc: 0.8761 - val_loss: 0.5014 - val_acc: 0.7958\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.529610238510762\n",
      "Epoch 15/100\n",
      "158362/158362 [==============================] - 97s 611us/step - loss: 0.3863 - acc: 0.8895 - val_loss: 0.5264 - val_acc: 0.7936\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5329827369383788\n",
      "Epoch 16/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.3553 - acc: 0.8992 - val_loss: 0.5210 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5224500389307034\n",
      "Epoch 17/100\n",
      "158362/158362 [==============================] - 96s 607us/step - loss: 0.3342 - acc: 0.9056 - val_loss: 0.5555 - val_acc: 0.7973\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5295497185741088\n",
      "Epoch 18/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.3073 - acc: 0.9139 - val_loss: 0.5523 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5279162512462612\n",
      "Epoch 19/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.3018 - acc: 0.9169 - val_loss: 0.5615 - val_acc: 0.8052\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5312917729979341\n",
      "Epoch 20/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.2962 - acc: 0.9181 - val_loss: 0.5591 - val_acc: 0.8088\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5268158519814977\n",
      "Epoch 21/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.2930 - acc: 0.9193 - val_loss: 0.5595 - val_acc: 0.8114\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5246402648669298\n",
      "Epoch 22/100\n",
      "158362/158362 [==============================] - 96s 607us/step - loss: 0.2871 - acc: 0.9220 - val_loss: 0.5654 - val_acc: 0.8079\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5256330298116504\n",
      "Epoch 23/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.2872 - acc: 0.9206 - val_loss: 0.5657 - val_acc: 0.8083\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.525565695711964\n",
      "Epoch 24/100\n",
      "158362/158362 [==============================] - 97s 612us/step - loss: 0.2864 - acc: 0.9210 - val_loss: 0.5650 - val_acc: 0.8087\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5247301029374843\n",
      "Epoch 25/100\n",
      "158362/158362 [==============================] - 97s 612us/step - loss: 0.2884 - acc: 0.9220 - val_loss: 0.5679 - val_acc: 0.8075\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5247005988023952\n",
      "Epoch 26/100\n",
      "158362/158362 [==============================] - 97s 611us/step - loss: 0.2872 - acc: 0.9206 - val_loss: 0.5666 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5255511022044088\n",
      "Epoch 27/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.2852 - acc: 0.9217 - val_loss: 0.5676 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.39718\n",
      "epoch end, f1-score:  0.5248902821316614\n",
      "Epoch 00027: early stopping\n",
      "Train on 158362 samples, validate on 19795 samples\n",
      "Epoch 1/100\n",
      "158362/158362 [==============================] - 105s 660us/step - loss: 1.2074 - acc: 0.5033 - val_loss: 0.7105 - val_acc: 0.5410\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.71050, saving model to ../data/checkpoints/stack/3create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.34925864909390447\n",
      "Epoch 2/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 1.0914 - acc: 0.5998 - val_loss: 0.5862 - val_acc: 0.7017\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.71050 to 0.58620, saving model to ../data/checkpoints/stack/3create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.4436068972015453\n",
      "Epoch 3/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 1.0007 - acc: 0.6598 - val_loss: 0.4985 - val_acc: 0.7446\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.58620 to 0.49850, saving model to ../data/checkpoints/stack/3create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.48047677764077273\n",
      "Epoch 4/100\n",
      "158362/158362 [==============================] - 96s 607us/step - loss: 0.9426 - acc: 0.6810 - val_loss: 0.6734 - val_acc: 0.6114\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49850\n",
      "epoch end, f1-score:  0.4441875587024059\n",
      "Epoch 5/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 0.8984 - acc: 0.7029 - val_loss: 0.4552 - val_acc: 0.7851\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49850 to 0.45525, saving model to ../data/checkpoints/stack/3create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.5074108383510886\n",
      "Epoch 6/100\n",
      "158362/158362 [==============================] - 97s 611us/step - loss: 0.8596 - acc: 0.7167 - val_loss: 0.5292 - val_acc: 0.7164\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.4929997290217686\n",
      "Epoch 7/100\n",
      "158362/158362 [==============================] - 97s 611us/step - loss: 0.8195 - acc: 0.7319 - val_loss: 0.5436 - val_acc: 0.7112\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.4940697468578509\n",
      "Epoch 8/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.7813 - acc: 0.7426 - val_loss: 0.5750 - val_acc: 0.6989\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.4912520269693607\n",
      "Epoch 9/100\n",
      "158362/158362 [==============================] - 96s 606us/step - loss: 0.7370 - acc: 0.7615 - val_loss: 0.5291 - val_acc: 0.7264\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.5083083628439118\n",
      "Epoch 10/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.6886 - acc: 0.7785 - val_loss: 0.5671 - val_acc: 0.7160\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.49843847595252966\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.5488 - acc: 0.8306 - val_loss: 0.4958 - val_acc: 0.7808\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.5354887057060271\n",
      "Epoch 12/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 0.4784 - acc: 0.8569 - val_loss: 0.5202 - val_acc: 0.7789\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.5336174746936602\n",
      "Epoch 13/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.4383 - acc: 0.8709 - val_loss: 0.5301 - val_acc: 0.7796\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.532418818990462\n",
      "Epoch 14/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 0.4062 - acc: 0.8813 - val_loss: 0.5308 - val_acc: 0.7900\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.533811820118874\n",
      "Epoch 15/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.3817 - acc: 0.8899 - val_loss: 0.5467 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.5333181455746668\n",
      "Epoch 16/100\n",
      "158362/158362 [==============================] - 97s 612us/step - loss: 0.3489 - acc: 0.9005 - val_loss: 0.5377 - val_acc: 0.8056\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.5333979876348648\n",
      "Epoch 17/100\n",
      "158362/158362 [==============================] - 97s 613us/step - loss: 0.3441 - acc: 0.9023 - val_loss: 0.5429 - val_acc: 0.8034\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.5322115384615385\n",
      "Epoch 18/100\n",
      "158362/158362 [==============================] - 97s 611us/step - loss: 0.3356 - acc: 0.9051 - val_loss: 0.5480 - val_acc: 0.8025\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.5313998082454459\n",
      "Epoch 19/100\n",
      "158362/158362 [==============================] - 97s 611us/step - loss: 0.3365 - acc: 0.9052 - val_loss: 0.5468 - val_acc: 0.8035\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.5279766962009953\n",
      "Epoch 20/100\n",
      "158362/158362 [==============================] - 97s 609us/step - loss: 0.3324 - acc: 0.9070 - val_loss: 0.5461 - val_acc: 0.8063\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.5300318705565089\n",
      "Epoch 21/100\n",
      "158362/158362 [==============================] - 97s 611us/step - loss: 0.3286 - acc: 0.9082 - val_loss: 0.5489 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.5302029836145757\n",
      "Epoch 22/100\n",
      "158362/158362 [==============================] - 96s 607us/step - loss: 0.3276 - acc: 0.9084 - val_loss: 0.5476 - val_acc: 0.8053\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.5292465502503358\n",
      "Epoch 23/100\n",
      "158362/158362 [==============================] - 96s 606us/step - loss: 0.3264 - acc: 0.9078 - val_loss: 0.5477 - val_acc: 0.8058\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.5293181539968173\n",
      "Epoch 24/100\n",
      "158362/158362 [==============================] - 97s 609us/step - loss: 0.3290 - acc: 0.9079 - val_loss: 0.5491 - val_acc: 0.8054\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.5295554469956033\n",
      "Epoch 25/100\n",
      "158362/158362 [==============================] - 97s 609us/step - loss: 0.3291 - acc: 0.9083 - val_loss: 0.5497 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.45525\n",
      "epoch end, f1-score:  0.5289457647775746\n",
      "Epoch 00025: early stopping\n",
      "Train on 158362 samples, validate on 19795 samples\n",
      "Epoch 1/100\n",
      "158362/158362 [==============================] - 105s 664us/step - loss: 1.1926 - acc: 0.5287 - val_loss: 0.7253 - val_acc: 0.4864\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.72530, saving model to ../data/checkpoints/stack/4create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.3617301776633812\n",
      "Epoch 2/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 1.0872 - acc: 0.6308 - val_loss: 0.6593 - val_acc: 0.6159\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.72530 to 0.65934, saving model to ../data/checkpoints/stack/4create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.43206095465750355\n",
      "Epoch 3/100\n",
      "158362/158362 [==============================] - 96s 606us/step - loss: 0.9921 - acc: 0.6752 - val_loss: 0.8315 - val_acc: 0.4856\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.65934\n",
      "epoch end, f1-score:  0.39684890126162414\n",
      "Epoch 4/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 0.9286 - acc: 0.7015 - val_loss: 0.6203 - val_acc: 0.6610\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.65934 to 0.62031, saving model to ../data/checkpoints/stack/4create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.4695652173913044\n",
      "Epoch 5/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.8860 - acc: 0.7167 - val_loss: 0.7450 - val_acc: 0.5937\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62031\n",
      "epoch end, f1-score:  0.44637202258020103\n",
      "Epoch 6/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.8415 - acc: 0.7342 - val_loss: 0.4577 - val_acc: 0.7780\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62031 to 0.45769, saving model to ../data/checkpoints/stack/4create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.5215592334494774\n",
      "Epoch 7/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.8041 - acc: 0.7450 - val_loss: 0.5184 - val_acc: 0.7345\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.45769\n",
      "epoch end, f1-score:  0.5068493150684932\n",
      "Epoch 8/100\n",
      "158362/158362 [==============================] - 96s 607us/step - loss: 0.7601 - acc: 0.7584 - val_loss: 0.6059 - val_acc: 0.6668\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.45769\n",
      "epoch end, f1-score:  0.4782849458112491\n",
      "Epoch 9/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 0.7190 - acc: 0.7713 - val_loss: 0.4185 - val_acc: 0.8021\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.45769 to 0.41848, saving model to ../data/checkpoints/stack/4create_model_cnnweights.hdf5\n",
      "epoch end, f1-score:  0.5210268948655257\n",
      "Epoch 10/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 0.6754 - acc: 0.7860 - val_loss: 0.5182 - val_acc: 0.7372\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5087818696883851\n",
      "Epoch 11/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 0.6303 - acc: 0.8039 - val_loss: 0.5810 - val_acc: 0.7238\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5049791779829802\n",
      "Epoch 12/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.5848 - acc: 0.8195 - val_loss: 0.5538 - val_acc: 0.7301\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5036238617357369\n",
      "Epoch 13/100\n",
      "158362/158362 [==============================] - 96s 606us/step - loss: 0.5416 - acc: 0.8354 - val_loss: 0.4712 - val_acc: 0.7954\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5218930721114128\n",
      "Epoch 14/100\n",
      "158362/158362 [==============================] - 96s 607us/step - loss: 0.5029 - acc: 0.8489 - val_loss: 0.4931 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.505600416775202\n",
      "Epoch 15/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.3769 - acc: 0.8923 - val_loss: 0.5297 - val_acc: 0.7932\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5231772653156301\n",
      "Epoch 16/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.3153 - acc: 0.9132 - val_loss: 0.5431 - val_acc: 0.8028\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5239024390243902\n",
      "Epoch 17/100\n",
      "158362/158362 [==============================] - 97s 611us/step - loss: 0.2862 - acc: 0.9227 - val_loss: 0.5697 - val_acc: 0.7998\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5225355507351169\n",
      "Epoch 18/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 0.2608 - acc: 0.9302 - val_loss: 0.5894 - val_acc: 0.7999\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5238610409905037\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.2421 - acc: 0.9360 - val_loss: 0.5920 - val_acc: 0.8105\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5164367667912854\n",
      "Epoch 20/100\n",
      "158362/158362 [==============================] - 97s 610us/step - loss: 0.2197 - acc: 0.9419 - val_loss: 0.5995 - val_acc: 0.8149\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5159841479524439\n",
      "Epoch 21/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.2161 - acc: 0.9437 - val_loss: 0.6082 - val_acc: 0.8109\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5166537567776918\n",
      "Epoch 22/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.2143 - acc: 0.9439 - val_loss: 0.6096 - val_acc: 0.8139\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5165354330708661\n",
      "Epoch 23/100\n",
      "158362/158362 [==============================] - 96s 608us/step - loss: 0.2096 - acc: 0.9453 - val_loss: 0.6151 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5141663402532968\n",
      "Epoch 24/100\n",
      "158362/158362 [==============================] - 96s 609us/step - loss: 0.2074 - acc: 0.9460 - val_loss: 0.6115 - val_acc: 0.8166\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5132743362831859\n",
      "Epoch 25/100\n",
      "158362/158362 [==============================] - 97s 612us/step - loss: 0.2069 - acc: 0.9470 - val_loss: 0.6148 - val_acc: 0.8142\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5165615141955836\n",
      "Epoch 26/100\n",
      "158362/158362 [==============================] - 97s 611us/step - loss: 0.2047 - acc: 0.9466 - val_loss: 0.6162 - val_acc: 0.8142\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5162478621234048\n",
      "Epoch 27/100\n",
      "158362/158362 [==============================] - 97s 612us/step - loss: 0.2035 - acc: 0.9472 - val_loss: 0.6173 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5178925128746863\n",
      "Epoch 28/100\n",
      "158362/158362 [==============================] - 95s 598us/step - loss: 0.2032 - acc: 0.9475 - val_loss: 0.6147 - val_acc: 0.8154\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5161546610169492\n",
      "Epoch 29/100\n",
      "158362/158362 [==============================] - 95s 598us/step - loss: 0.2057 - acc: 0.9468 - val_loss: 0.6160 - val_acc: 0.8150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.41848\n",
      "epoch end, f1-score:  0.5171413502109705\n",
      "Epoch 00029: early stopping\n"
     ]
    }
   ],
   "source": [
    "X_train_q1 = word_squence_ques1_char\n",
    "X_train_q2 = word_squence_ques2_char\n",
    "X_train_q10 = word_squence_ques1_word\n",
    "X_train_q20 = word_squence_ques2_word\n",
    "y_train = y_label\n",
    "def parse_train_data(ix_train, ix_val):\n",
    "    \n",
    "    X_fold_train_q1 = np.vstack([X_train_q1[ix_train], X_train_q2[ix_train]])\n",
    "    X_fold_train_q2 = np.vstack([X_train_q2[ix_train], X_train_q1[ix_train]])\n",
    "    X_fold_train_q10 = np.vstack([X_train_q10[ix_train], X_train_q20[ix_train]])\n",
    "    X_fold_train_q20 = np.vstack([X_train_q20[ix_train], X_train_q10[ix_train]])\n",
    "   \n",
    "    X_fold_val_q1 = X_train_q1[ix_val]# np.vstack([X_train_q1[ix_val], X_train_q2[ix_val]])\n",
    "    X_fold_val_q2 = X_train_q2[ix_val]# np.vstack([X_train_q2[ix_val], X_train_q1[ix_val]])\n",
    "    X_fold_val_q10 = X_train_q10[ix_val]# np.vstack([X_train_q10[ix_val], X_train_q20[ix_val]])\n",
    "    X_fold_val_q20 = X_train_q20[ix_val]# np.vstack([X_train_q20[ix_val], X_train_q10[ix_val]])\n",
    "    \n",
    "    # Ground truth should also be \"mirrored\".\n",
    "    y_fold_train = np.concatenate([y_train[ix_train], y_train[ix_train]])\n",
    "    y_fold_val = y_train[ix_val]# np.concatenate([y_train[ix_val], y_train[ix_val]])\n",
    "    return [X_fold_train_q1, X_fold_train_q2, X_fold_train_q10, X_fold_train_q20], y_fold_train, [\n",
    "        X_fold_val_q1, X_fold_val_q2, X_fold_val_q10, X_fold_val_q20], y_fold_val\n",
    "\n",
    "model_params = {\n",
    "    'dense_dropout_rate': 0.3,\n",
    "    'lstm_dropout_rate': 0.3,\n",
    "    'num_dense': 150,\n",
    "    'num_lstm': 128,\n",
    "    'num_filters':32\n",
    "}\n",
    "BATCH_SIZE = 256\n",
    "MAX_EPOCHS = 100\n",
    "for model_func in [create_model_dssm, create_model_cnn]:\n",
    "    train_oofp = np.zeros((y_train.shape[0], 1))\n",
    "    for fold_num, (ix_train, ix_val) in enumerate(kfold):\n",
    "        train_x, train_y, val_x, val_y = parse_train_data(ix_train, ix_val)\n",
    "        model = model_func(model_params)\n",
    "        model.fit(train_x, train_y,\n",
    "                    validation_data=(val_x, val_y),\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    verbose=1,\n",
    "                    shuffle=True,\n",
    "                    class_weight=class_weight,\n",
    "                    callbacks=[\n",
    "                        # Stop training when the validation loss stops improving.\n",
    "                        EarlyStopping(\n",
    "                            monitor='val_loss',\n",
    "                            min_delta=0.001,\n",
    "                            patience=20,\n",
    "                            verbose=1,\n",
    "                            mode='auto',\n",
    "                        ),\n",
    "                        ReduceLROnPlateau(\n",
    "                            monitor='val_loss', \n",
    "                            factor=0.1, \n",
    "                            patience=5, \n",
    "                            min_delta=0.0001,\n",
    "                            cooldown=1, \n",
    "                        ),\n",
    "                        ModelCheckpoint(\n",
    "                            os.path.join('../data/checkpoints/stack', str(fold_num) + \n",
    "                                         str(model_func.func_name) + 'weights.hdf5'),\n",
    "                            monitor='val_loss',\n",
    "                            save_best_only=True,\n",
    "                            verbose=2,\n",
    "                            mode='auto',\n",
    "                        ),\n",
    "                        metrics\n",
    "                    ],\n",
    "                )\n",
    "        train_oofp[ix_val] = model.predict(val_x)\n",
    "        \n",
    "        K.clear_session()\n",
    "        del train_x, val_x, train_y, val_y\n",
    "        del model\n",
    "        gc.collect()\n",
    "    stacking_res.append(train_oofp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_stack = np.concatenate(stacking_res, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stacking_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.314372\tvalid-error:0.317741\ttrain-F1:0.406627\tvalid-F1:0.40638\n",
      "Multiple eval metrics have been passed: 'valid-F1' will be used for early stopping.\n",
      "\n",
      "Will train until valid-F1 hasn't improved in 200 rounds.\n",
      "[15]\ttrain-error:0.255633\tvalid-error:0.255961\ttrain-F1:0.527775\tvalid-F1:0.53518\n",
      "[30]\ttrain-error:0.253827\tvalid-error:0.254395\ttrain-F1:0.529937\tvalid-F1:0.536365\n",
      "[45]\ttrain-error:0.254471\tvalid-error:0.254799\ttrain-F1:0.530074\tvalid-F1:0.536823\n",
      "[60]\ttrain-error:0.254092\tvalid-error:0.254445\ttrain-F1:0.530687\tvalid-F1:0.538102\n",
      "[75]\ttrain-error:0.255418\tvalid-error:0.256365\ttrain-F1:0.528512\tvalid-F1:0.535639\n",
      "[90]\ttrain-error:0.255481\tvalid-error:0.256264\ttrain-F1:0.529306\tvalid-F1:0.536924\n",
      "[105]\ttrain-error:0.255759\tvalid-error:0.255405\ttrain-F1:0.528092\tvalid-F1:0.538012\n",
      "[120]\ttrain-error:0.255948\tvalid-error:0.2549\ttrain-F1:0.527929\tvalid-F1:0.538166\n",
      "[135]\ttrain-error:0.256151\tvalid-error:0.255961\ttrain-F1:0.527974\tvalid-F1:0.536964\n",
      "[150]\ttrain-error:0.25581\tvalid-error:0.255607\ttrain-F1:0.52824\tvalid-F1:0.537223\n",
      "[165]\ttrain-error:0.255633\tvalid-error:0.25586\ttrain-F1:0.528303\tvalid-F1:0.536808\n",
      "[180]\ttrain-error:0.2549\tvalid-error:0.254698\ttrain-F1:0.529281\tvalid-F1:0.538278\n",
      "[195]\ttrain-error:0.255633\tvalid-error:0.256617\ttrain-F1:0.529334\tvalid-F1:0.536242\n",
      "[210]\ttrain-error:0.254875\tvalid-error:0.255607\ttrain-F1:0.530117\tvalid-F1:0.537308\n",
      "[225]\ttrain-error:0.255279\tvalid-error:0.255405\ttrain-F1:0.529635\tvalid-F1:0.537589\n",
      "[240]\ttrain-error:0.254635\tvalid-error:0.255961\ttrain-F1:0.530592\tvalid-F1:0.53688\n",
      "[255]\ttrain-error:0.255001\tvalid-error:0.255809\ttrain-F1:0.530365\tvalid-F1:0.537365\n",
      "[270]\ttrain-error:0.2549\tvalid-error:0.255506\ttrain-F1:0.530704\tvalid-F1:0.537829\n",
      "[285]\ttrain-error:0.255052\tvalid-error:0.255405\ttrain-F1:0.530556\tvalid-F1:0.53818\n",
      "[300]\ttrain-error:0.255115\tvalid-error:0.255557\ttrain-F1:0.530429\tvalid-F1:0.537695\n",
      "Stopping. Best iteration:\n",
      "[102]\ttrain-error:0.255443\tvalid-error:0.2549\ttrain-F1:0.528488\tvalid-F1:0.538925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 设置boosting迭代计算次数  \n",
    "Xtr, Xv, ytr, yv = train_test_split(x_train_stack, y_train, test_size=0.2, random_state=2000)\n",
    "dtrain = xgb.DMatrix(Xtr, label=ytr)\n",
    "dvalid = xgb.DMatrix(Xv, label=yv)\n",
    "num_round = 2  \n",
    "param = {'max_depth':2, 'eta':1, 'silent':0, 'objective':'binary:logistic' }  \n",
    "\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "#自定义马修斯相关系数\n",
    "def evalacc(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    _val_f1 = f1_score(labels, preds > THRESHOLD)\n",
    "    return 'F1', _val_f1\n",
    "\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.5,\n",
    "    'learning_rate': 0.001,\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 2,\n",
    "    'min_child_weight': 1,\n",
    "     'scale_pos_weight':5,\n",
    "}\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "model_xgb = xgb.train(xgb_params, dtrain, 500, watchlist, early_stopping_rounds=200,\n",
    "                  maximize=True, verbose_eval=15,  feval=evalacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.5,\n",
    "    'learning_rate': 0.001,\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 2,\n",
    "    'min_child_weight': 1,\n",
    "}\n",
    "\n",
    "model = XGBClassifier(**xgb_params)\n",
    "\n",
    "model.fit(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(Xv)\n",
    "predicted_test_xgb = model_xgb.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_xgb.save_model('../xgb.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'create_model_dssm'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
