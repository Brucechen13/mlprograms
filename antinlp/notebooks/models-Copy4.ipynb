{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.250 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tflearn.data_utils import pad_sequences\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras import initializers\n",
    "import os\n",
    "import keras\n",
    "import jieba\n",
    "from collections import Counter \n",
    "jieba.add_word('花呗')\n",
    "jieba.add_word('借呗')\n",
    "jieba.add_word('收钱码')\n",
    "jieba.add_word('收款码')\n",
    "\n",
    "from keras.optimizers import Adadelta\n",
    "import gc\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "VOCAB_LENGTH = 3000\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0                                     1                                                  2  3\n",
      "0  1                        11 239 3 2 216  6 5 3 2 19 151 5 216 9 11 965 582 44 5 31 5 17...  1\n",
      "1  2  105 153 34 3 2 9 72 586 8 139 3241 8                             829 135 139 211 3 2 35  0\n",
      "2  3               3 2 113 182 12 23 21 10                           6 5 202 14 21 3 2 123 10  0\n",
      "3  4                             57 54 4 2                                      67 568 54 4 2  0\n",
      "4  5                           3 2 313 579                                212 1037 14 33 2 10  0\n"
     ]
    }
   ],
   "source": [
    "train_data_word = pd.read_csv('../train-w_split.csv', header=None)\n",
    "train_data_char = pd.read_csv('../train-c_split.csv', header=None)\n",
    "print train_data_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\\u82b1\\u5457', u'\\u5206\\u671f', u'\\u67e5\\u8be2']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.lcut('花呗分期查询'.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "PAD_ID = 0\n",
    "UNK_ID=1\n",
    "\n",
    "def translate(text, translation):\n",
    "    for token, replacement in translation.items():\n",
    "        text = text.replace(token, ' ' + replacement + ' ')\n",
    "    text = text.replace('  ', ' ')\n",
    "    return text\n",
    "def token_string_as_list(string, token_string='char'):\n",
    "    string = string.decode('UTF-8')\n",
    "    translation = {\n",
    "        '***':'*',\n",
    "        u'花被':u'花呗'\n",
    "    }\n",
    "    translate(string, translation)\n",
    "    length = len(string)\n",
    "    if token_string == 'char':\n",
    "        listt = [string[i] for i in range(length)]\n",
    "    elif token_string == 'word':\n",
    "        listt = jieba.lcut(string)\n",
    "    elif token_string == 'pinyin':\n",
    "        string = ''.join(jibe.lcut(string))\n",
    "        listt = ''.join(lazy_pinyin(string)).split()\n",
    "    listt = [item for item in listt if item.strip()]\n",
    "    return listt\n",
    "def create_voca(train_data, token_string='char', voca_size=5000):\n",
    "    size = train_data.shape[0]\n",
    "    input_count = Counter()\n",
    "    for i in range(size):\n",
    "        token_list = token_string_as_list(train_data.iloc[i, 1], token_string=token_string)\n",
    "        input_count.update(token_list)\n",
    "        token_list = token_string_as_list(train_data.iloc[i, 2], token_string=token_string)\n",
    "        input_count.update(token_list)\n",
    "    vocab_worddict = {}\n",
    "    vocab_indexdict = {}\n",
    "    vocab_list = input_count.most_common(voca_size)\n",
    "    for i, tuplee in enumerate(vocab_list):\n",
    "        word,_ = tuplee\n",
    "        vocab_worddict[i+2] = word\n",
    "        vocab_indexdict[word] = i+2\n",
    "    return vocab_worddict, vocab_indexdict\n",
    "def parse_train_data(train_data, vocab_indexdict, token_string):\n",
    "    size = train_data.shape[0]\n",
    "    input_count = Counter()\n",
    "    parse_data = train_data.copy()\n",
    "    temp_data = train_data.copy()\n",
    "    for i in range(size):\n",
    "        token_list1 = token_string_as_list(train_data.iloc[i, 1], token_string=token_string)\n",
    "        token_list2 = token_string_as_list(train_data.iloc[i, 2], token_string=token_string)\n",
    "        temp_data.iloc[i, 1] = ' '.join(token_list1)\n",
    "        temp_data.iloc[i, 2] = ' '.join(token_list2)\n",
    "        #print(token_list1, train_data.iloc[i, 1].decode('UTF-8'))\n",
    "        parse_data.iloc[i, 1] = ' '.join([str(vocab_indexdict[item]) for item in token_list1 if item in vocab_indexdict])\n",
    "        parse_data.iloc[i, 2] = ' '.join([str(vocab_indexdict[item]) for item in token_list2 if item in vocab_indexdict])\n",
    "    return parse_data\n",
    "\n",
    "\n",
    "\n",
    "def parse_split(train_data):\n",
    "    vocab_worddict_word, vocab_indexdict_word = create_voca(train_data, token_string='char')\n",
    "    parse_data = parse_train_data(train_data, vocab_indexdict_word,token_string='char')\n",
    "    return parse_data\n",
    "\n",
    "train_data_char_df = parse_split(train_data_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_split2(train_data):\n",
    "    vocab_worddict_word, vocab_indexdict_word = create_voca(train_data, token_string='word')\n",
    "    parse_data = parse_train_data(train_data, vocab_indexdict_word,token_string='word')\n",
    "    return parse_data\n",
    "train_data_word_df = parse_split2(train_data_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11 239 3 2 216</td>\n",
       "      <td>6 5 3 2 19 151 5 216 9 11 965 582 44 5 31 5 17...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>105 153 34 3 2 9 72 586 8 139 3241 8</td>\n",
       "      <td>829 135 139 211 3 2 35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3 2 113 182 12 23 21 10</td>\n",
       "      <td>6 5 202 14 21 3 2 123 10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>57 54 4 2</td>\n",
       "      <td>67 568 54 4 2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3 2 313 579</td>\n",
       "      <td>212 1037 14 33 2 10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                     1                                                  2  3\n",
       "0  1                        11 239 3 2 216  6 5 3 2 19 151 5 216 9 11 965 582 44 5 31 5 17...  1\n",
       "1  2  105 153 34 3 2 9 72 586 8 139 3241 8                             829 135 139 211 3 2 35  0\n",
       "2  3               3 2 113 182 12 23 21 10                           6 5 202 14 21 3 2 123 10  0\n",
       "3  4                             57 54 4 2                                      67 568 54 4 2  0\n",
       "4  5                           3 2 313 579                                212 1037 14 33 2 10  0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_char_df.to_csv('../train-c_split.csv', index=False, header=False)\n",
    "train_data_word_df.to_csv('../train-w_split.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "VOC_SIZE_CHAR = 2087\n",
    "VOC_SIZE_WORD = 5002\n",
    "from keras.layers import *\n",
    "from keras.activations import softmax\n",
    "from keras.models import Model\n",
    "\n",
    "def unchanged_shape(input_shape):\n",
    "    return input_shape\n",
    "\n",
    "def time_distributed(x, layers):\n",
    "    for l in layers:\n",
    "        x = TimeDistributed(l)(x)\n",
    "    return x\n",
    "\n",
    "def align(input_1, input_2):\n",
    "    attention = Dot(axes=-1)([input_1, input_2])\n",
    "    w_att_1 = Lambda(lambda x: softmax(x, axis=1),\n",
    "                     output_shape=unchanged_shape)(attention)\n",
    "    w_att_2 = Permute((2,1))(Lambda(lambda x: softmax(x, axis=2),\n",
    "                             output_shape=unchanged_shape)(attention))\n",
    "    in1_aligned = Dot(axes=1)([w_att_1, input_1])\n",
    "    in2_aligned = Dot(axes=1)([w_att_2, input_2])\n",
    "    return in1_aligned, in2_aligned\n",
    "\n",
    "def aggregate(x1, x2, x10, x20,  num_class, dense_dim=300, dropout_rate=0.2, activation=\"relu\"):#\n",
    "    feat1 = concatenate(list(map(lambda l: l(x1), [GlobalAvgPool1D(), GlobalMaxPool1D()])))\n",
    "    feat2 = concatenate(list(map(lambda l: l(x2), [GlobalAvgPool1D(), GlobalMaxPool1D()])))\n",
    "    feat10= concatenate(list(map(lambda l: l(x10), [GlobalAvgPool1D(), GlobalMaxPool1D()])))\n",
    "    feat20 = concatenate(list(map(lambda l: l(x20), [GlobalAvgPool1D(), GlobalMaxPool1D()])))\n",
    "    x = Concatenate()([feat1, feat2, feat10, feat20]) #\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(dense_dim, activation=activation)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(dense_dim, activation=activation)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    return x  \n",
    "\n",
    "def abs_diff(X):\n",
    "    s = X[0]\n",
    "    for i in range(1, len(X)):\n",
    "        s -= X[i]\n",
    "    s = K.abs(s)\n",
    "    return s\n",
    "\n",
    "def aggregate_both(x1, x2, num_class, dense_dim=300, dropout_rate=0.2, activation=\"relu\"):#\n",
    "    feat1 = concatenate(list(map(lambda l: l(x1), [GlobalAvgPool1D(), GlobalMaxPool1D()])))\n",
    "    feat2 = concatenate(list(map(lambda l: l(x2), [GlobalAvgPool1D(), GlobalMaxPool1D()])))\n",
    "    feat3 = add([feat1, feat2])\n",
    "    feat4 = subtract([feat1, feat2])\n",
    "    feat5 = Lambda(lambda x: abs(x[0]-x[1]))([feat1, feat2])\n",
    "    x = Concatenate()([feat1, feat2]) #, feat3, feat4\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(dense_dim, activation=activation)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(dense_dim, activation=activation)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def embed_project_atten(q1, q2, encode, projection_dim=300, projection_hidden=200, projection_dropout=0.2,\n",
    "                           compare_dim=500, compare_dropout=0.2,\n",
    "                           dense_dim=300, dropout_rate=0.2,\n",
    "                           lr=1e-3, activation='relu'):\n",
    "    q1_embed = encode(q1)\n",
    "    q2_embed = encode(q2)\n",
    "    \n",
    "    # Projection\n",
    "    projection_layers = []\n",
    "    if projection_hidden > 0:\n",
    "        projection_layers.extend([\n",
    "                Dense(projection_hidden, activation=activation),\n",
    "                Dropout(rate=projection_dropout),\n",
    "            ])\n",
    "    projection_layers.extend([\n",
    "            Dense(projection_dim, activation=None),\n",
    "            Dropout(rate=projection_dropout),\n",
    "        ])\n",
    "    q1_encoded = time_distributed(q1_embed, projection_layers)\n",
    "    q2_encoded = time_distributed(q2_embed, projection_layers)\n",
    "    \n",
    "    # Attention\n",
    "    q1_aligned, q2_aligned = align(q1_encoded, q2_encoded)    \n",
    "    \n",
    "    # Compare\n",
    "    q1_combined = concatenate([q1_encoded, q2_aligned])\n",
    "    q2_combined = concatenate([q2_encoded, q1_aligned]) \n",
    "    compare_layers = [\n",
    "        Dense(compare_dim, activation=activation),\n",
    "        Dropout(compare_dropout),\n",
    "        Dense(compare_dim, activation=activation),\n",
    "        Dropout(compare_dropout),\n",
    "    ]\n",
    "    q1_compare = time_distributed(q1_combined, compare_layers)\n",
    "    q2_compare = time_distributed(q2_combined, compare_layers)\n",
    "    return q1_compare, q2_compare\n",
    "\n",
    "def lstm_project_atten(q1_embed, q2_embed, projection_dim=300, projection_hidden=200, projection_dropout=0.2,\n",
    "                           compare_dim=500, compare_dropout=0.2,\n",
    "                           dense_dim=300, dropout_rate=0.2,\n",
    "                           lr=1e-3, activation='relu'):\n",
    "    \n",
    "    # CuDNN\n",
    "    lstm0 = CuDNNLSTM(model_params['num_lstm'],\n",
    "                 return_sequences = True)\n",
    "    lstm1 = Bidirectional(CuDNNLSTM(model_params['num_lstm'],\n",
    "                 return_sequences = True))\n",
    "    lstm2 = CuDNNLSTM(model_params['num_lstm'])\n",
    "    \n",
    "    att1 = Lambda(lambda x: K.max(x,axis = 1))\n",
    "    q1_embed = lstm1(q1_embed)\n",
    "    q2_embed = lstm1(q2_embed)\n",
    "    \n",
    "    # Projection\n",
    "    projection_layers = []\n",
    "    if projection_hidden > 0:\n",
    "        projection_layers.extend([\n",
    "                Dense(projection_hidden, activation=activation),\n",
    "                Dropout(rate=projection_dropout),\n",
    "            ])\n",
    "    projection_layers.extend([\n",
    "            Dense(projection_dim, activation=None),\n",
    "            Dropout(rate=projection_dropout),\n",
    "        ])\n",
    "    q1_encoded = time_distributed(q1_embed, projection_layers)\n",
    "    q2_encoded = time_distributed(q2_embed, projection_layers)\n",
    "    \n",
    "    # Attention\n",
    "    q1_aligned, q2_aligned = align(q1_encoded, q2_encoded)  \n",
    "    \n",
    "    # Compare\n",
    "    q1_combined = concatenate([q1_encoded, q2_aligned])\n",
    "    q2_combined = concatenate([q2_encoded, q1_aligned]) \n",
    "    compare_layers = [\n",
    "        Dense(compare_dim, activation=activation),\n",
    "        Dropout(compare_dropout),\n",
    "        Dense(compare_dim, activation=activation),\n",
    "        Dropout(compare_dropout),\n",
    "    ]\n",
    "    q1_compare = time_distributed(q1_combined, compare_layers)\n",
    "    q2_compare = time_distributed(q2_combined, compare_layers)\n",
    "    return q1_compare, q2_compare\n",
    "\n",
    "def build_model_combine(params, num_class=1,projection_dim=300, projection_hidden=200, projection_dropout=0.2,\n",
    "                           compare_dim=500, compare_dropout=0.2,\n",
    "                           dense_dim=300, dropout_rate=0.2,\n",
    "                           lr=1e-3, activation='relu'):\n",
    "    q1 = Input(name='q1',shape=(MAX_SEQUENCE_LENGTH_CHAR,), dtype='int32')\n",
    "    q2 = Input(name='q2',shape=(MAX_SEQUENCE_LENGTH_CHAR,), dtype='int32')\n",
    "    \n",
    "    q1_com = Input(name='q1_com',shape=(MAX_SEQUENCE_LENGTH_CHAR, 1), dtype='float32')\n",
    "    q2_com = Input(name='q2_com',shape=(MAX_SEQUENCE_LENGTH_CHAR, 1), dtype='float32')\n",
    "    \n",
    "    encode = Embedding(VOC_SIZE_CHAR+2, 300, input_length = MAX_SEQUENCE_LENGTH_CHAR) # \n",
    "    #encode0 = StaticEmbedding(embedding_matrix_char) #\n",
    "    \n",
    "    # Word Representation Layer\n",
    "    q1_embed = encode(q1) #concatenate([, q1_com])\n",
    "    q2_embed = encode(q2) #concatenate([, q2_com])\n",
    "    \n",
    "    # Densely-connected Co-attentive networks\n",
    "    q1_compare0, q2_compare0 = lstm_project_atten(q1_embed, q2_embed)\n",
    "    \n",
    "    \n",
    "    q1_compare0 = concatenate([q1_compare0, q1_embed])\n",
    "    q2_compare0 = concatenate([q2_compare0, q2_embed])\n",
    "    \n",
    "    q1_compare1, q2_compare1 = lstm_project_atten(q1_compare0, q2_compare0)\n",
    "    \n",
    "    q1_compare1 = concatenate([q1_compare1, q1_compare0])\n",
    "    q2_compare1 = concatenate([q2_compare1, q2_compare0])\n",
    "    \n",
    "    q1_compare2, q2_compare2 = lstm_project_atten(q1_compare1, q2_compare1)\n",
    "    \n",
    "    \n",
    "    # Aggregate\n",
    "    x = aggregate_both(q1_compare1, q2_compare1, num_class)#, q10_compare1, q20_compare1\n",
    "    #x1 = aggregate(q1_embed, q2_embed, q10_embed, q20_embed, num_class)#\n",
    "    #x = concatenate([x0, x1])\n",
    "    scores = Dense(num_class, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=[q1, q2, q1_com, q2_com], outputs=scores)#\n",
    "    return model\n",
    "\n",
    "def build_model(params, num_class=1,projection_dim=300, projection_hidden=200, projection_dropout=0.2,\n",
    "                           compare_dim=500, compare_dropout=0.2,\n",
    "                           dense_dim=300, dropout_rate=0.2,\n",
    "                           lr=1e-3, activation='relu'):\n",
    "    q1 = Input(name='q1',shape=(MAX_SEQUENCE_LENGTH_CHAR,), dtype='int32')\n",
    "    q2 = Input(name='q2',shape=(MAX_SEQUENCE_LENGTH_CHAR,), dtype='int32')\n",
    "    q10 = Input(name='q10',shape=(MAX_SEQUENCE_LENGTH_WORD, ), dtype='int32')\n",
    "    q20 = Input(name='q20',shape=(MAX_SEQUENCE_LENGTH_WORD, ), dtype='int32')\n",
    "    \n",
    "    #  #Embedding Embedding(VOC_SIZE_CHAR, 300, input_length = MAX_SEQUENCE_LENGTH_CHAR)  Embedding(VOC_SIZE_WORD, 300, input_length = MAX_SEQUENCE_LENGTH_WORD) StaticEmbedding(embedding_matrix_char)\n",
    "    encode = Embedding(VOC_SIZE_CHAR+2, 300, input_length = MAX_SEQUENCE_LENGTH_CHAR) # StaticEmbedding(embedding_matrix_char) #\n",
    "    q1_compare, q2_compare = embed_project_atten(q1, q2, encode)\n",
    "    \n",
    "    \n",
    "    encode = Embedding(VOC_SIZE_WORD+2, 300, input_length = MAX_SEQUENCE_LENGTH_WORD)  # StaticEmbedding(embedding_matrix_word) # \n",
    "    q10_compare, q20_compare = embed_project_atten(q10, q20, encode)\n",
    "    \n",
    "    \n",
    "    # Aggregate\n",
    "    scores = aggregate(q1_compare, q2_compare, q10_compare, q20_compare, num_class)#\n",
    "    \n",
    "    scores = Dense(num_class, activation='sigmoid')(scores)\n",
    "    model = Model(inputs=[q1, q2, q10, q20], outputs=scores)#\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_char_df = train_data_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1517 15 4 186 128 3 2 56 72 43 59</td>\n",
       "      <td>7 9 3 2 18 23 52 9 56 72 43 59 14 15 4 186 128...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>159 31 13 10 3 2 14 94 65 88 10 199 123 195 10</td>\n",
       "      <td>563 9 178 199 94 18 3 2 25 8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3 2 154 131 23 51 5 21 31 36 16</td>\n",
       "      <td>7 9 242 212 22 23 31 36 3 2 6 8 16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>76 84 259 226 68 95 6 2</td>\n",
       "      <td>91 459 141 68 95 6 2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3 2 230 59 25 34</td>\n",
       "      <td>179 270 59 230 680 22 23 11 3 2 16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                               1                                                  2  3\n",
       "0  1               1517 15 4 186 128 3 2 56 72 43 59  7 9 3 2 18 23 52 9 56 72 43 59 14 15 4 186 128...  1\n",
       "1  2  159 31 13 10 3 2 14 94 65 88 10 199 123 195 10                       563 9 178 199 94 18 3 2 25 8  0\n",
       "2  3                 3 2 154 131 23 51 5 21 31 36 16                 7 9 242 212 22 23 31 36 3 2 6 8 16  0\n",
       "3  4                         76 84 259 226 68 95 6 2                               91 459 141 68 95 6 2  0\n",
       "4  5                                3 2 230 59 25 34                 179 270 59 230 680 22 23 11 3 2 16  0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_char_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_squence_ques1_char = list(train_data_char_df.iloc[:, 1])\n",
    "word_squence_ques1_char = [[int(im) for im in item.split(' ')] for item in word_squence_ques1_char]\n",
    "word_squence_ques2_char = list(train_data_char_df.iloc[:, 2])\n",
    "word_squence_ques2_char = [[int(im) for im in item.split(' ')] for item in word_squence_ques2_char]\n",
    "\n",
    "# word_squence_ques1_word = list(train_data_word.iloc[:, 1])\n",
    "# word_squence_ques1_word = [[int(im) for im in item.split(' ')] for item in word_squence_ques1_word]\n",
    "# word_squence_ques2_word = list(train_data_word.iloc[:, 2])\n",
    "# word_squence_ques2_word = [[int(im) for im in item.split(' ')] for item in word_squence_ques2_word]\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH_WORD = 30 # char 40 word 30\n",
    "MAX_SEQUENCE_LENGTH_CHAR = 40 # char 40 word 30\n",
    "word_squence_ques1_char = pad_sequences(word_squence_ques1_char, maxlen=MAX_SEQUENCE_LENGTH_CHAR)\n",
    "word_squence_ques2_char = pad_sequences(word_squence_ques2_char, maxlen=MAX_SEQUENCE_LENGTH_CHAR)\n",
    "def cal_comman_sen(word_squence_ques1_char, word_squence_ques2_char):\n",
    "    comman_sen1 = []\n",
    "    comman_sen2 = []\n",
    "    for ques1, ques2 in zip(word_squence_ques1_char, word_squence_ques2_char):\n",
    "        comman_sen1.append([[1] if item in ques2 and item!=0 else [0] for item in ques1])\n",
    "        comman_sen2.append([[1] if item in ques1 and item!=0 else [0] for item in ques2])\n",
    "    return comman_sen1, comman_sen2\n",
    "comman_sen1, comman_sen2 = cal_comman_sen(word_squence_ques1_char, word_squence_ques2_char)\n",
    "# word_squence_ques1_word = pad_sequences(word_squence_ques1_word, maxlen=MAX_SEQUENCE_LENGTH_WORD)\n",
    "# word_squence_ques2_word = pad_sequences(word_squence_ques2_word, maxlen=MAX_SEQUENCE_LENGTH_WORD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting fold\n",
      "Train on 184456 samples, validate on 20498 samples\n",
      "Epoch 1/100\n",
      "  5632/184456 [..............................] - ETA: 3:46 - loss: 1.3841 - acc: 0.4361"
     ]
    }
   ],
   "source": [
    "class Metrics(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[:-3])))#\n",
    "        val_label = self.validation_data[-3]\n",
    "        for weight in range(10, 90, 5):\n",
    "            val_pred = [1 if item > weight*0.01 else 0 for item in val_predict]\n",
    "            _val_f1 = f1_score(val_label, val_pred)\n",
    "            print(\"epoch end \", weight*0.01, \", f1-score: \", _val_f1, \"precision: \", \n",
    "                  precision_score(val_label, val_pred, average='binary'), \"recall: \", \n",
    "                  recall_score(val_label, val_pred, average='binary'))\n",
    "metrics = Metrics()\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n",
    "X_train_q1 = word_squence_ques1_char\n",
    "X_train_q2 = word_squence_ques2_char\n",
    "X_train_q1_com = np.array(comman_sen1)\n",
    "X_train_q2_com = np.array(comman_sen2)\n",
    "y_train = train_data_char_df.iloc[:, 3]\n",
    "\n",
    "class_weight = {0: 1.,\n",
    "                1: 5.,}\n",
    "\n",
    "X_fold_val_q1_out,X_fold_val_q2_out, X_fold_val_magic_out = [0, 0, 0]\n",
    "\n",
    "def model_train(model_func, X_train_q1, X_train_q2, y_train, model_checkpoint_path):\n",
    "    NUM_FOLDS = 10\n",
    "    kfold = StratifiedKFold(\n",
    "        n_splits=NUM_FOLDS,\n",
    "        shuffle=True,\n",
    "        random_state=30\n",
    "    )\n",
    "    BATCH_SIZE = 256\n",
    "    MAX_EPOCHS = 100\n",
    "\n",
    "    for fold_num, (ix_train, ix_val) in enumerate(kfold.split(X_train_q1, y_train)):\n",
    "\n",
    "        # Augment the training set by mirroring the pairs.\n",
    "        X_fold_train_q1 = np.vstack([X_train_q1[ix_train], X_train_q2[ix_train]])\n",
    "        X_fold_train_q2 = np.vstack([X_train_q2[ix_train], X_train_q1[ix_train]])\n",
    "        X_fold_train_q1_com = np.vstack([X_train_q1_com[ix_train], X_train_q2_com[ix_train]])\n",
    "        X_fold_train_q2_com = np.vstack([X_train_q2_com[ix_train], X_train_q1_com[ix_train]])\n",
    "#         X_fold_train_q10 = np.vstack([X_train_q10[ix_train], X_train_q20[ix_train]])\n",
    "#         X_fold_train_q20 = np.vstack([X_train_q20[ix_train], X_train_q10[ix_train]])\n",
    "        #X_fold_train_magic = np.vstack([X_train_magic[ix_train], X_train_magic[ix_train]])\n",
    "\n",
    "        X_fold_val_q1 = np.vstack([X_train_q1[ix_val], X_train_q2[ix_val]])\n",
    "        X_fold_val_q2 = np.vstack([X_train_q2[ix_val], X_train_q1[ix_val]])\n",
    "        X_fold_val_q1_com = np.vstack([X_train_q1_com[ix_val], X_train_q2_com[ix_val]])\n",
    "        X_fold_val_q2_com = np.vstack([X_train_q2_com[ix_val], X_train_q1_com[ix_val]])\n",
    "#         X_fold_val_q10 = np.vstack([X_train_q10[ix_val], X_train_q20[ix_val]])\n",
    "#         X_fold_val_q20 = np.vstack([X_train_q20[ix_val], X_train_q10[ix_val]])\n",
    "        #X_fold_val_magic = np.vstack([X_train_magic[ix_val], X_train_magic[ix_val]])\n",
    "        \n",
    "        \n",
    "        # Ground truth should also be \"mirrored\".\n",
    "        y_fold_train = np.concatenate([y_train[ix_train], y_train[ix_train]])\n",
    "        y_fold_val = np.concatenate([y_train[ix_val], y_train[ix_val]])\n",
    "\n",
    "        print('Fitting fold')\n",
    "\n",
    "        # Compile a new model.\n",
    "        # model1 = create_model_cnn(model_params)\n",
    "        model = model_func(model_params)\n",
    "\n",
    "        model.compile(optimizer= keras.optimizers.Adam(lr = 0.001), loss=\"binary_crossentropy\", \n",
    "            metrics=['accuracy'])\n",
    "        # Train.\n",
    "        model.fit(\n",
    "            [X_fold_train_q1, X_fold_train_q2, \n",
    "             X_fold_train_q1_com, X_fold_train_q2_com,\n",
    "            # X_fold_train_q10, X_fold_train_q20, \n",
    "#              X_fold_train_char0_len, X_fold_train_char1_len,\n",
    "#              X_fold_train_word0_len, X_fold_train_word1_len,\n",
    "             # X_fold_train_magic\n",
    "            ], y_fold_train,#\n",
    "            validation_data=([\n",
    "                X_fold_val_q1, X_fold_val_q2, \n",
    "             X_fold_val_q1_com, X_fold_val_q2_com,\n",
    "                           #   X_fold_val_q10, X_fold_val_q20, \n",
    "#                               X_fold_val_char0_len, X_fold_val_char1_len,\n",
    "#                               X_fold_val_word0_len,X_fold_val_word1_len,\n",
    "                              # X_fold_val_magic\n",
    "                             ], y_fold_val),#\n",
    "\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=MAX_EPOCHS,\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            class_weight=class_weight,\n",
    "\n",
    "            callbacks=[\n",
    "                # Stop training when the validation loss stops improving.\n",
    "                EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    min_delta=0.001,\n",
    "                    patience=20,\n",
    "                    verbose=1,\n",
    "                    mode='auto',\n",
    "                ),\n",
    "                ReduceLROnPlateau(\n",
    "                    monitor='val_loss', \n",
    "                    factor=0.1, \n",
    "                    patience=5, \n",
    "                    min_delta=0.0001,\n",
    "                    cooldown=1, \n",
    "                ),\n",
    "                # Save the weights of the best epoch.\n",
    "                ModelCheckpoint(\n",
    "                    os.path.join(model_checkpoint_path + str(fold_num) + 'weights.{epoch:02d}.hdf5'),\n",
    "                    monitor='val_loss',\n",
    "                    save_best_only=False,\n",
    "                    verbose=2,\n",
    "                    mode='auto',\n",
    "                ),\n",
    "                metrics\n",
    "            ],\n",
    "        )\n",
    "        break\n",
    "    \n",
    "    # Clear GPU memory.\n",
    "    K.clear_session()\n",
    "    del X_fold_train_q1, X_fold_train_q2, X_fold_train_magic\n",
    "    del X_fold_val_q1, X_fold_val_q2, X_fold_val_magic\n",
    "    del model\n",
    "    gc.collect()\n",
    "    # return score\n",
    "\n",
    "model_path = '../data/checkpoints/test/'\n",
    "# model_train(create_model, X_train_q1, X_train_q2, X_train_magic, y_train, model_path)\n",
    "\n",
    "\n",
    "model_params = {\n",
    "    'dense_dropout_rate': 0.3,\n",
    "    'num_dense': 150,\n",
    "    'num_lstm': 128,\n",
    "    'num_filters':32\n",
    "}\n",
    "\n",
    "model_train(build_model_combine, X_train_q1, X_train_q2, y_train, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    83792\n",
       "1    18684\n",
       "Name: 1.1, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102476, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_char_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH_CHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH_WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " '4 22 21 2 29 8 4 16 24 4 17 25 5 11 12 2 6 7 50 29 32 2 45 12 5 19 57 3 62 51 4 17 25 50 29 21 2 46 32 4 17 60 4 17 25',\n",
       " '10 20 21 10 13 38 2 14 68 50 29 21 2 45 12 3 14 23 5 11 12 2 6 7 4 18 14 3 35 30',\n",
       " 0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_data_char_df.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>﻿怎么更改花呗手机号码</th>\n",
       "      <th>我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号</th>\n",
       "      <th>1.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>也开不了花呗，就这样了？完事了</td>\n",
       "      <td>真的嘛？就是花呗付款</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>花呗冻结以后还能开通吗</td>\n",
       "      <td>我的条件可以开通花呗借款吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>如何得知关闭借呗</td>\n",
       "      <td>想永久关闭借呗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>花呗扫码付钱</td>\n",
       "      <td>二维码扫描可以用花呗吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>花呗逾期后不能分期吗</td>\n",
       "      <td>我这个 逾期后还完了 最低还款 后 能分期吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1      ﻿怎么更改花呗手机号码 我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号  1.1\n",
       "0  2  也开不了花呗，就这样了？完事了                     真的嘛？就是花呗付款    0\n",
       "1  3      花呗冻结以后还能开通吗                  我的条件可以开通花呗借款吗    0\n",
       "2  4         如何得知关闭借呗                        想永久关闭借呗    0\n",
       "3  5           花呗扫码付钱                    二维码扫描可以用花呗吗    0\n",
       "4  6       花呗逾期后不能分期吗         我这个 逾期后还完了 最低还款 后 能分期吗    0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_char.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
