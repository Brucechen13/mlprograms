{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import sys\n",
    "import gensim\n",
    "import time\n",
    "import keras\n",
    "from string import punctuation\n",
    "from tflearn.data_utils import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merged_lstm():\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            embedding_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            input_length=seq_length,\n",
    "            trainable=False)\n",
    "    \n",
    "    lstm_layer = LSTM(128, dropout=0.25, recurrent_dropout=0.2,\n",
    "                     go_backwards = False, implementation = 2)\n",
    "\n",
    "    sequence_1_input = Input(shape=(seq_length,), dtype='int32')\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "    sequence_2_input = Input(shape=(seq_length,), dtype='int32')\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "    dense_input = Input(shape = (ncols,))\n",
    "    d = Dense(256, kernel_initializer = 'he_normal')(dense_input)\n",
    "    d = PReLU()(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Dropout(0.4)(d)\n",
    "    \n",
    "    d2 = Dense(512, kernel_initializer = 'he_normal')(d)\n",
    "    d2 = PReLU()(d2)\n",
    "    d2 = BatchNormalization()(d2)\n",
    "    d2 = Dropout(0.2)(d2)\n",
    "    \n",
    "    d3 = Dense(512, kernel_initializer = 'he_normal')(d2)\n",
    "    d3 = PReLU()(d3)\n",
    "    d3 = Dropout(0.2)(d3)\n",
    "    \n",
    "    merged = concatenate([x1, y1, d3])\n",
    "    merged = Dropout(0.25)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    merged = Dense(256)(merged)\n",
    "    merged = PReLU()(merged)\n",
    "    merged = Dropout(0.25)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    preds = Dense(1, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input, dense_input], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98976, 2)\n",
      "(98976, 16)\n",
      "(98976, 5)\n",
      "(98976, 2)\n",
      "(98976, 16)\n",
      "(98976, 5)\n",
      "(98976, 4)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 30\n",
    "embedding_dim = 50\n",
    "nb_words = 5000 + 2\n",
    "\n",
    "\n",
    "word_embedding_matrix = np.load('../data/aux/fastvec_word.npy')\n",
    "\n",
    "train_data_word = pd.read_csv('../data/aux/train_word_indexvec.csv')\n",
    "\n",
    "\n",
    "word_squence_ques1_word = list(train_data_word.iloc[:, 1])\n",
    "word_squence_ques1_word = [[int(im) for im in item.split(' ')] for item in word_squence_ques1_word]\n",
    "word_squence_ques2_word = list(train_data_word.iloc[:, 2])\n",
    "word_squence_ques2_word = [[int(im) for im in item.split(' ')] for item in word_squence_ques2_word]\n",
    "\n",
    "MAX_SEQUENCE_LENGTH_WORD = 30 # char 40 word 30\n",
    "word_squence_ques1_word = pad_sequences(word_squence_ques1_word, maxlen=MAX_SEQUENCE_LENGTH_WORD)\n",
    "word_squence_ques2_word = pad_sequences(word_squence_ques2_word, maxlen=MAX_SEQUENCE_LENGTH_WORD)\n",
    "\n",
    "q1 = word_squence_ques1_word\n",
    "q2 = word_squence_ques2_word\n",
    "y = train_data_word.iloc[:, 3]\n",
    "\n",
    "lda_feas_char = pd.read_csv('../lda_features_char.csv')\n",
    "lda_feas_char = lda_feas_char.values\n",
    "print(lda_feas_char.shape)\n",
    "ngram_feas_char = pd.read_csv('../ngram_features_char.csv')\n",
    "ngram_feas_char = ngram_feas_char.values\n",
    "print(ngram_feas_char.shape)\n",
    "simsummary_feas_char = pd.read_csv('../simsummary_features_char.csv')\n",
    "simsummary_feas_char = simsummary_feas_char.values\n",
    "print(simsummary_feas_char.shape)\n",
    "lda_feas_word = pd.read_csv('../lda_features_word.csv')\n",
    "lda_feas_word = lda_feas_word.values\n",
    "print(lda_feas_word.shape)\n",
    "ngram_feas_word = pd.read_csv('../ngram_features_word.csv')\n",
    "ngram_feas_word = ngram_feas_word.values\n",
    "print(ngram_feas_word.shape)\n",
    "simsummary_feas_word = pd.read_csv('../simsummary_features_word.csv')\n",
    "simsummary_feas_word = simsummary_feas_word.values\n",
    "print(simsummary_feas_word.shape)\n",
    "tfidf_feas = pd.read_csv('../tfidf_features.csv')\n",
    "tfidf_feas = tfidf_feas.values\n",
    "print(tfidf_feas.shape)\n",
    "all_feas = np.concatenate([ngram_feas_char,lda_feas_char, simsummary_feas_char, tfidf_feas], axis=1)\n",
    "X_train = all_feas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[:-3])))#\n",
    "        val_predict = [1 if item > 0.25 else 0 for item in val_predict]\n",
    "        val_targ = self.validation_data[-3]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        print \"epoch end, f1-score: \", _val_f1\n",
    "metrics = Metrics()\n",
    "\n",
    "def lstm_foldrun(X, q1, q2, y, X_test = None, q1_test = None, q2_test = None, start_fold = 0,\n",
    "                name = 'LSTM_merged866cols', save = True):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits = 10, random_state = 111, shuffle = True)\n",
    "    if isinstance(X, pd.core.frame.DataFrame):\n",
    "        X = X.values\n",
    "    if isinstance(X_test, pd.core.frame.DataFrame):\n",
    "        X_test = X_test.values\n",
    "    if isinstance(y, pd.core.frame.DataFrame):\n",
    "        y = y.is_duplicate.values\n",
    "    if isinstance(y, pd.core.frame.Series):\n",
    "        y = y.values\n",
    "    \n",
    "    i = 0\n",
    "    losses = []\n",
    "    train_splits = []\n",
    "    val_splits = []\n",
    "    for tr_index, val_index in skf.split(X, y):\n",
    "        train_splits.append(tr_index)\n",
    "        val_splits.append(val_index)\n",
    "    for i in range(start_fold, 10):\n",
    "        X_tr, X_val = X[train_splits[i]], X[val_splits[i]]\n",
    "        q1_tr, q1_val = q1[train_splits[i]], q1[val_splits[i]]\n",
    "        q2_tr, q2_val = q2[train_splits[i]], q2[val_splits[i]]\n",
    "        y_tr, y_val = y[train_splits[i]], y[val_splits[i]]\n",
    "\n",
    "        t = time.time()\n",
    "        print('Start training on fold: {}'.format(i))\n",
    "        callbacks = [ModelCheckpoint('../data/checkpoints/{}_fold{}.h5'.format(name, name, i),\n",
    "                                    monitor='val_loss', \n",
    "                                    verbose = 0, save_best_only = True),\n",
    "                 EarlyStopping(monitor='val_loss', patience = 7, verbose = 1), metrics]\n",
    "        \n",
    "        model = merged_lstm()\n",
    "        model.fit([q1_tr, q2_tr, X_tr], y_tr, validation_data=([q1_val, q2_val, X_val], y_val),\n",
    "                epochs=200, batch_size=512, callbacks = callbacks)\n",
    "        ,class_weight = {0: 1.,1: 5,}\n",
    "        \n",
    "        val_pred = model.predict([q1_val, q2_val, X_val], batch_size = 64)\n",
    "        \n",
    "        val_predict = [1 if item > 0.25 else 0 for item in val_pred]\n",
    "        print 'val score', f1_score(y_val, val_predict)\n",
    "        \n",
    "        del X_tr, X_val, q1_tr, q1_val, q2_tr, q2_val\n",
    "        gc.collect()\n",
    "        i += 1\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training on fold: 9\n",
      "Train on 89079 samples, validate on 9897 samples\n",
      "Epoch 1/200\n",
      "89079/89079 [==============================] - 16s 176us/step - loss: 0.5475 - acc: 0.7484 - val_loss: 0.4376 - val_acc: 0.8136\n",
      "epoch end, f1-score:  0.4266949152542373\n",
      "Epoch 2/200\n",
      "89079/89079 [==============================] - 12s 137us/step - loss: 0.4490 - acc: 0.8155 - val_loss: 0.4207 - val_acc: 0.8265\n",
      "epoch end, f1-score:  0.43754478146644377\n",
      "Epoch 3/200\n",
      "89079/89079 [==============================] - 13s 146us/step - loss: 0.4321 - acc: 0.8206 - val_loss: 0.4176 - val_acc: 0.8262\n",
      "epoch end, f1-score:  0.43645213628988644\n",
      "Epoch 4/200\n",
      "89079/89079 [==============================] - 13s 143us/step - loss: 0.4240 - acc: 0.8224 - val_loss: 0.4176 - val_acc: 0.8261\n",
      "epoch end, f1-score:  0.4384432088959491\n",
      "Epoch 5/200\n",
      "89079/89079 [==============================] - 13s 144us/step - loss: 0.4198 - acc: 0.8239 - val_loss: 0.4165 - val_acc: 0.8267\n",
      "epoch end, f1-score:  0.43902439024390244\n",
      "Epoch 6/200\n",
      "89079/89079 [==============================] - 13s 147us/step - loss: 0.4171 - acc: 0.8248 - val_loss: 0.4118 - val_acc: 0.8282\n",
      "epoch end, f1-score:  0.44610202117420594\n",
      "Epoch 7/200\n",
      "89079/89079 [==============================] - 13s 145us/step - loss: 0.4148 - acc: 0.8243 - val_loss: 0.4105 - val_acc: 0.8273\n",
      "epoch end, f1-score:  0.44619836885030256\n",
      "Epoch 8/200\n",
      "89079/89079 [==============================] - 13s 147us/step - loss: 0.4134 - acc: 0.8247 - val_loss: 0.4087 - val_acc: 0.8263\n",
      "epoch end, f1-score:  0.4484611626770884\n",
      "Epoch 9/200\n",
      "89079/89079 [==============================] - 13s 148us/step - loss: 0.4117 - acc: 0.8254 - val_loss: 0.4102 - val_acc: 0.8258\n",
      "epoch end, f1-score:  0.4599275070479259\n",
      "Epoch 10/200\n",
      "89079/89079 [==============================] - 13s 145us/step - loss: 0.4093 - acc: 0.8260 - val_loss: 0.4058 - val_acc: 0.8270\n",
      "epoch end, f1-score:  0.4612628750559785\n",
      "Epoch 11/200\n",
      "89079/89079 [==============================] - 13s 141us/step - loss: 0.4087 - acc: 0.8251 - val_loss: 0.4068 - val_acc: 0.8286\n",
      "epoch end, f1-score:  0.4512771996215705\n",
      "Epoch 12/200\n",
      "89079/89079 [==============================] - 13s 146us/step - loss: 0.4049 - acc: 0.8267 - val_loss: 0.4092 - val_acc: 0.8262\n",
      "epoch end, f1-score:  0.4513681929016527\n",
      "Epoch 13/200\n",
      "89079/89079 [==============================] - 12s 138us/step - loss: 0.4043 - acc: 0.8269 - val_loss: 0.4106 - val_acc: 0.8272\n",
      "epoch end, f1-score:  0.44425675675675674\n",
      "Epoch 14/200\n",
      "89079/89079 [==============================] - 13s 143us/step - loss: 0.4032 - acc: 0.8278 - val_loss: 0.4022 - val_acc: 0.8271\n",
      "epoch end, f1-score:  0.46424328819196964\n",
      "Epoch 15/200\n",
      "89079/89079 [==============================] - 12s 137us/step - loss: 0.4014 - acc: 0.8272 - val_loss: 0.4023 - val_acc: 0.8285\n",
      "epoch end, f1-score:  0.4634713820381573\n",
      "Epoch 16/200\n",
      "89079/89079 [==============================] - 13s 141us/step - loss: 0.3999 - acc: 0.8281 - val_loss: 0.4031 - val_acc: 0.8284\n",
      "epoch end, f1-score:  0.4611138986452584\n",
      "Epoch 17/200\n",
      "89079/89079 [==============================] - 13s 143us/step - loss: 0.3980 - acc: 0.8286 - val_loss: 0.4020 - val_acc: 0.8282\n",
      "epoch end, f1-score:  0.4674035641777577\n",
      "Epoch 18/200\n",
      "89079/89079 [==============================] - 13s 146us/step - loss: 0.3961 - acc: 0.8295 - val_loss: 0.4026 - val_acc: 0.8260\n",
      "epoch end, f1-score:  0.4614624505928854\n",
      "Epoch 19/200\n",
      "89079/89079 [==============================] - 13s 149us/step - loss: 0.3952 - acc: 0.8294 - val_loss: 0.4030 - val_acc: 0.8271\n",
      "epoch end, f1-score:  0.45758100420479847\n",
      "Epoch 20/200\n",
      "89079/89079 [==============================] - 13s 145us/step - loss: 0.3944 - acc: 0.8308 - val_loss: 0.4013 - val_acc: 0.8279\n",
      "epoch end, f1-score:  0.46142754145638065\n",
      "Epoch 21/200\n",
      "89079/89079 [==============================] - 13s 143us/step - loss: 0.3922 - acc: 0.8306 - val_loss: 0.4000 - val_acc: 0.8298\n",
      "epoch end, f1-score:  0.4643759474482062\n",
      "Epoch 22/200\n",
      "89079/89079 [==============================] - 13s 147us/step - loss: 0.3912 - acc: 0.8303 - val_loss: 0.3997 - val_acc: 0.8265\n",
      "epoch end, f1-score:  0.4675086107921929\n",
      "Epoch 23/200\n",
      "89079/89079 [==============================] - 13s 147us/step - loss: 0.3895 - acc: 0.8319 - val_loss: 0.3981 - val_acc: 0.8281\n",
      "epoch end, f1-score:  0.4714640198511166\n",
      "Epoch 24/200\n",
      "89079/89079 [==============================] - 13s 144us/step - loss: 0.3883 - acc: 0.8318 - val_loss: 0.4041 - val_acc: 0.8296\n",
      "epoch end, f1-score:  0.46403835908364416\n",
      "Epoch 25/200\n",
      "89079/89079 [==============================] - 13s 148us/step - loss: 0.3866 - acc: 0.8333 - val_loss: 0.4023 - val_acc: 0.8292\n",
      "epoch end, f1-score:  0.4608945854041329\n",
      "Epoch 26/200\n",
      "89079/89079 [==============================] - 13s 141us/step - loss: 0.3847 - acc: 0.8325 - val_loss: 0.4035 - val_acc: 0.8317\n",
      "epoch end, f1-score:  0.4667673716012085\n",
      "Epoch 27/200\n",
      "89079/89079 [==============================] - 13s 144us/step - loss: 0.3830 - acc: 0.8335 - val_loss: 0.3994 - val_acc: 0.8274\n",
      "epoch end, f1-score:  0.4663449326898654\n",
      "Epoch 28/200\n",
      "89079/89079 [==============================] - 12s 139us/step - loss: 0.3825 - acc: 0.8345 - val_loss: 0.3997 - val_acc: 0.8278\n",
      "epoch end, f1-score:  0.46525529265255294\n",
      "Epoch 29/200\n",
      "89079/89079 [==============================] - 13s 143us/step - loss: 0.3805 - acc: 0.8343 - val_loss: 0.3979 - val_acc: 0.8288\n",
      "epoch end, f1-score:  0.4670075480886292\n",
      "Epoch 30/200\n",
      "89079/89079 [==============================] - 12s 139us/step - loss: 0.3790 - acc: 0.8362 - val_loss: 0.4019 - val_acc: 0.8281\n",
      "epoch end, f1-score:  0.46652719665271963\n",
      "Epoch 31/200\n",
      "89079/89079 [==============================] - 12s 139us/step - loss: 0.3782 - acc: 0.8347 - val_loss: 0.3995 - val_acc: 0.8281\n",
      "epoch end, f1-score:  0.4669459962756052\n",
      "Epoch 32/200\n",
      "89079/89079 [==============================] - 12s 139us/step - loss: 0.3770 - acc: 0.8363 - val_loss: 0.4005 - val_acc: 0.8273\n",
      "epoch end, f1-score:  0.47984224364592465\n",
      "Epoch 33/200\n",
      "89079/89079 [==============================] - 13s 142us/step - loss: 0.3757 - acc: 0.8367 - val_loss: 0.3975 - val_acc: 0.8306\n",
      "epoch end, f1-score:  0.4671496377716712\n",
      "Epoch 34/200\n",
      "89079/89079 [==============================] - 13s 145us/step - loss: 0.3737 - acc: 0.8371 - val_loss: 0.4065 - val_acc: 0.8284\n",
      "epoch end, f1-score:  0.4650053022269353\n",
      "Epoch 35/200\n",
      "89079/89079 [==============================] - 13s 149us/step - loss: 0.3721 - acc: 0.8375 - val_loss: 0.3989 - val_acc: 0.8296\n",
      "epoch end, f1-score:  0.47454503570605844\n",
      "Epoch 36/200\n",
      "89079/89079 [==============================] - 13s 144us/step - loss: 0.3707 - acc: 0.8389 - val_loss: 0.4015 - val_acc: 0.8262\n",
      "epoch end, f1-score:  0.4722662440570523\n",
      "Epoch 37/200\n",
      "89079/89079 [==============================] - 13s 145us/step - loss: 0.3688 - acc: 0.8394 - val_loss: 0.4004 - val_acc: 0.8285\n",
      "epoch end, f1-score:  0.47003745318352064\n",
      "Epoch 38/200\n",
      "89079/89079 [==============================] - 13s 143us/step - loss: 0.3674 - acc: 0.8395 - val_loss: 0.4001 - val_acc: 0.8256\n",
      "epoch end, f1-score:  0.4818718381112985\n",
      "Epoch 39/200\n",
      "89079/89079 [==============================] - 13s 145us/step - loss: 0.3669 - acc: 0.8387 - val_loss: 0.3978 - val_acc: 0.8309\n",
      "epoch end, f1-score:  0.4750830564784053\n",
      "Epoch 40/200\n",
      "89079/89079 [==============================] - 13s 143us/step - loss: 0.3636 - acc: 0.8422 - val_loss: 0.4029 - val_acc: 0.8265\n",
      "epoch end, f1-score:  0.47368421052631576\n",
      "Epoch 00040: early stopping\n",
      "val score 0.47368421052631576\n"
     ]
    }
   ],
   "source": [
    "ncols = X_train.shape[1]\n",
    "lstm_foldrun(X_train, q1, q2, y, start_fold = 9, name = 'LSTM_merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
