{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenchi/anaconda3/envs/py27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tflearn.data_utils import pad_sequences\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras import initializers\n",
    "import os\n",
    "import keras\n",
    "\n",
    "from keras.optimizers import Adadelta\n",
    "import gc\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "VOCAB_LENGTH = 3000\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0      1      2      3\n",
      "count   98976  98976  98976  98976\n",
      "unique      1      1      1      1\n",
      "top     False  False  False  False\n",
      "freq    98976  98976  98976  98976\n",
      "(98976, 2)\n",
      "(98976, 16)\n",
      "(98976, 5)\n",
      "(98976, 2)\n",
      "(98976, 16)\n",
      "(98976, 5)\n",
      "(98976, 4)\n",
      "(98976, 27)\n"
     ]
    }
   ],
   "source": [
    "train_data_word = pd.read_csv('../data/aux/train_word_indexvec.csv')\n",
    "train_data_char = pd.read_csv('../data/aux/train_char_indexvec.csv')\n",
    "train_data = train_data_word\n",
    "train_data.head()\n",
    "nan_test = pd.isnull(train_data)\n",
    "print nan_test.describe()\n",
    "\n",
    "lda_feas_char = pd.read_csv('../lda_features_char.csv')\n",
    "lda_feas_char = lda_feas_char.values\n",
    "print(lda_feas_char.shape)\n",
    "ngram_feas_char = pd.read_csv('../ngram_features_char.csv')\n",
    "ngram_feas_char = ngram_feas_char.values\n",
    "print(ngram_feas_char.shape)\n",
    "simsummary_feas_char = pd.read_csv('../simsummary_features_char.csv')\n",
    "simsummary_feas_char = simsummary_feas_char.values\n",
    "print(simsummary_feas_char.shape)\n",
    "lda_feas_word = pd.read_csv('../lda_features_word.csv')\n",
    "lda_feas_word = lda_feas_word.values\n",
    "print(lda_feas_word.shape)\n",
    "ngram_feas_word = pd.read_csv('../ngram_features_word.csv')\n",
    "ngram_feas_word = ngram_feas_word.values\n",
    "print(ngram_feas_word.shape)\n",
    "simsummary_feas_word = pd.read_csv('../simsummary_features_word.csv')\n",
    "simsummary_feas_word = simsummary_feas_word.values\n",
    "print(simsummary_feas_word.shape)\n",
    "tfidf_feas = pd.read_csv('../tfidf_features.csv')\n",
    "tfidf_feas = tfidf_feas.values\n",
    "print(tfidf_feas.shape)\n",
    "\n",
    "all_feas = np.concatenate([ngram_feas_char,lda_feas_char, simsummary_feas_char, tfidf_feas], axis=1)\n",
    "\n",
    "# ngram_feas_word, lda_feas_word, simsummary_feas_word\n",
    "                           \n",
    "print all_feas.shape\n",
    "\n",
    "\n",
    "embedding_matrix_char = np.load('../data/aux/vec_char.npy')\n",
    "\n",
    "embedding_matrix_word = np.load('../data/aux/vec_word.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VOC_SIZE_CHAR = 2087\n",
    "VOC_SIZE_WORD = 5002\n",
    "from keras.layers import *\n",
    "from keras.activations import softmax\n",
    "from keras.models import Model\n",
    "\n",
    "def get_embedding_output(input_x1, input_x2, params, MAX_SEQUENCE_LENGTH):\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        VOCAB_LENGTH,\n",
    "        EMBEDDING_DIM,\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False,\n",
    "        mask_zero=True, \n",
    "    )\n",
    "    embedding_seq1 = embedding_layer(input_x1)\n",
    "    embedding_seq2 = embedding_layer(input_x2)\n",
    "    return embedding_seq1, embedding_seq2\n",
    "\n",
    "def get_embedding_output_vec(input_x1, input_x2, params, MAX_SEQUENCE_LENGTH, embedding_matrix, vocab_len):\n",
    "    embedder = Embedding(vocab_len, 300, input_length = MAX_SEQUENCE_LENGTH, \n",
    "                         weights = [embedding_matrix], mask_zero=True, trainable = False)\n",
    "    embedding_seq1 = embedder(input_x1)\n",
    "    embedding_seq2 = embedder(input_x2)\n",
    "    return embedding_seq1, embedding_seq2\n",
    "\n",
    "num_stack = 3\n",
    "def get_bilstm_output(input_x1, input_x2, params):\n",
    "    \n",
    "    lstm_layer = keras.layers.Bidirectional(keras.layers.GRU(\n",
    "        params['num_lstm'],\n",
    "        dropout=params['lstm_dropout_rate'],\n",
    "        recurrent_dropout=params['lstm_dropout_rate'],\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    x1 = lstm_layer(input_x1)\n",
    "    x2 = lstm_layer(input_x2)\n",
    "    return x1, x2;\n",
    "\n",
    "def get_cnn_output(input_x1, params):\n",
    "    # cnn0模块，kernel_size = 2\n",
    "    conv0_1 = Convolution1D(256, 2, padding='same')(input_x1)\n",
    "    bn0_1 = BatchNormalization()(conv0_1)\n",
    "    relu0_1 = Activation('relu')(bn0_1)\n",
    "    conv0_2 = Convolution1D(128, 2, padding='same')(relu0_1)\n",
    "    bn0_2 = BatchNormalization()(conv0_2)\n",
    "    relu0_2 = Activation('relu')(bn0_2)\n",
    "    cnn0 = MaxPool1D(pool_size=4)(relu0_2)\n",
    "    # cnn1模块，kernel_size = 3\n",
    "    conv1_1 = Convolution1D(256, 3, padding='same')(input_x1)\n",
    "    bn1_1 = BatchNormalization()(conv1_1)\n",
    "    relu1_1 = Activation('relu')(bn1_1)\n",
    "    conv1_2 = Convolution1D(128, 3, padding='same')(relu1_1)\n",
    "    bn1_2 = BatchNormalization()(conv1_2)\n",
    "    relu1_2 = Activation('relu')(bn1_2)\n",
    "    cnn1 = MaxPool1D(pool_size=4)(relu1_2)\n",
    "    # cnn2模块，kernel_size = 4\n",
    "    conv2_1 = Convolution1D(256, 4, padding='same')(input_x1)\n",
    "    bn2_1 = BatchNormalization()(conv2_1)\n",
    "    relu2_1 = Activation('relu')(bn2_1)\n",
    "    conv2_2 = Convolution1D(128, 4, padding='same')(relu2_1)\n",
    "    bn2_2 = BatchNormalization()(conv2_2)\n",
    "    relu2_2 = Activation('relu')(bn2_2)\n",
    "    cnn2 = MaxPool1D(pool_size=4)(relu2_2)\n",
    "    # cnn3模块，kernel_size = 5\n",
    "    conv3_1 = Convolution1D(256, 5, padding='same')(input_x1)\n",
    "    bn3_1 = BatchNormalization()(conv3_1)\n",
    "    relu3_1 = Activation('relu')(bn3_1)\n",
    "    conv3_2 = Convolution1D(128, 5, padding='same')(relu3_1)\n",
    "    bn3_2 = BatchNormalization()(conv3_2)\n",
    "    relu3_2 = Activation('relu')(bn3_2)\n",
    "    cnn3 = MaxPool1D(pool_size=4)(relu3_2)\n",
    "    # 拼接三个模块\n",
    "    cnn = concatenate([cnn0,cnn1,cnn2,cnn3], axis=-1)\n",
    "    return cnn\n",
    "\n",
    "class MulAttLayer(Layer):\n",
    "    def __init__(self, max_seq_len, hidden_units=150, **kwargs):\n",
    "        self.dense_layer = TimeDistributed(Dense(hidden_units, activation='softmax', kernel_initializer='zero'))\n",
    "        self.max_seq_len = max_seq_len;\n",
    "        super(MulAttLayer, self).__init__(** kwargs)\n",
    "        \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        a, b, a_lengths, b_lengths = inputs\n",
    "        a_norm = tf.nn.l2_normalize(a, axis=2)\n",
    "        b_norm = tf.nn.l2_normalize(b, axis=2)\n",
    "        logits = tf.matmul(a_norm, tf.transpose(b_norm, (0, 2, 1)))\n",
    "        logits = logits - tf.expand_dims(tf.reduce_max(logits, axis=2), 2)\n",
    "        attn = tf.exp(logits)\n",
    "        #attn = mask_attention_weights(attn, a_lengths, b_lengths, self.max_seq_len)\n",
    "        \n",
    "        a_mask = tf.expand_dims(tf.sequence_mask(a_lengths, maxlen=self.max_seq_len), 2)\n",
    "        b_mask = tf.expand_dims(tf.sequence_mask(b_lengths, maxlen=self.max_seq_len), 1)\n",
    "        a_mask = tf.reshape(a_mask, shape=(-1, self.max_seq_len, 1))\n",
    "        b_mask = tf.reshape(b_mask, shape=(-1, 1, self.max_seq_len))\n",
    "        seq_mask = tf.cast(tf.matmul(tf.cast(a_mask, tf.int32), tf.cast(b_mask, tf.int32)), tf.bool)\n",
    "        attn = tf.where(seq_mask, attn, tf.zeros_like(attn))\n",
    "        attn = attn / tf.expand_dims(tf.reduce_sum(attn, axis=2) + 1e-10, 2)\n",
    "        #attn = attention_func(a, b, a_lengths, b_lengths, max_seq_len, **attention_func_kwargs)\n",
    "        return tf.reduce_max(tf.einsum('ijk,ikl->ijkl', attn, b), axis=2)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], self.max_seq_len, input_shape[0][2])\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, init='glorot_uniform', kernel_regularizer=None, \n",
    "                 bias_regularizer=None, kernel_constraint=None, \n",
    "                 bias_constraint=None,  **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get(init)\n",
    "        self.kernel_initializer = initializers.get(init)\n",
    "        \n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(kernel_regularizer)\n",
    "        \n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        \n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = self.add_weight((input_shape[-1], 1),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.kernel_regularizer,\n",
    "                                 constraint=self.kernel_constraint)\n",
    "        self.b = self.add_weight((input_shape[1],),\n",
    "                                 initializer='zero',\n",
    "                                 name='{}_b'.format(self.name),\n",
    "                                 regularizer=self.bias_regularizer,\n",
    "                                 constraint=self.bias_constraint)\n",
    "        self.u = self.add_weight((input_shape[1],),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.kernel_regularizer,\n",
    "                                 constraint=self.kernel_constraint)\n",
    "        \n",
    "        self.built = True\n",
    "        \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W) # (x, 40, 1)\n",
    "        uit = K.squeeze(uit, -1) # (x, 40)\n",
    "        uit = uit + self.b # (x, 40) + (40,)\n",
    "        uit = K.tanh(uit) # (x, 40)\n",
    "\n",
    "        ait = uit * self.u # (x, 40) * (40, 1) => (x, 1)\n",
    "        ait = K.exp(ait) # (X, 1)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx()) #(x, 40)\n",
    "            ait = mask*ait #(x, 40) * (x, 40, )\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "\n",
    "### LSTM + FEATURES\n",
    "def create_model(params):\n",
    "    sequence_1_input_char = keras.Input(shape=(MAX_SEQUENCE_LENGTH_CHAR, ), dtype='int32')\n",
    "    sequence_2_input_char = keras.Input(shape=(MAX_SEQUENCE_LENGTH_CHAR, ), dtype='int32')\n",
    "    sequence_1_input_word = keras.Input(shape=(MAX_SEQUENCE_LENGTH_WORD, ), dtype='int32')\n",
    "    sequence_2_input_word = keras.Input(shape=(MAX_SEQUENCE_LENGTH_WORD, ), dtype='int32')\n",
    "    sequence_1_input_char_len = keras.Input(shape=(1,), dtype='int32')\n",
    "    sequence_2_input_char_len = keras.Input(shape=(1,), dtype='int32')\n",
    "    sequence_1_input_word_len = keras.Input(shape=(1,), dtype='int32')\n",
    "    sequence_2_input_word_len = keras.Input(shape=(1,), dtype='int32')\n",
    "    magic_input = keras.Input(shape=(all_feas.shape[-1], ))\n",
    "    \n",
    "    embedding_seq1_char, embedding_seq2_char = get_embedding_output_vec(sequence_1_input_char,\n",
    "                                                          sequence_2_input_char, params, MAX_SEQUENCE_LENGTH_CHAR,\n",
    "                                                                   embedding_matrix_char, 2087)\n",
    "    \n",
    "    embedding_seq1_word, embedding_seq2_word = get_embedding_output_vec(\n",
    "        sequence_1_input_word, sequence_2_input_word, params, MAX_SEQUENCE_LENGTH_WORD, embedding_matrix_word, 5002)\n",
    "    \n",
    "#     x1,x2 = get_bilstm_output(, params)\n",
    "    x1, x2 = embedding_seq1_char, embedding_seq2_char\n",
    "    \n",
    "    \n",
    "    concatenate_layer = keras.layers.Concatenate()\n",
    "    \n",
    "    rnn_x1 = MulAttLayer(MAX_SEQUENCE_LENGTH_CHAR)([x1, x2, sequence_1_input_char_len, \n",
    "                            sequence_2_input_char_len])\n",
    "    att_layer0 = keras.layers.Bidirectional(keras.layers.GRU(\n",
    "        params['num_lstm'],\n",
    "        dropout=params['lstm_dropout_rate'],\n",
    "        recurrent_dropout=params['lstm_dropout_rate'],\n",
    "        return_sequences=False, return_state=False\n",
    "    ))\n",
    "    rnn_x1 = att_layer0(rnn_x1)\n",
    "    \n",
    "    \n",
    "    #x1,x2 = get_bilstm_output(embedding_seq1_word, embedding_seq2_word, params)\n",
    "    x1, x2 = embedding_seq1_word, embedding_seq2_word\n",
    "    rnn_x2 = MulAttLayer(MAX_SEQUENCE_LENGTH_WORD)([x1, x2, sequence_1_input_word_len, \n",
    "                            sequence_2_input_word_len])\n",
    "    \n",
    "    att_layer1 = keras.layers.Bidirectional(keras.layers.GRU(\n",
    "        params['num_lstm'],\n",
    "        dropout=params['lstm_dropout_rate'],\n",
    "        recurrent_dropout=params['lstm_dropout_rate'],\n",
    "        return_sequences=False, return_state=False\n",
    "    ))\n",
    "    rnn_x2 = att_layer1(rnn_x2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #magic_features = keras.layers.Dense(128, activation='relu')(magic_input)\n",
    "    merged = concatenate_layer([rnn_x1, rnn_x2, magic_input])#\n",
    "    merged = keras.layers.Dropout(params['dense_dropout_rate'])(merged)\n",
    "    merged = keras.layers.BatchNormalization()(merged)\n",
    "\n",
    "    merged = keras.layers.Dense(params['num_dense'], activation='relu')(merged)\n",
    "    merged = keras.layers.Dropout(params['dense_dropout_rate'])(merged)\n",
    "    merged = keras.layers.BatchNormalization()(merged)\n",
    "\n",
    "    output = keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[sequence_1_input_char, sequence_2_input_char, \n",
    "                sequence_1_input_word, sequence_2_input_word, \n",
    "                sequence_1_input_char_len, sequence_2_input_char_len,\n",
    "                sequence_1_input_word_len, sequence_2_input_word_len,\n",
    "                magic_input],#\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_model_dssm(params):\n",
    "    # dssm是一个简单的双塔模型\n",
    "    input1 = keras.Input(shape=(MAX_SEQUENCE_LENGTH_WORD, ), dtype='int32')\n",
    "    input2 = keras.Input(shape=(MAX_SEQUENCE_LENGTH_WORD, ), dtype='int32')\n",
    "    input1c = keras.Input(shape=(MAX_SEQUENCE_LENGTH_CHAR, ), dtype='int32')\n",
    "    input2c = keras.Input(shape=(MAX_SEQUENCE_LENGTH_CHAR, ), dtype='int32')\n",
    "    # magic_input = keras.Input(shape=(all_feas.shape[-1], ))\n",
    "    \n",
    "    embedder = Embedding(5002, 300, input_length = MAX_SEQUENCE_LENGTH_WORD, \n",
    "                         weights = [embedding_matrix_word])# , mask_zero=True, trainable = False\n",
    "    # CuDNN\n",
    "    lstm0 = LSTM(params['num_lstm'],\n",
    "        dropout=params['lstm_dropout_rate'],\n",
    "        recurrent_dropout=params['lstm_dropout_rate'],\n",
    "                 return_sequences = True)\n",
    "    lstm1 = Bidirectional(LSTM(params['num_lstm'],\n",
    "        dropout=params['lstm_dropout_rate'],\n",
    "        recurrent_dropout=params['lstm_dropout_rate']))\n",
    "    lstm2 = LSTM(params['num_lstm'])\n",
    "    # att1 = AttLayer() #Attention(10)\n",
    "    den = Dense(64,activation = 'tanh')\n",
    "\n",
    "    att1 = Lambda(lambda x: K.max(x,axis = 1))\n",
    "    v1 = embedder(input1)\n",
    "    v2 = embedder(input2)\n",
    "    v11 = lstm1(v1)\n",
    "    v22 = lstm1(v2)\n",
    "    v1ls = lstm2(lstm0(v1))\n",
    "    v2ls = lstm2(lstm0(v2))\n",
    "    v1 = Concatenate(axis=1)([att1(v1),v11])\n",
    "    v2 = Concatenate(axis=1)([att1(v2),v22])\n",
    "    \n",
    "    embedder = Embedding(2087, 300, input_length = MAX_SEQUENCE_LENGTH_CHAR, \n",
    "                         weights = [embedding_matrix_char])# , mask_zero=True, trainable = False\n",
    "    lstm1c = Bidirectional(LSTM(params['num_lstm']))\n",
    "    #att1c = Attention(10)\n",
    "    att1c = Lambda(lambda x: K.max(x,axis = 1))\n",
    "    # att1c = AttLayer()\n",
    "    v1c = embedder(input1c)\n",
    "    v2c = embedder(input2c)\n",
    "    v11c = lstm1c(v1c)\n",
    "    v22c = lstm1c(v2c)\n",
    "    v1c = Concatenate(axis=1)([att1c(v1c),v11c])\n",
    "    v2c = Concatenate(axis=1)([att1c(v2c),v22c])\n",
    "\n",
    "\n",
    "    mul = Multiply()([v1,v2])\n",
    "    sub = Lambda(lambda x: K.abs(x))(Subtract()([v1,v2]))\n",
    "    maximum = Maximum()([Multiply()([v1,v1]),Multiply()([v2,v2])])\n",
    "    mulc = Multiply()([v1c,v2c])\n",
    "    subc = Lambda(lambda x: K.abs(x))(Subtract()([v1c,v2c]))\n",
    "    maximumc = Maximum()([Multiply()([v1c,v1c]),Multiply()([v2c,v2c])])\n",
    "    sub2 = Lambda(lambda x: K.abs(x))(Subtract()([v1ls,v2ls]))\n",
    "    matchlist = Concatenate(axis=1)([mul,sub,mulc,subc,maximum,maximumc,sub2])\n",
    "    matchlist = Dropout(0.2)(matchlist)\n",
    "    \n",
    "\n",
    "    merged = Concatenate(axis=1)([Dense(32,activation = 'relu')(matchlist),\n",
    "                                     Dense(48,activation = 'sigmoid')(matchlist)])\n",
    "    \n",
    "    merged = keras.layers.Dense(params['num_dense'], activation='relu')(merged)\n",
    "    # merged = keras.layers.Dropout(params['dense_dropout_rate'])(merged)\n",
    "    merged = keras.layers.BatchNormalization()(merged)\n",
    "    res = Dense(1, activation = 'sigmoid')(merged)\n",
    "\n",
    "\n",
    "    model = keras.Model(inputs=[input1c, input2c, input1, input2], outputs=res)\n",
    "    model.compile(optimizer= keras.optimizers.Adam(lr = 0.001), loss=\"binary_crossentropy\", \n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "### CNN\n",
    "def create_model_cnn(params):\n",
    "    input1 = keras.Input(shape=(MAX_SEQUENCE_LENGTH_WORD, ), dtype='int32')\n",
    "    input2 = keras.Input(shape=(MAX_SEQUENCE_LENGTH_WORD, ), dtype='int32')\n",
    "    input1c = keras.Input(shape=(MAX_SEQUENCE_LENGTH_CHAR, ), dtype='int32')\n",
    "    input2c = keras.Input(shape=(MAX_SEQUENCE_LENGTH_CHAR, ), dtype='int32')\n",
    "    \n",
    "    embedder = Embedding(5002, 300, input_length = MAX_SEQUENCE_LENGTH_WORD, \n",
    "                         weights = [embedding_matrix_word], trainable = False)#mask_zero=True, \n",
    "    v1 = embedder(input1)\n",
    "    v2 = embedder(input2)\n",
    "    \n",
    "#     x1,x2 = get_bilstm_output(embedding_seq1, embedding_seq2, params)\n",
    "#     rnn_x1 = AttLayer()(x1)\n",
    "#     rnn_x2 = AttLayer()(x2)\n",
    "\n",
    "    att1 = Lambda(lambda x: K.max(x,axis = 1))\n",
    "    \n",
    "    v11 = get_cnn_output(v1, params)\n",
    "    v22 = get_cnn_output(v2, params)\n",
    "    \n",
    "    v1 = Concatenate(axis=1)([att1(v1),att1(v11)])\n",
    "    v2 = Concatenate(axis=1)([att1(v2),att1(v22)])\n",
    "    \n",
    "    embedder = Embedding(2087, 300, input_length = MAX_SEQUENCE_LENGTH_CHAR, \n",
    "                         weights = [embedding_matrix_char], trainable = False)# , mask_zero=True\n",
    "    att1c = Lambda(lambda x: K.max(x,axis = 1))\n",
    "    v1c = embedder(input1c)\n",
    "    v2c = embedder(input2c)\n",
    "    v11c = get_cnn_output(v1c, params)\n",
    "    v22c = get_cnn_output(v2c, params)\n",
    "    v1c = Concatenate(axis=1)([att1c(v1c),att1(v11c)])\n",
    "    v2c = Concatenate(axis=1)([att1c(v2c),att1(v22c)])\n",
    "    \n",
    "    mul = Multiply()([v1,v2])\n",
    "    sub = Lambda(lambda x: K.abs(x))(Subtract()([v1,v2]))\n",
    "    maximum = Maximum()([Multiply()([v1,v1]),Multiply()([v2,v2])])\n",
    "    mulc = Multiply()([v1c,v2c])\n",
    "    subc = Lambda(lambda x: K.abs(x))(Subtract()([v1c,v2c]))\n",
    "    maximumc = Maximum()([Multiply()([v1c,v1c]),Multiply()([v2c,v2c])])\n",
    "    matchlist = Concatenate(axis=1)([mul,sub,mulc,subc,maximum,maximumc])\n",
    "    matchlist = Dropout(0.2)(matchlist)\n",
    "\n",
    "    merged = Concatenate(axis=1)([Dense(32,activation = 'relu')(matchlist),\n",
    "                                     Dense(48,activation = 'sigmoid')(matchlist)])\n",
    "    \n",
    "    merged = keras.layers.Dense(params['num_dense'], activation='relu')(merged)\n",
    "    merged = keras.layers.Dropout(params['dense_dropout_rate'])(merged)\n",
    "    merged = keras.layers.BatchNormalization()(merged)\n",
    "    res = Dense(1, activation = 'sigmoid')(merged)\n",
    "\n",
    "\n",
    "    model = keras.Model(inputs=[input1c, input2c, input1, input2], outputs=res)\n",
    "    model.compile(optimizer= keras.optimizers.Adam(lr = 0.001), loss=\"binary_crossentropy\", \n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def unchanged_shape(input_shape):\n",
    "    return input_shape\n",
    "\n",
    "def time_distributed(x, layers):\n",
    "    for l in layers:\n",
    "        x = TimeDistributed(l)(x)\n",
    "    return x\n",
    "\n",
    "def align(input_1, input_2):\n",
    "    attention = Dot(axes=-1)([input_1, input_2])\n",
    "    w_att_1 = Lambda(lambda x: softmax(x, axis=1),\n",
    "                     output_shape=unchanged_shape)(attention)\n",
    "    w_att_2 = Permute((2,1))(Lambda(lambda x: softmax(x, axis=2),\n",
    "                             output_shape=unchanged_shape)(attention))\n",
    "    in1_aligned = Dot(axes=1)([w_att_1, input_1])\n",
    "    in2_aligned = Dot(axes=1)([w_att_2, input_2])\n",
    "    return in1_aligned, in2_aligned\n",
    "\n",
    "def aggregate(x1, x2, x10, x20,  num_class, dense_dim=300, dropout_rate=0.2, activation=\"relu\"):#\n",
    "    feat1 = concatenate(list(map(lambda l: l(x1), [GlobalAvgPool1D(), GlobalMaxPool1D()])))\n",
    "    feat2 = concatenate(list(map(lambda l: l(x2), [GlobalAvgPool1D(), GlobalMaxPool1D()])))\n",
    "    feat10= concatenate(list(map(lambda l: l(x10), [GlobalAvgPool1D(), GlobalMaxPool1D()])))\n",
    "    feat20 = concatenate(list(map(lambda l: l(x20), [GlobalAvgPool1D(), GlobalMaxPool1D()])))\n",
    "    x = Concatenate()([feat1, feat2, feat10, feat20]) #\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(dense_dim, activation=activation)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(dense_dim, activation=activation)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    return x  \n",
    "\n",
    "def embed_project_atten(q1, q2, encode, projection_dim=300, projection_hidden=200, projection_dropout=0.2,\n",
    "                           compare_dim=500, compare_dropout=0.2,\n",
    "                           dense_dim=300, dropout_rate=0.2,\n",
    "                           lr=1e-3, activation='relu'):\n",
    "    q1_embed = encode(q1)\n",
    "    q2_embed = encode(q2)\n",
    "    \n",
    "    # Projection\n",
    "    projection_layers = []\n",
    "    if projection_hidden > 0:\n",
    "        projection_layers.extend([\n",
    "                Dense(projection_hidden, activation=activation),\n",
    "                Dropout(rate=projection_dropout),\n",
    "            ])\n",
    "    projection_layers.extend([\n",
    "            Dense(projection_dim, activation=None),\n",
    "            Dropout(rate=projection_dropout),\n",
    "        ])\n",
    "    q1_encoded = time_distributed(q1_embed, projection_layers)\n",
    "    q2_encoded = time_distributed(q2_embed, projection_layers)\n",
    "    \n",
    "    # Attention\n",
    "    q1_aligned, q2_aligned = align(q1_encoded, q2_encoded)    \n",
    "    \n",
    "    # Compare\n",
    "    q1_combined = concatenate([q1_encoded, q2_aligned])\n",
    "    q2_combined = concatenate([q2_encoded, q1_aligned]) \n",
    "    compare_layers = [\n",
    "        Dense(compare_dim, activation=activation),\n",
    "        Dropout(compare_dropout),\n",
    "        Dense(compare_dim, activation=activation),\n",
    "        Dropout(compare_dropout),\n",
    "    ]\n",
    "    q1_compare = time_distributed(q1_combined, compare_layers)\n",
    "    q2_compare = time_distributed(q2_combined, compare_layers)\n",
    "    return q1_compare, q2_compare\n",
    "\n",
    "def lstm_project_atten(q1_embed, q2_embed, projection_dim=300, projection_hidden=200, projection_dropout=0.2,\n",
    "                           compare_dim=500, compare_dropout=0.2,\n",
    "                           dense_dim=300, dropout_rate=0.2,\n",
    "                           lr=1e-3, activation='relu'):\n",
    "    \n",
    "    # CuDNN\n",
    "    lstm0 = CuDNNLSTM(model_params['num_lstm'],\n",
    "                 return_sequences = True)\n",
    "    lstm1 = Bidirectional(CuDNNLSTM(model_params['num_lstm']))\n",
    "    lstm2 = CuDNNLSTM(model_params['num_lstm'])\n",
    "    \n",
    "    att1 = Lambda(lambda x: K.max(x,axis = 1))\n",
    "    q1_embed = lstm0(q1_embed)\n",
    "    q2_embed = lstm0(q2_embed)\n",
    "    \n",
    "    # Projection\n",
    "    projection_layers = []\n",
    "    if projection_hidden > 0:\n",
    "        projection_layers.extend([\n",
    "                Dense(projection_hidden, activation=activation),\n",
    "                Dropout(rate=projection_dropout),\n",
    "            ])\n",
    "    projection_layers.extend([\n",
    "            Dense(projection_dim, activation=None),\n",
    "            Dropout(rate=projection_dropout),\n",
    "        ])\n",
    "    q1_encoded = time_distributed(q1_embed, projection_layers)\n",
    "    q2_encoded = time_distributed(q2_embed, projection_layers)\n",
    "    \n",
    "    # Attention\n",
    "    q1_aligned, q2_aligned = align(q1_encoded, q2_encoded)    \n",
    "    \n",
    "    # Compare\n",
    "    q1_combined = concatenate([q1_encoded, q2_aligned])\n",
    "    q2_combined = concatenate([q2_encoded, q1_aligned]) \n",
    "    compare_layers = [\n",
    "        Dense(compare_dim, activation=activation),\n",
    "        Dropout(compare_dropout),\n",
    "        Dense(compare_dim, activation=activation),\n",
    "        Dropout(compare_dropout),\n",
    "    ]\n",
    "    q1_compare = time_distributed(q1_combined, compare_layers)\n",
    "    q2_compare = time_distributed(q2_combined, compare_layers)\n",
    "    return q1_compare, q2_compare\n",
    "\n",
    "def build_model_combine(params, num_class=1,projection_dim=300, projection_hidden=200, projection_dropout=0.2,\n",
    "                           compare_dim=500, compare_dropout=0.2,\n",
    "                           dense_dim=300, dropout_rate=0.2,\n",
    "                           lr=1e-3, activation='relu'):\n",
    "    q1 = Input(name='q1',shape=(MAX_SEQUENCE_LENGTH_CHAR,), dtype='int32')\n",
    "    q2 = Input(name='q2',shape=(MAX_SEQUENCE_LENGTH_CHAR,), dtype='int32')\n",
    "    q10 = Input(name='q10',shape=(MAX_SEQUENCE_LENGTH_WORD,), dtype='int32')\n",
    "    q20 = Input(name='q20',shape=(MAX_SEQUENCE_LENGTH_WORD,), dtype='int32')\n",
    "    \n",
    "    #  #Embedding Embedding(VOC_SIZE_CHAR, 300, input_length = MAX_SEQUENCE_LENGTH_CHAR)  Embedding(VOC_SIZE_WORD, 300, input_length = MAX_SEQUENCE_LENGTH_WORD) StaticEmbedding(embedding_matrix_char)\n",
    "    encode = Embedding(VOC_SIZE_CHAR+2, 300, input_length = MAX_SEQUENCE_LENGTH_CHAR) # StaticEmbedding(embedding_matrix_char) #\n",
    "    \n",
    "    q1_embed = encode(q1)\n",
    "    q2_embed = encode(q2)\n",
    "    \n",
    "    q1_compare, q2_compare = embed_project_atten(q1, q2, encode)\n",
    "    \n",
    "    \n",
    "    encode = Embedding(VOC_SIZE_WORD+2, 300, input_length = MAX_SEQUENCE_LENGTH_WORD)  # StaticEmbedding(embedding_matrix_word) # \n",
    "    q10_embed = encode(q10)\n",
    "    q20_embed = encode(q20)\n",
    "    #q10_compare, q20_compare = embed_project_atten(q10_embed, q20_embed, encode)\n",
    "    \n",
    "    q1_compare0, q2_compare0 = lstm_project_atten(q1_embed, q2_embed)\n",
    "    \n",
    "    q10_compare0, q20_compare0 = lstm_project_atten(q10_embed, q20_embed)\n",
    "    \n",
    "    \n",
    "    q1_compare0 = concatenate([q1_compare0, q1_embed])\n",
    "    q2_compare0 = concatenate([q2_compare0, q2_embed])\n",
    "    q10_compare0 = concatenate([q10_compare0, q10_embed])\n",
    "    q20_compare0 = concatenate([q20_compare0, q20_embed])\n",
    "    # import pdb; pdb.set_trace()\n",
    "    q1_compare1, q2_compare1 = lstm_project_atten(q1_compare0, q2_compare0)\n",
    "    \n",
    "    q10_compare1, q20_compare1 = lstm_project_atten(q10_compare0, q20_compare0)\n",
    "    \n",
    "    \n",
    "    # Aggregate\n",
    "    x0 = aggregate(q1_compare1, q2_compare1, q10_compare1, q20_compare1, num_class)#\n",
    "    x1 = aggregate(q1_embed, q2_embed, q10_embed, q20_embed, num_class)#\n",
    "    x = concatenate([x0, x1])\n",
    "    scores = Dense(num_class, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=[q1, q2, q10, q20], outputs=scores)#\n",
    "    return model\n",
    "\n",
    "def build_model(params, num_class=1,projection_dim=300, projection_hidden=200, projection_dropout=0.2,\n",
    "                           compare_dim=500, compare_dropout=0.2,\n",
    "                           dense_dim=300, dropout_rate=0.2,\n",
    "                           lr=1e-3, activation='relu'):\n",
    "    q1 = Input(name='q1',shape=(MAX_SEQUENCE_LENGTH_CHAR,), dtype='int32')\n",
    "    q2 = Input(name='q2',shape=(MAX_SEQUENCE_LENGTH_CHAR,), dtype='int32')\n",
    "    q10 = Input(name='q10',shape=(MAX_SEQUENCE_LENGTH_WORD,), dtype='int32')\n",
    "    q20 = Input(name='q20',shape=(MAX_SEQUENCE_LENGTH_WORD,), dtype='int32')\n",
    "    \n",
    "    #  #Embedding Embedding(VOC_SIZE_CHAR, 300, input_length = MAX_SEQUENCE_LENGTH_CHAR)  Embedding(VOC_SIZE_WORD, 300, input_length = MAX_SEQUENCE_LENGTH_WORD) StaticEmbedding(embedding_matrix_char)\n",
    "    encode = Embedding(VOC_SIZE_CHAR+2, 300, input_length = MAX_SEQUENCE_LENGTH_CHAR) # StaticEmbedding(embedding_matrix_char) #\n",
    "    q1_compare, q2_compare = embed_project_atten(q1, q2, encode)\n",
    "    \n",
    "    \n",
    "    encode = Embedding(VOC_SIZE_WORD+2, 300, input_length = MAX_SEQUENCE_LENGTH_WORD)  # StaticEmbedding(embedding_matrix_word) # \n",
    "    q10_compare, q20_compare = embed_project_atten(q10, q20, encode)\n",
    "    \n",
    "    \n",
    "    # Aggregate\n",
    "    scores = aggregate(q1_compare, q2_compare, q10_compare, q20_compare, num_class)#\n",
    "    \n",
    "    scores = Dense(num_class, activation='sigmoid')(scores)\n",
    "    model = Model(inputs=[q1, q2, q10, q20], outputs=scores)#\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_embedding_output_vec>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding_output_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {1: 19, 2: 539, 3: 6450, 4: 16541, 5: 28024, 6: 32512, 7: 29288, 8: 22182, 9: 15893, 10: 11586, 11: 8220, 12: 6044, 13: 4422, 14: 3369, 15: 2625, 16: 1995, 17: 1599, 18: 1247, 19: 1006, 20: 757, 21: 643, 22: 499, 23: 416, 24: 337, 25: 251, 26: 232, 27: 211, 28: 169, 29: 107, 30: 110, 31: 75, 32: 69, 33: 75, 34: 49, 35: 47, 36: 42, 37: 33, 38: 36, 39: 42, 40: 21, 41: 15, 42: 17, 43: 17, 44: 10, 45: 12, 46: 11, 47: 16, 48: 4, 49: 16, 50: 9, 51: 7, 52: 7, 53: 5, 54: 5, 55: 3, 56: 3, 58: 2, 59: 1, 60: 2, 61: 1, 62: 1, 64: 3, 65: 1, 66: 1, 90: 1})\n"
     ]
    }
   ],
   "source": [
    "len_dict = defaultdict(int)\n",
    "for i in range(train_data.shape[0]):\n",
    "    s1 = str(train_data.iloc[i, 1]).split(' ')\n",
    "    len_dict[len(s1)] += 1\n",
    "    s1 = str(train_data.iloc[i, 2]).split(' ')\n",
    "    len_dict[len(s1)] += 1\n",
    "print len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_squence_ques1_char = list(train_data_char.iloc[:, 1])\n",
    "word_squence_ques1_char = [[int(im) for im in item.split(' ')] for item in word_squence_ques1_char]\n",
    "word_squence_ques2_char = list(train_data_char.iloc[:, 2])\n",
    "word_squence_ques2_char = [[int(im) for im in item.split(' ')] for item in word_squence_ques2_char]\n",
    "\n",
    "word_squence_ques1_word = list(train_data_word.iloc[:, 1])\n",
    "word_squence_ques1_word = [[int(im) for im in item.split(' ')] for item in word_squence_ques1_word]\n",
    "word_squence_ques2_word = list(train_data_word.iloc[:, 2])\n",
    "word_squence_ques2_word = [[int(im) for im in item.split(' ')] for item in word_squence_ques2_word]\n",
    "\n",
    "MAX_SEQUENCE_LENGTH_WORD = 50 # char 40 word 30\n",
    "MAX_SEQUENCE_LENGTH_CHAR = 60 # char 40 word 30\n",
    "word_squence_ques1_char_len = [len(item) for item in word_squence_ques1_char]\n",
    "word_squence_ques2_char_len = [len(item) for item in word_squence_ques2_char]\n",
    "word_squence_ques1_word_len = [len(item) for item in word_squence_ques1_word]\n",
    "word_squence_ques2_word_len = [len(item) for item in word_squence_ques2_word]\n",
    "word_squence_ques1_char = pad_sequences(word_squence_ques1_char, maxlen=MAX_SEQUENCE_LENGTH_CHAR)\n",
    "word_squence_ques2_char = pad_sequences(word_squence_ques2_char, maxlen=MAX_SEQUENCE_LENGTH_CHAR)\n",
    "word_squence_ques1_word = pad_sequences(word_squence_ques1_word, maxlen=MAX_SEQUENCE_LENGTH_WORD)\n",
    "word_squence_ques2_word = pad_sequences(word_squence_ques2_word, maxlen=MAX_SEQUENCE_LENGTH_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158360, 1)\n",
      "(158360, 1)\n",
      "(158360, 1)\n",
      "Fitting fold\n",
      "Train on 158360 samples, validate on 39592 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[51200,500] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: time_distributed_471/MatMul = MatMul[T=DT_FLOAT, _class=[\"loc:@training_1/Adam/gradients/time_distributed_471/MatMul_grad/MatMul\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_470/cond/Merge, time_distributed_467/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss_1/mul/_3535 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_11171_loss_1/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-0ffb481c941e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m }\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_model_combine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_q1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_q2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_q10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_q20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_magic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-0ffb481c941e>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(model_func, X_train_q1, X_train_q2, X_train_q10, X_train_q20, X_train_magic, y_train, model_checkpoint_path)\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 ),\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             ],\n\u001b[1;32m    130\u001b[0m         )\n",
      "\u001b[0;32m/home/chenchi/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/chenchi/.local/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chenchi/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chenchi/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chenchi/anaconda3/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[0;32m-> 1454\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chenchi/anaconda3/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[51200,500] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: time_distributed_471/MatMul = MatMul[T=DT_FLOAT, _class=[\"loc:@training_1/Adam/gradients/time_distributed_471/MatMul_grad/MatMul\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_470/cond/Merge, time_distributed_467/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss_1/mul/_3535 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_11171_loss_1/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "class Metrics(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[:-3])))#\n",
    "        val_predict = [1 if item > 0.63 else 0 for item in val_predict]\n",
    "        val_targ = self.validation_data[-3]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        print \"epoch end, f1-score: \", _val_f1\n",
    "metrics = Metrics()\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n",
    "X_train_q1 = word_squence_ques1_char\n",
    "X_train_q2 = word_squence_ques2_char\n",
    "X_train_q10 = word_squence_ques1_word\n",
    "X_train_q20 = word_squence_ques2_word\n",
    "X_train_magic = all_feas\n",
    "y_train = train_data.iloc[:, 3]\n",
    "word_squence_ques1_char_len = np.array(word_squence_ques1_char_len).reshape(-1, 1)\n",
    "word_squence_ques2_char_len = np.array(word_squence_ques2_char_len).reshape(-1, 1)\n",
    "word_squence_ques1_word_len = np.array(word_squence_ques1_word_len).reshape(-1, 1)\n",
    "word_squence_ques2_word_len = np.array(word_squence_ques2_word_len).reshape(-1, 1)\n",
    "model_checkpoint_path = '../data/checkpoints/stacking-word-word-mix-checkpoint.h5'\n",
    "\n",
    "class_weight = {0: 1.,\n",
    "                1: 5.,}\n",
    "\n",
    "X_fold_val_q1_out,X_fold_val_q2_out, X_fold_val_magic_out = [0, 0, 0]\n",
    "\n",
    "def model_train(model_func, X_train_q1, X_train_q2, X_train_q10, X_train_q20, X_train_magic, y_train, model_checkpoint_path):\n",
    "    NUM_FOLDS = 5\n",
    "    kfold = StratifiedKFold(\n",
    "        n_splits=NUM_FOLDS,\n",
    "        shuffle=True,\n",
    "        random_state=30\n",
    "    )\n",
    "    BATCH_SIZE = 1024\n",
    "    MAX_EPOCHS = 100\n",
    "\n",
    "    for fold_num, (ix_train, ix_val) in enumerate(kfold.split(X_train_q1, y_train)):\n",
    "\n",
    "        # Augment the training set by mirroring the pairs.\n",
    "        X_fold_train_q1 = np.vstack([X_train_q1[ix_train], X_train_q2[ix_train]])\n",
    "        X_fold_train_q2 = np.vstack([X_train_q2[ix_train], X_train_q1[ix_train]])\n",
    "        X_fold_train_q10 = np.vstack([X_train_q10[ix_train], X_train_q20[ix_train]])\n",
    "        X_fold_train_q20 = np.vstack([X_train_q20[ix_train], X_train_q10[ix_train]])\n",
    "        X_fold_train_char0_len = np.vstack([word_squence_ques1_char_len[ix_train], word_squence_ques2_char_len[ix_train]])\n",
    "        X_fold_train_char1_len = np.vstack([word_squence_ques2_char_len[ix_train], word_squence_ques1_char_len[ix_train]])\n",
    "        X_fold_train_word0_len = np.vstack([word_squence_ques1_word_len[ix_train], word_squence_ques2_word_len[ix_train]])\n",
    "        X_fold_train_word1_len = np.vstack([word_squence_ques2_word_len[ix_train], word_squence_ques1_word_len[ix_train]])\n",
    "        X_fold_train_magic = np.vstack([X_train_magic[ix_train], X_train_magic[ix_train]])\n",
    "        print X_fold_train_char0_len.shape\n",
    "        print X_fold_train_char1_len.shape\n",
    "        print X_fold_train_word1_len.shape\n",
    "\n",
    "        X_fold_val_q1 = np.vstack([X_train_q1[ix_val], X_train_q2[ix_val]])\n",
    "        X_fold_val_q2 = np.vstack([X_train_q2[ix_val], X_train_q1[ix_val]])\n",
    "        X_fold_val_q10 = np.vstack([X_train_q10[ix_val], X_train_q20[ix_val]])\n",
    "        X_fold_val_q20 = np.vstack([X_train_q20[ix_val], X_train_q10[ix_val]])\n",
    "        X_fold_val_char0_len = np.vstack([word_squence_ques1_char_len[ix_val], word_squence_ques2_char_len[ix_val]])\n",
    "        X_fold_val_char1_len = np.vstack([word_squence_ques2_char_len[ix_val], word_squence_ques1_char_len[ix_val]])\n",
    "        X_fold_val_word0_len = np.vstack([word_squence_ques1_word_len[ix_val], word_squence_ques2_word_len[ix_val]])\n",
    "        X_fold_val_word1_len = np.vstack([word_squence_ques2_word_len[ix_val], word_squence_ques1_word_len[ix_val]])\n",
    "        X_fold_val_magic = np.vstack([X_train_magic[ix_val], X_train_magic[ix_val]])\n",
    "        X_fold_train_magic[np.isnan(X_fold_train_magic)] = 0\n",
    "        \n",
    "        \n",
    "        # Ground truth should also be \"mirrored\".\n",
    "        y_fold_train = np.concatenate([y_train[ix_train], y_train[ix_train]])\n",
    "        y_fold_val = np.concatenate([y_train[ix_val], y_train[ix_val]])\n",
    "\n",
    "        print('Fitting fold')\n",
    "\n",
    "        # Compile a new model.\n",
    "        # model1 = create_model_cnn(model_params)\n",
    "        model = model_func(model_params)\n",
    "\n",
    "        model.compile(optimizer= keras.optimizers.Adam(lr = 0.001), loss=\"binary_crossentropy\", \n",
    "            metrics=['accuracy'])\n",
    "        # Train.\n",
    "        model.fit(\n",
    "            [X_fold_train_q1, X_fold_train_q2, \n",
    "             X_fold_train_q10, X_fold_train_q20, \n",
    "#              X_fold_train_char0_len, X_fold_train_char1_len,\n",
    "#              X_fold_train_word0_len, X_fold_train_word1_len,\n",
    "             # X_fold_train_magic\n",
    "            ], y_fold_train,#\n",
    "            validation_data=([X_fold_val_q1, X_fold_val_q2, \n",
    "                              X_fold_val_q10, X_fold_val_q20, \n",
    "#                               X_fold_val_char0_len, X_fold_val_char1_len,\n",
    "#                               X_fold_val_word0_len,X_fold_val_word1_len,\n",
    "                              # X_fold_val_magic\n",
    "                             ], y_fold_val),#\n",
    "\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=MAX_EPOCHS,\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            class_weight=class_weight,\n",
    "\n",
    "            callbacks=[\n",
    "                # Stop training when the validation loss stops improving.\n",
    "                EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    min_delta=0.001,\n",
    "                    patience=20,\n",
    "                    verbose=1,\n",
    "                    mode='auto',\n",
    "                ),\n",
    "                ReduceLROnPlateau(\n",
    "                    monitor='val_loss', \n",
    "                    factor=0.1, \n",
    "                    patience=5, \n",
    "                    min_delta=0.0001,\n",
    "                    cooldown=1, \n",
    "                ),\n",
    "                # Save the weights of the best epoch.\n",
    "                ModelCheckpoint(\n",
    "                    os.path.join(model_checkpoint_path + str(fold_num) + 'weights.{epoch:02d}.hdf5'),\n",
    "                    monitor='val_loss',\n",
    "                    save_best_only=False,\n",
    "                    verbose=2,\n",
    "                    mode='auto',\n",
    "                ),\n",
    "                metrics\n",
    "            ],\n",
    "        )\n",
    "        break\n",
    "    \n",
    "    # Clear GPU memory.\n",
    "    K.clear_session()\n",
    "    del X_fold_train_q1, X_fold_train_q2, X_fold_train_magic\n",
    "    del X_fold_val_q1, X_fold_val_q2, X_fold_val_magic\n",
    "    del model\n",
    "    gc.collect()\n",
    "    # return score\n",
    "\n",
    "model_path = '../data/checkpoints/test/'\n",
    "# model_train(create_model, X_train_q1, X_train_q2, X_train_magic, y_train, model_path)\n",
    "\n",
    "\n",
    "model_params = {\n",
    "    'dense_dropout_rate': 0.3,\n",
    "    'lstm_dropout_rate': 0.3,\n",
    "    'num_dense': 150,\n",
    "    'num_lstm': 128,\n",
    "    'num_filters':32\n",
    "}\n",
    "\n",
    "model_train(build_model_combine, X_train_q1, X_train_q2, X_train_q10, X_train_q20, X_train_magic, y_train, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_33_1:0' shape=(?, 40) dtype=int32>,\n",
       " <tf.Tensor 'input_34_1:0' shape=(?, 40) dtype=int32>,\n",
       " <tf.Tensor 'input_31_1:0' shape=(?, 30) dtype=int32>,\n",
       " <tf.Tensor 'input_32_1:0' shape=(?, 30) dtype=int32>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH_CHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH_WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = keras.models.load_model('../data/checkpoints/test/0weights.01.hdf5', custom_objects={'AttLayer':AttLayer})\n",
    "model2.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
