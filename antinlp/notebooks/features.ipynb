{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽取特征\n",
    "\n",
    "统计类特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98976, 4)\n"
     ]
    }
   ],
   "source": [
    "train_data_char = pd.read_csv('../data/aux/train_char_indexvec.csv')\n",
    "train_data_word = pd.read_csv('../data/aux/train_word_indexvec.csv')\n",
    "train_data_char.columns = ['id', 'question1', 'question2', 'label']\n",
    "train_data_word.columns = ['id', 'question1', 'question2', 'label']\n",
    "print(train_data_char.shape)\n",
    "\n",
    "train_ori_data = pd.read_csv('../data/aux/train_parse.csv', sep='\\t', header=None)\n",
    "train_ori_data.columns = ['id', 'question1', 'question2', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id        question1                       question2  label\n",
      "0   1      ﻿怎么更改花呗手机号码  我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号      1\n",
      "1   2  也开不了花呗，就这样了？完事了                      真的嘛？就是花呗付款      0\n",
      "2   3      花呗冻结以后还能开通吗                   我的条件可以开通花呗借款吗      0\n",
      "3   4         如何得知关闭借呗                         想永久关闭借呗      0\n",
      "4   5           花呗扫码付钱                     二维码扫描可以用花呗吗      0\n",
      "   id                           question1                                          question2  label\n",
      "0   1                        11 239 2 213  3 5 2 18 149 5 213 8 11 980 588 40 5 30 5 172 106      1\n",
      "1   2  103 152 31 2 8 71 599 7 136 3142 7                               819 133 136 211 2 32      0\n",
      "2   3                2 114 181 10 23 20 9                              3 5 201 13 20 2 120 9      0\n",
      "3   4                             56 51 4                                        67 560 51 4      0\n",
      "4   5                           2 319 585                                 212 1033 13 15 2 9      0\n"
     ]
    }
   ],
   "source": [
    "print train_ori_data.head()\n",
    "print train_data_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_ori_data.iloc[:, 3]\n",
    "def test_feas(X):\n",
    "    Xt, Xv, yt, yv = train_test_split(X, y)\n",
    "    clf = LogisticRegression(class_weight={0: 1.,1: 5,})\n",
    "    clf.fit(Xt, yt)\n",
    "    f1 = f1_score(yv, clf.predict(Xv))\n",
    "    print 'weights: ', f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_string_as_list_by_ngram(input_string,ngram_value):\n",
    "    input_string=\"\".join([string for string in input_string if string.strip()])\n",
    "    length = len(input_string)\n",
    "    result_string=[]\n",
    "    for i in range(length):\n",
    "        if i + ngram_value < length + 1:\n",
    "            result_string.append(input_string[i:i+ngram_value])\n",
    "    return result_string\n",
    "\n",
    "\n",
    "def compute_blue_ngram(x1_list,x2_list):\n",
    "    \"\"\"\n",
    "    compute blue score use ngram information. x1_list as predict sentence,x2_list as target sentence\n",
    "    :param x1_list:\n",
    "    :param x2_list:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    count_dict={}\n",
    "    count_dict_clip={}\n",
    "    #1. count for each token at predict sentence side.\n",
    "    for token in x1_list:\n",
    "        if token not in count_dict:\n",
    "            count_dict[token]=1\n",
    "        else:\n",
    "            count_dict[token]=count_dict[token]+1\n",
    "    count=np.sum([value for key,value in count_dict.items()])\n",
    "\n",
    "    #2.count for tokens existing in predict sentence for target sentence side.\n",
    "    for token in x2_list:\n",
    "        if token in count_dict:\n",
    "            if token not in count_dict_clip:\n",
    "                count_dict_clip[token]=1\n",
    "            else:\n",
    "                count_dict_clip[token]=count_dict_clip[token]+1\n",
    "\n",
    "    #3. clip value to ceiling value for that token\n",
    "    count_dict_clip={key:(value if value<=count_dict[key] else count_dict[key]) for key,value in count_dict_clip.items()}\n",
    "    count_clip=np.sum([value for key,value in count_dict_clip.items()])\n",
    "    result=float(count_clip)/(float(count)+0.00000001)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_ngram(csv_data, ngram_value):\n",
    "    ngram_lt1 = []\n",
    "    ngram_lt2 = []\n",
    "    for i in range(csv_data.shape[0]):\n",
    "        x1_list = csv_data.iloc[i, 1].split(' ')\n",
    "        x2_list = csv_data.iloc[i, 2].split(' ')\n",
    "        res1 = compute_blue_ngram(split_string_as_list_by_ngram(x1_list, ngram_value), \n",
    "                                  split_string_as_list_by_ngram(x2_list,ngram_value))\n",
    "        res2 = compute_blue_ngram(split_string_as_list_by_ngram(x2_list, ngram_value), \n",
    "                                  split_string_as_list_by_ngram(x1_list,ngram_value))\n",
    "        ngram_lt1.append(res1)\n",
    "        ngram_lt2.append(res2)\n",
    "    return ngram_lt1,ngram_lt2\n",
    "\n",
    "fea_dict = {}\n",
    "for ngram in range(1, 9):\n",
    "    ngram_lt1,ngram_lt2 = cal_ngram(train_data_char, ngram)\n",
    "    fea_dict['ngram1'+str(ngram)] = ngram_lt1\n",
    "    fea_dict['ngram2'+str(ngram)] = ngram_lt2\n",
    "    \n",
    "save_data_char = pd.DataFrame(fea_dict)\n",
    "save_data_char.to_csv('../ngram_features_char.csv', index=False)\n",
    "\n",
    "fea_dict = {}\n",
    "for ngram in range(1, 9):\n",
    "    ngram_lt1,ngram_lt2 = cal_ngram(train_data_word, ngram)\n",
    "    fea_dict['ngram1'+str(ngram)] = ngram_lt1\n",
    "    fea_dict['ngram2'+str(ngram)] = ngram_lt2\n",
    "    \n",
    "save_data_word = pd.DataFrame(fea_dict)\n",
    "save_data_word.to_csv('../ngram_features_word.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  0.37924297924297923\n",
      "weights:  0.3337535266222463\n",
      "weights:  0.37465258476931634\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram11</th>\n",
       "      <th>ngram12</th>\n",
       "      <th>ngram13</th>\n",
       "      <th>ngram14</th>\n",
       "      <th>ngram15</th>\n",
       "      <th>ngram16</th>\n",
       "      <th>ngram17</th>\n",
       "      <th>ngram18</th>\n",
       "      <th>ngram21</th>\n",
       "      <th>ngram22</th>\n",
       "      <th>ngram23</th>\n",
       "      <th>ngram24</th>\n",
       "      <th>ngram25</th>\n",
       "      <th>ngram26</th>\n",
       "      <th>ngram27</th>\n",
       "      <th>ngram28</th>\n",
       "      <th>ngram11</th>\n",
       "      <th>ngram12</th>\n",
       "      <th>ngram13</th>\n",
       "      <th>ngram14</th>\n",
       "      <th>ngram15</th>\n",
       "      <th>ngram16</th>\n",
       "      <th>ngram17</th>\n",
       "      <th>ngram18</th>\n",
       "      <th>ngram21</th>\n",
       "      <th>ngram22</th>\n",
       "      <th>ngram23</th>\n",
       "      <th>ngram24</th>\n",
       "      <th>ngram25</th>\n",
       "      <th>ngram26</th>\n",
       "      <th>ngram27</th>\n",
       "      <th>ngram28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ngram11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.779065</td>\n",
       "      <td>0.537676</td>\n",
       "      <td>0.432923</td>\n",
       "      <td>0.355230</td>\n",
       "      <td>0.297301</td>\n",
       "      <td>0.240264</td>\n",
       "      <td>0.208584</td>\n",
       "      <td>-0.583697</td>\n",
       "      <td>-0.399037</td>\n",
       "      <td>-0.185816</td>\n",
       "      <td>-0.105913</td>\n",
       "      <td>-0.052851</td>\n",
       "      <td>-0.021169</td>\n",
       "      <td>0.008396</td>\n",
       "      <td>0.020463</td>\n",
       "      <td>0.747008</td>\n",
       "      <td>0.504818</td>\n",
       "      <td>0.319161</td>\n",
       "      <td>0.228436</td>\n",
       "      <td>0.182614</td>\n",
       "      <td>0.145635</td>\n",
       "      <td>0.110902</td>\n",
       "      <td>0.084278</td>\n",
       "      <td>-0.585551</td>\n",
       "      <td>-0.284581</td>\n",
       "      <td>-0.090015</td>\n",
       "      <td>-0.015892</td>\n",
       "      <td>0.011320</td>\n",
       "      <td>0.025278</td>\n",
       "      <td>0.031009</td>\n",
       "      <td>0.029033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram12</th>\n",
       "      <td>0.779065</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.856481</td>\n",
       "      <td>0.751880</td>\n",
       "      <td>0.663182</td>\n",
       "      <td>0.584745</td>\n",
       "      <td>0.498815</td>\n",
       "      <td>0.444421</td>\n",
       "      <td>-0.398241</td>\n",
       "      <td>0.086422</td>\n",
       "      <td>0.275733</td>\n",
       "      <td>0.310127</td>\n",
       "      <td>0.320415</td>\n",
       "      <td>0.311363</td>\n",
       "      <td>0.294594</td>\n",
       "      <td>0.275611</td>\n",
       "      <td>0.638008</td>\n",
       "      <td>0.639218</td>\n",
       "      <td>0.543497</td>\n",
       "      <td>0.444672</td>\n",
       "      <td>0.378304</td>\n",
       "      <td>0.312699</td>\n",
       "      <td>0.246075</td>\n",
       "      <td>0.194360</td>\n",
       "      <td>-0.358341</td>\n",
       "      <td>0.028699</td>\n",
       "      <td>0.212071</td>\n",
       "      <td>0.237728</td>\n",
       "      <td>0.227962</td>\n",
       "      <td>0.204694</td>\n",
       "      <td>0.173930</td>\n",
       "      <td>0.143534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram13</th>\n",
       "      <td>0.537676</td>\n",
       "      <td>0.856481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951748</td>\n",
       "      <td>0.878298</td>\n",
       "      <td>0.796535</td>\n",
       "      <td>0.691158</td>\n",
       "      <td>0.620564</td>\n",
       "      <td>-0.184841</td>\n",
       "      <td>0.276217</td>\n",
       "      <td>0.595330</td>\n",
       "      <td>0.627975</td>\n",
       "      <td>0.615297</td>\n",
       "      <td>0.578430</td>\n",
       "      <td>0.521473</td>\n",
       "      <td>0.476293</td>\n",
       "      <td>0.467732</td>\n",
       "      <td>0.659145</td>\n",
       "      <td>0.688804</td>\n",
       "      <td>0.599281</td>\n",
       "      <td>0.520568</td>\n",
       "      <td>0.431504</td>\n",
       "      <td>0.341368</td>\n",
       "      <td>0.271136</td>\n",
       "      <td>-0.147329</td>\n",
       "      <td>0.259630</td>\n",
       "      <td>0.449512</td>\n",
       "      <td>0.437448</td>\n",
       "      <td>0.396486</td>\n",
       "      <td>0.339467</td>\n",
       "      <td>0.278754</td>\n",
       "      <td>0.226180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram14</th>\n",
       "      <td>0.432923</td>\n",
       "      <td>0.751880</td>\n",
       "      <td>0.951748</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966365</td>\n",
       "      <td>0.901253</td>\n",
       "      <td>0.799729</td>\n",
       "      <td>0.724942</td>\n",
       "      <td>-0.104790</td>\n",
       "      <td>0.311459</td>\n",
       "      <td>0.629358</td>\n",
       "      <td>0.725453</td>\n",
       "      <td>0.732928</td>\n",
       "      <td>0.700569</td>\n",
       "      <td>0.637844</td>\n",
       "      <td>0.584367</td>\n",
       "      <td>0.390908</td>\n",
       "      <td>0.640118</td>\n",
       "      <td>0.739207</td>\n",
       "      <td>0.678397</td>\n",
       "      <td>0.602464</td>\n",
       "      <td>0.504312</td>\n",
       "      <td>0.401746</td>\n",
       "      <td>0.320386</td>\n",
       "      <td>-0.069602</td>\n",
       "      <td>0.328234</td>\n",
       "      <td>0.536447</td>\n",
       "      <td>0.531462</td>\n",
       "      <td>0.484485</td>\n",
       "      <td>0.414394</td>\n",
       "      <td>0.339238</td>\n",
       "      <td>0.274942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram15</th>\n",
       "      <td>0.355230</td>\n",
       "      <td>0.663182</td>\n",
       "      <td>0.878298</td>\n",
       "      <td>0.966365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968130</td>\n",
       "      <td>0.885447</td>\n",
       "      <td>0.813086</td>\n",
       "      <td>-0.050995</td>\n",
       "      <td>0.322956</td>\n",
       "      <td>0.618215</td>\n",
       "      <td>0.734604</td>\n",
       "      <td>0.789265</td>\n",
       "      <td>0.778309</td>\n",
       "      <td>0.725335</td>\n",
       "      <td>0.670659</td>\n",
       "      <td>0.328911</td>\n",
       "      <td>0.598983</td>\n",
       "      <td>0.750758</td>\n",
       "      <td>0.729618</td>\n",
       "      <td>0.666562</td>\n",
       "      <td>0.566543</td>\n",
       "      <td>0.455627</td>\n",
       "      <td>0.365480</td>\n",
       "      <td>-0.020449</td>\n",
       "      <td>0.352176</td>\n",
       "      <td>0.575992</td>\n",
       "      <td>0.592553</td>\n",
       "      <td>0.550527</td>\n",
       "      <td>0.475417</td>\n",
       "      <td>0.390776</td>\n",
       "      <td>0.317640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram16</th>\n",
       "      <td>0.297301</td>\n",
       "      <td>0.584745</td>\n",
       "      <td>0.796535</td>\n",
       "      <td>0.901253</td>\n",
       "      <td>0.968130</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959836</td>\n",
       "      <td>0.899733</td>\n",
       "      <td>-0.015489</td>\n",
       "      <td>0.317167</td>\n",
       "      <td>0.584075</td>\n",
       "      <td>0.704915</td>\n",
       "      <td>0.780976</td>\n",
       "      <td>0.818662</td>\n",
       "      <td>0.796901</td>\n",
       "      <td>0.750420</td>\n",
       "      <td>0.281173</td>\n",
       "      <td>0.552684</td>\n",
       "      <td>0.732936</td>\n",
       "      <td>0.763050</td>\n",
       "      <td>0.724417</td>\n",
       "      <td>0.630067</td>\n",
       "      <td>0.514527</td>\n",
       "      <td>0.416834</td>\n",
       "      <td>0.011017</td>\n",
       "      <td>0.353666</td>\n",
       "      <td>0.580720</td>\n",
       "      <td>0.632146</td>\n",
       "      <td>0.606606</td>\n",
       "      <td>0.534517</td>\n",
       "      <td>0.444673</td>\n",
       "      <td>0.364446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram17</th>\n",
       "      <td>0.240264</td>\n",
       "      <td>0.498815</td>\n",
       "      <td>0.691158</td>\n",
       "      <td>0.799729</td>\n",
       "      <td>0.885447</td>\n",
       "      <td>0.959836</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967820</td>\n",
       "      <td>0.017299</td>\n",
       "      <td>0.302811</td>\n",
       "      <td>0.528914</td>\n",
       "      <td>0.643909</td>\n",
       "      <td>0.729725</td>\n",
       "      <td>0.798709</td>\n",
       "      <td>0.837916</td>\n",
       "      <td>0.812553</td>\n",
       "      <td>0.233926</td>\n",
       "      <td>0.489433</td>\n",
       "      <td>0.678801</td>\n",
       "      <td>0.765281</td>\n",
       "      <td>0.762472</td>\n",
       "      <td>0.684664</td>\n",
       "      <td>0.571712</td>\n",
       "      <td>0.469438</td>\n",
       "      <td>0.039504</td>\n",
       "      <td>0.338731</td>\n",
       "      <td>0.553848</td>\n",
       "      <td>0.644387</td>\n",
       "      <td>0.644758</td>\n",
       "      <td>0.585256</td>\n",
       "      <td>0.496308</td>\n",
       "      <td>0.411714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram18</th>\n",
       "      <td>0.208584</td>\n",
       "      <td>0.444421</td>\n",
       "      <td>0.620564</td>\n",
       "      <td>0.724942</td>\n",
       "      <td>0.813086</td>\n",
       "      <td>0.899733</td>\n",
       "      <td>0.967820</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031584</td>\n",
       "      <td>0.286067</td>\n",
       "      <td>0.486023</td>\n",
       "      <td>0.592977</td>\n",
       "      <td>0.677949</td>\n",
       "      <td>0.755623</td>\n",
       "      <td>0.816205</td>\n",
       "      <td>0.839149</td>\n",
       "      <td>0.208012</td>\n",
       "      <td>0.450295</td>\n",
       "      <td>0.635770</td>\n",
       "      <td>0.742784</td>\n",
       "      <td>0.775996</td>\n",
       "      <td>0.724802</td>\n",
       "      <td>0.626176</td>\n",
       "      <td>0.524838</td>\n",
       "      <td>0.051582</td>\n",
       "      <td>0.324581</td>\n",
       "      <td>0.526182</td>\n",
       "      <td>0.630340</td>\n",
       "      <td>0.659374</td>\n",
       "      <td>0.621999</td>\n",
       "      <td>0.543545</td>\n",
       "      <td>0.459839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram21</th>\n",
       "      <td>-0.583697</td>\n",
       "      <td>-0.398241</td>\n",
       "      <td>-0.184841</td>\n",
       "      <td>-0.104790</td>\n",
       "      <td>-0.050995</td>\n",
       "      <td>-0.015489</td>\n",
       "      <td>0.017299</td>\n",
       "      <td>0.031584</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.779095</td>\n",
       "      <td>0.538284</td>\n",
       "      <td>0.433050</td>\n",
       "      <td>0.355342</td>\n",
       "      <td>0.300212</td>\n",
       "      <td>0.246479</td>\n",
       "      <td>0.216714</td>\n",
       "      <td>-0.587573</td>\n",
       "      <td>-0.276336</td>\n",
       "      <td>-0.086629</td>\n",
       "      <td>-0.012057</td>\n",
       "      <td>0.017443</td>\n",
       "      <td>0.031841</td>\n",
       "      <td>0.039265</td>\n",
       "      <td>0.040367</td>\n",
       "      <td>0.745532</td>\n",
       "      <td>0.512542</td>\n",
       "      <td>0.322757</td>\n",
       "      <td>0.232239</td>\n",
       "      <td>0.187710</td>\n",
       "      <td>0.150757</td>\n",
       "      <td>0.117617</td>\n",
       "      <td>0.093677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram22</th>\n",
       "      <td>-0.399037</td>\n",
       "      <td>0.086422</td>\n",
       "      <td>0.276217</td>\n",
       "      <td>0.311459</td>\n",
       "      <td>0.322956</td>\n",
       "      <td>0.317167</td>\n",
       "      <td>0.302811</td>\n",
       "      <td>0.286067</td>\n",
       "      <td>0.779095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.856390</td>\n",
       "      <td>0.751950</td>\n",
       "      <td>0.663005</td>\n",
       "      <td>0.587220</td>\n",
       "      <td>0.504957</td>\n",
       "      <td>0.452686</td>\n",
       "      <td>-0.361615</td>\n",
       "      <td>0.033870</td>\n",
       "      <td>0.213660</td>\n",
       "      <td>0.240645</td>\n",
       "      <td>0.233100</td>\n",
       "      <td>0.210134</td>\n",
       "      <td>0.180487</td>\n",
       "      <td>0.153431</td>\n",
       "      <td>0.639987</td>\n",
       "      <td>0.645165</td>\n",
       "      <td>0.547885</td>\n",
       "      <td>0.449070</td>\n",
       "      <td>0.383736</td>\n",
       "      <td>0.318714</td>\n",
       "      <td>0.255157</td>\n",
       "      <td>0.205814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram23</th>\n",
       "      <td>-0.185816</td>\n",
       "      <td>0.275733</td>\n",
       "      <td>0.595330</td>\n",
       "      <td>0.629358</td>\n",
       "      <td>0.618215</td>\n",
       "      <td>0.584075</td>\n",
       "      <td>0.528914</td>\n",
       "      <td>0.486023</td>\n",
       "      <td>0.538284</td>\n",
       "      <td>0.856390</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951739</td>\n",
       "      <td>0.878185</td>\n",
       "      <td>0.798512</td>\n",
       "      <td>0.695808</td>\n",
       "      <td>0.626950</td>\n",
       "      <td>-0.151817</td>\n",
       "      <td>0.261561</td>\n",
       "      <td>0.449291</td>\n",
       "      <td>0.439320</td>\n",
       "      <td>0.400807</td>\n",
       "      <td>0.344377</td>\n",
       "      <td>0.284521</td>\n",
       "      <td>0.235157</td>\n",
       "      <td>0.472550</td>\n",
       "      <td>0.662663</td>\n",
       "      <td>0.692884</td>\n",
       "      <td>0.602603</td>\n",
       "      <td>0.525535</td>\n",
       "      <td>0.437137</td>\n",
       "      <td>0.350500</td>\n",
       "      <td>0.282751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram24</th>\n",
       "      <td>-0.105913</td>\n",
       "      <td>0.310127</td>\n",
       "      <td>0.627975</td>\n",
       "      <td>0.725453</td>\n",
       "      <td>0.734604</td>\n",
       "      <td>0.704915</td>\n",
       "      <td>0.643909</td>\n",
       "      <td>0.592977</td>\n",
       "      <td>0.433050</td>\n",
       "      <td>0.751950</td>\n",
       "      <td>0.951739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966582</td>\n",
       "      <td>0.903121</td>\n",
       "      <td>0.803575</td>\n",
       "      <td>0.730421</td>\n",
       "      <td>-0.074192</td>\n",
       "      <td>0.328356</td>\n",
       "      <td>0.534582</td>\n",
       "      <td>0.531856</td>\n",
       "      <td>0.487643</td>\n",
       "      <td>0.418530</td>\n",
       "      <td>0.344476</td>\n",
       "      <td>0.283623</td>\n",
       "      <td>0.395157</td>\n",
       "      <td>0.642599</td>\n",
       "      <td>0.743067</td>\n",
       "      <td>0.681761</td>\n",
       "      <td>0.606996</td>\n",
       "      <td>0.509172</td>\n",
       "      <td>0.410148</td>\n",
       "      <td>0.331778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram25</th>\n",
       "      <td>-0.052851</td>\n",
       "      <td>0.320415</td>\n",
       "      <td>0.615297</td>\n",
       "      <td>0.732928</td>\n",
       "      <td>0.789265</td>\n",
       "      <td>0.780976</td>\n",
       "      <td>0.729725</td>\n",
       "      <td>0.677949</td>\n",
       "      <td>0.355342</td>\n",
       "      <td>0.663005</td>\n",
       "      <td>0.878185</td>\n",
       "      <td>0.966582</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968889</td>\n",
       "      <td>0.887303</td>\n",
       "      <td>0.816348</td>\n",
       "      <td>-0.025622</td>\n",
       "      <td>0.350455</td>\n",
       "      <td>0.572381</td>\n",
       "      <td>0.591228</td>\n",
       "      <td>0.552259</td>\n",
       "      <td>0.478506</td>\n",
       "      <td>0.395356</td>\n",
       "      <td>0.326165</td>\n",
       "      <td>0.333537</td>\n",
       "      <td>0.601265</td>\n",
       "      <td>0.754710</td>\n",
       "      <td>0.732932</td>\n",
       "      <td>0.670281</td>\n",
       "      <td>0.570322</td>\n",
       "      <td>0.463029</td>\n",
       "      <td>0.376384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram26</th>\n",
       "      <td>-0.021169</td>\n",
       "      <td>0.311363</td>\n",
       "      <td>0.578430</td>\n",
       "      <td>0.700569</td>\n",
       "      <td>0.778309</td>\n",
       "      <td>0.818662</td>\n",
       "      <td>0.798709</td>\n",
       "      <td>0.755623</td>\n",
       "      <td>0.300212</td>\n",
       "      <td>0.587220</td>\n",
       "      <td>0.798512</td>\n",
       "      <td>0.903121</td>\n",
       "      <td>0.968889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959418</td>\n",
       "      <td>0.900573</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.349493</td>\n",
       "      <td>0.575153</td>\n",
       "      <td>0.628657</td>\n",
       "      <td>0.606508</td>\n",
       "      <td>0.536116</td>\n",
       "      <td>0.448322</td>\n",
       "      <td>0.372894</td>\n",
       "      <td>0.288085</td>\n",
       "      <td>0.556624</td>\n",
       "      <td>0.738264</td>\n",
       "      <td>0.766808</td>\n",
       "      <td>0.727501</td>\n",
       "      <td>0.633060</td>\n",
       "      <td>0.521053</td>\n",
       "      <td>0.427446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram27</th>\n",
       "      <td>0.008396</td>\n",
       "      <td>0.294594</td>\n",
       "      <td>0.521473</td>\n",
       "      <td>0.637844</td>\n",
       "      <td>0.725335</td>\n",
       "      <td>0.796901</td>\n",
       "      <td>0.837916</td>\n",
       "      <td>0.816205</td>\n",
       "      <td>0.246479</td>\n",
       "      <td>0.504957</td>\n",
       "      <td>0.695808</td>\n",
       "      <td>0.803575</td>\n",
       "      <td>0.887303</td>\n",
       "      <td>0.959418</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968419</td>\n",
       "      <td>0.029808</td>\n",
       "      <td>0.332613</td>\n",
       "      <td>0.546918</td>\n",
       "      <td>0.639259</td>\n",
       "      <td>0.643105</td>\n",
       "      <td>0.585725</td>\n",
       "      <td>0.499283</td>\n",
       "      <td>0.420429</td>\n",
       "      <td>0.242333</td>\n",
       "      <td>0.494008</td>\n",
       "      <td>0.684921</td>\n",
       "      <td>0.770766</td>\n",
       "      <td>0.765576</td>\n",
       "      <td>0.689125</td>\n",
       "      <td>0.578518</td>\n",
       "      <td>0.480486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram28</th>\n",
       "      <td>0.020463</td>\n",
       "      <td>0.275611</td>\n",
       "      <td>0.476293</td>\n",
       "      <td>0.584367</td>\n",
       "      <td>0.670659</td>\n",
       "      <td>0.750420</td>\n",
       "      <td>0.812553</td>\n",
       "      <td>0.839149</td>\n",
       "      <td>0.216714</td>\n",
       "      <td>0.452686</td>\n",
       "      <td>0.626950</td>\n",
       "      <td>0.730421</td>\n",
       "      <td>0.816348</td>\n",
       "      <td>0.900573</td>\n",
       "      <td>0.968419</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040929</td>\n",
       "      <td>0.316614</td>\n",
       "      <td>0.517282</td>\n",
       "      <td>0.622758</td>\n",
       "      <td>0.655112</td>\n",
       "      <td>0.619888</td>\n",
       "      <td>0.544349</td>\n",
       "      <td>0.467888</td>\n",
       "      <td>0.217572</td>\n",
       "      <td>0.455961</td>\n",
       "      <td>0.643164</td>\n",
       "      <td>0.750374</td>\n",
       "      <td>0.781948</td>\n",
       "      <td>0.732466</td>\n",
       "      <td>0.632978</td>\n",
       "      <td>0.535787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram11</th>\n",
       "      <td>0.747008</td>\n",
       "      <td>0.638008</td>\n",
       "      <td>0.467732</td>\n",
       "      <td>0.390908</td>\n",
       "      <td>0.328911</td>\n",
       "      <td>0.281173</td>\n",
       "      <td>0.233926</td>\n",
       "      <td>0.208012</td>\n",
       "      <td>-0.587573</td>\n",
       "      <td>-0.361615</td>\n",
       "      <td>-0.151817</td>\n",
       "      <td>-0.074192</td>\n",
       "      <td>-0.025622</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.029808</td>\n",
       "      <td>0.040929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.673114</td>\n",
       "      <td>0.422968</td>\n",
       "      <td>0.307175</td>\n",
       "      <td>0.243889</td>\n",
       "      <td>0.194377</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.114063</td>\n",
       "      <td>-0.362638</td>\n",
       "      <td>-0.140705</td>\n",
       "      <td>-0.005521</td>\n",
       "      <td>0.053164</td>\n",
       "      <td>0.067011</td>\n",
       "      <td>0.071155</td>\n",
       "      <td>0.068252</td>\n",
       "      <td>0.059140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram12</th>\n",
       "      <td>0.504818</td>\n",
       "      <td>0.639218</td>\n",
       "      <td>0.659145</td>\n",
       "      <td>0.640118</td>\n",
       "      <td>0.598983</td>\n",
       "      <td>0.552684</td>\n",
       "      <td>0.489433</td>\n",
       "      <td>0.450295</td>\n",
       "      <td>-0.276336</td>\n",
       "      <td>0.033870</td>\n",
       "      <td>0.261561</td>\n",
       "      <td>0.328356</td>\n",
       "      <td>0.350455</td>\n",
       "      <td>0.349493</td>\n",
       "      <td>0.332613</td>\n",
       "      <td>0.316614</td>\n",
       "      <td>0.673114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.810457</td>\n",
       "      <td>0.647921</td>\n",
       "      <td>0.537812</td>\n",
       "      <td>0.435738</td>\n",
       "      <td>0.339944</td>\n",
       "      <td>0.265548</td>\n",
       "      <td>-0.133694</td>\n",
       "      <td>0.447180</td>\n",
       "      <td>0.486668</td>\n",
       "      <td>0.441658</td>\n",
       "      <td>0.384973</td>\n",
       "      <td>0.324572</td>\n",
       "      <td>0.265379</td>\n",
       "      <td>0.213606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram13</th>\n",
       "      <td>0.319161</td>\n",
       "      <td>0.543497</td>\n",
       "      <td>0.688804</td>\n",
       "      <td>0.739207</td>\n",
       "      <td>0.750758</td>\n",
       "      <td>0.732936</td>\n",
       "      <td>0.678801</td>\n",
       "      <td>0.635770</td>\n",
       "      <td>-0.086629</td>\n",
       "      <td>0.213660</td>\n",
       "      <td>0.449291</td>\n",
       "      <td>0.534582</td>\n",
       "      <td>0.572381</td>\n",
       "      <td>0.575153</td>\n",
       "      <td>0.546918</td>\n",
       "      <td>0.517282</td>\n",
       "      <td>0.422968</td>\n",
       "      <td>0.810457</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883911</td>\n",
       "      <td>0.755980</td>\n",
       "      <td>0.618973</td>\n",
       "      <td>0.485861</td>\n",
       "      <td>0.380600</td>\n",
       "      <td>-0.001784</td>\n",
       "      <td>0.486028</td>\n",
       "      <td>0.747582</td>\n",
       "      <td>0.701456</td>\n",
       "      <td>0.610307</td>\n",
       "      <td>0.508708</td>\n",
       "      <td>0.409952</td>\n",
       "      <td>0.327128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram14</th>\n",
       "      <td>0.228436</td>\n",
       "      <td>0.444672</td>\n",
       "      <td>0.599281</td>\n",
       "      <td>0.678397</td>\n",
       "      <td>0.729618</td>\n",
       "      <td>0.763050</td>\n",
       "      <td>0.765281</td>\n",
       "      <td>0.742784</td>\n",
       "      <td>-0.012057</td>\n",
       "      <td>0.240645</td>\n",
       "      <td>0.439320</td>\n",
       "      <td>0.531856</td>\n",
       "      <td>0.591228</td>\n",
       "      <td>0.628657</td>\n",
       "      <td>0.639259</td>\n",
       "      <td>0.622758</td>\n",
       "      <td>0.307175</td>\n",
       "      <td>0.647921</td>\n",
       "      <td>0.883911</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.912947</td>\n",
       "      <td>0.771815</td>\n",
       "      <td>0.617844</td>\n",
       "      <td>0.489828</td>\n",
       "      <td>0.056619</td>\n",
       "      <td>0.442644</td>\n",
       "      <td>0.703274</td>\n",
       "      <td>0.816733</td>\n",
       "      <td>0.753136</td>\n",
       "      <td>0.645844</td>\n",
       "      <td>0.528546</td>\n",
       "      <td>0.425909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram15</th>\n",
       "      <td>0.182614</td>\n",
       "      <td>0.378304</td>\n",
       "      <td>0.520568</td>\n",
       "      <td>0.602464</td>\n",
       "      <td>0.666562</td>\n",
       "      <td>0.724417</td>\n",
       "      <td>0.762472</td>\n",
       "      <td>0.775996</td>\n",
       "      <td>0.017443</td>\n",
       "      <td>0.233100</td>\n",
       "      <td>0.400807</td>\n",
       "      <td>0.487643</td>\n",
       "      <td>0.552259</td>\n",
       "      <td>0.606508</td>\n",
       "      <td>0.643105</td>\n",
       "      <td>0.655112</td>\n",
       "      <td>0.243889</td>\n",
       "      <td>0.537812</td>\n",
       "      <td>0.755980</td>\n",
       "      <td>0.912947</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.904727</td>\n",
       "      <td>0.753110</td>\n",
       "      <td>0.610329</td>\n",
       "      <td>0.072415</td>\n",
       "      <td>0.389280</td>\n",
       "      <td>0.615980</td>\n",
       "      <td>0.757959</td>\n",
       "      <td>0.821794</td>\n",
       "      <td>0.755147</td>\n",
       "      <td>0.642893</td>\n",
       "      <td>0.529783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram16</th>\n",
       "      <td>0.145635</td>\n",
       "      <td>0.312699</td>\n",
       "      <td>0.431504</td>\n",
       "      <td>0.504312</td>\n",
       "      <td>0.566543</td>\n",
       "      <td>0.630067</td>\n",
       "      <td>0.684664</td>\n",
       "      <td>0.724802</td>\n",
       "      <td>0.031841</td>\n",
       "      <td>0.210134</td>\n",
       "      <td>0.344377</td>\n",
       "      <td>0.418530</td>\n",
       "      <td>0.478506</td>\n",
       "      <td>0.536116</td>\n",
       "      <td>0.585725</td>\n",
       "      <td>0.619888</td>\n",
       "      <td>0.194377</td>\n",
       "      <td>0.435738</td>\n",
       "      <td>0.618973</td>\n",
       "      <td>0.771815</td>\n",
       "      <td>0.904727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.895936</td>\n",
       "      <td>0.752604</td>\n",
       "      <td>0.077836</td>\n",
       "      <td>0.330974</td>\n",
       "      <td>0.516547</td>\n",
       "      <td>0.653611</td>\n",
       "      <td>0.759243</td>\n",
       "      <td>0.825593</td>\n",
       "      <td>0.757280</td>\n",
       "      <td>0.647947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram17</th>\n",
       "      <td>0.110902</td>\n",
       "      <td>0.246075</td>\n",
       "      <td>0.341368</td>\n",
       "      <td>0.401746</td>\n",
       "      <td>0.455627</td>\n",
       "      <td>0.514527</td>\n",
       "      <td>0.571712</td>\n",
       "      <td>0.626176</td>\n",
       "      <td>0.039265</td>\n",
       "      <td>0.180487</td>\n",
       "      <td>0.284521</td>\n",
       "      <td>0.344476</td>\n",
       "      <td>0.395356</td>\n",
       "      <td>0.448322</td>\n",
       "      <td>0.499283</td>\n",
       "      <td>0.544349</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.339944</td>\n",
       "      <td>0.485861</td>\n",
       "      <td>0.617844</td>\n",
       "      <td>0.753110</td>\n",
       "      <td>0.895936</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.895440</td>\n",
       "      <td>0.075844</td>\n",
       "      <td>0.272378</td>\n",
       "      <td>0.417938</td>\n",
       "      <td>0.536762</td>\n",
       "      <td>0.648298</td>\n",
       "      <td>0.758492</td>\n",
       "      <td>0.832089</td>\n",
       "      <td>0.760841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram18</th>\n",
       "      <td>0.084278</td>\n",
       "      <td>0.194360</td>\n",
       "      <td>0.271136</td>\n",
       "      <td>0.320386</td>\n",
       "      <td>0.365480</td>\n",
       "      <td>0.416834</td>\n",
       "      <td>0.469438</td>\n",
       "      <td>0.524838</td>\n",
       "      <td>0.040367</td>\n",
       "      <td>0.153431</td>\n",
       "      <td>0.235157</td>\n",
       "      <td>0.283623</td>\n",
       "      <td>0.326165</td>\n",
       "      <td>0.372894</td>\n",
       "      <td>0.420429</td>\n",
       "      <td>0.467888</td>\n",
       "      <td>0.114063</td>\n",
       "      <td>0.265548</td>\n",
       "      <td>0.380600</td>\n",
       "      <td>0.489828</td>\n",
       "      <td>0.610329</td>\n",
       "      <td>0.752604</td>\n",
       "      <td>0.895440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069305</td>\n",
       "      <td>0.222844</td>\n",
       "      <td>0.337288</td>\n",
       "      <td>0.437232</td>\n",
       "      <td>0.540484</td>\n",
       "      <td>0.657618</td>\n",
       "      <td>0.773088</td>\n",
       "      <td>0.833366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram21</th>\n",
       "      <td>-0.585551</td>\n",
       "      <td>-0.358341</td>\n",
       "      <td>-0.147329</td>\n",
       "      <td>-0.069602</td>\n",
       "      <td>-0.020449</td>\n",
       "      <td>0.011017</td>\n",
       "      <td>0.039504</td>\n",
       "      <td>0.051582</td>\n",
       "      <td>0.745532</td>\n",
       "      <td>0.639987</td>\n",
       "      <td>0.472550</td>\n",
       "      <td>0.395157</td>\n",
       "      <td>0.333537</td>\n",
       "      <td>0.288085</td>\n",
       "      <td>0.242333</td>\n",
       "      <td>0.217572</td>\n",
       "      <td>-0.362638</td>\n",
       "      <td>-0.133694</td>\n",
       "      <td>-0.001784</td>\n",
       "      <td>0.056619</td>\n",
       "      <td>0.072415</td>\n",
       "      <td>0.077836</td>\n",
       "      <td>0.075844</td>\n",
       "      <td>0.069305</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.678910</td>\n",
       "      <td>0.426294</td>\n",
       "      <td>0.308831</td>\n",
       "      <td>0.247492</td>\n",
       "      <td>0.199271</td>\n",
       "      <td>0.155092</td>\n",
       "      <td>0.123074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram22</th>\n",
       "      <td>-0.284581</td>\n",
       "      <td>0.028699</td>\n",
       "      <td>0.259630</td>\n",
       "      <td>0.328234</td>\n",
       "      <td>0.352176</td>\n",
       "      <td>0.353666</td>\n",
       "      <td>0.338731</td>\n",
       "      <td>0.324581</td>\n",
       "      <td>0.512542</td>\n",
       "      <td>0.645165</td>\n",
       "      <td>0.662663</td>\n",
       "      <td>0.642599</td>\n",
       "      <td>0.601265</td>\n",
       "      <td>0.556624</td>\n",
       "      <td>0.494008</td>\n",
       "      <td>0.455961</td>\n",
       "      <td>-0.140705</td>\n",
       "      <td>0.447180</td>\n",
       "      <td>0.486028</td>\n",
       "      <td>0.442644</td>\n",
       "      <td>0.389280</td>\n",
       "      <td>0.330974</td>\n",
       "      <td>0.272378</td>\n",
       "      <td>0.222844</td>\n",
       "      <td>0.678910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.809194</td>\n",
       "      <td>0.646118</td>\n",
       "      <td>0.537680</td>\n",
       "      <td>0.436905</td>\n",
       "      <td>0.344168</td>\n",
       "      <td>0.273742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram23</th>\n",
       "      <td>-0.090015</td>\n",
       "      <td>0.212071</td>\n",
       "      <td>0.449512</td>\n",
       "      <td>0.536447</td>\n",
       "      <td>0.575992</td>\n",
       "      <td>0.580720</td>\n",
       "      <td>0.553848</td>\n",
       "      <td>0.526182</td>\n",
       "      <td>0.322757</td>\n",
       "      <td>0.547885</td>\n",
       "      <td>0.692884</td>\n",
       "      <td>0.743067</td>\n",
       "      <td>0.754710</td>\n",
       "      <td>0.738264</td>\n",
       "      <td>0.684921</td>\n",
       "      <td>0.643164</td>\n",
       "      <td>-0.005521</td>\n",
       "      <td>0.486668</td>\n",
       "      <td>0.747582</td>\n",
       "      <td>0.703274</td>\n",
       "      <td>0.615980</td>\n",
       "      <td>0.516547</td>\n",
       "      <td>0.417938</td>\n",
       "      <td>0.337288</td>\n",
       "      <td>0.426294</td>\n",
       "      <td>0.809194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884300</td>\n",
       "      <td>0.757718</td>\n",
       "      <td>0.621615</td>\n",
       "      <td>0.491433</td>\n",
       "      <td>0.390952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram24</th>\n",
       "      <td>-0.015892</td>\n",
       "      <td>0.237728</td>\n",
       "      <td>0.437448</td>\n",
       "      <td>0.531462</td>\n",
       "      <td>0.592553</td>\n",
       "      <td>0.632146</td>\n",
       "      <td>0.644387</td>\n",
       "      <td>0.630340</td>\n",
       "      <td>0.232239</td>\n",
       "      <td>0.449070</td>\n",
       "      <td>0.602603</td>\n",
       "      <td>0.681761</td>\n",
       "      <td>0.732932</td>\n",
       "      <td>0.766808</td>\n",
       "      <td>0.770766</td>\n",
       "      <td>0.750374</td>\n",
       "      <td>0.053164</td>\n",
       "      <td>0.441658</td>\n",
       "      <td>0.701456</td>\n",
       "      <td>0.816733</td>\n",
       "      <td>0.757959</td>\n",
       "      <td>0.653611</td>\n",
       "      <td>0.536762</td>\n",
       "      <td>0.437232</td>\n",
       "      <td>0.308831</td>\n",
       "      <td>0.646118</td>\n",
       "      <td>0.884300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914514</td>\n",
       "      <td>0.773934</td>\n",
       "      <td>0.622565</td>\n",
       "      <td>0.500829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram25</th>\n",
       "      <td>0.011320</td>\n",
       "      <td>0.227962</td>\n",
       "      <td>0.396486</td>\n",
       "      <td>0.484485</td>\n",
       "      <td>0.550527</td>\n",
       "      <td>0.606606</td>\n",
       "      <td>0.644758</td>\n",
       "      <td>0.659374</td>\n",
       "      <td>0.187710</td>\n",
       "      <td>0.383736</td>\n",
       "      <td>0.525535</td>\n",
       "      <td>0.606996</td>\n",
       "      <td>0.670281</td>\n",
       "      <td>0.727501</td>\n",
       "      <td>0.765576</td>\n",
       "      <td>0.781948</td>\n",
       "      <td>0.067011</td>\n",
       "      <td>0.384973</td>\n",
       "      <td>0.610307</td>\n",
       "      <td>0.753136</td>\n",
       "      <td>0.821794</td>\n",
       "      <td>0.759243</td>\n",
       "      <td>0.648298</td>\n",
       "      <td>0.540484</td>\n",
       "      <td>0.247492</td>\n",
       "      <td>0.537680</td>\n",
       "      <td>0.757718</td>\n",
       "      <td>0.914514</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.904792</td>\n",
       "      <td>0.755260</td>\n",
       "      <td>0.621013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram26</th>\n",
       "      <td>0.025278</td>\n",
       "      <td>0.204694</td>\n",
       "      <td>0.339467</td>\n",
       "      <td>0.414394</td>\n",
       "      <td>0.475417</td>\n",
       "      <td>0.534517</td>\n",
       "      <td>0.585256</td>\n",
       "      <td>0.621999</td>\n",
       "      <td>0.150757</td>\n",
       "      <td>0.318714</td>\n",
       "      <td>0.437137</td>\n",
       "      <td>0.509172</td>\n",
       "      <td>0.570322</td>\n",
       "      <td>0.633060</td>\n",
       "      <td>0.689125</td>\n",
       "      <td>0.732466</td>\n",
       "      <td>0.071155</td>\n",
       "      <td>0.324572</td>\n",
       "      <td>0.508708</td>\n",
       "      <td>0.645844</td>\n",
       "      <td>0.755147</td>\n",
       "      <td>0.825593</td>\n",
       "      <td>0.758492</td>\n",
       "      <td>0.657618</td>\n",
       "      <td>0.199271</td>\n",
       "      <td>0.436905</td>\n",
       "      <td>0.621615</td>\n",
       "      <td>0.773934</td>\n",
       "      <td>0.904792</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893840</td>\n",
       "      <td>0.761984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram27</th>\n",
       "      <td>0.031009</td>\n",
       "      <td>0.173930</td>\n",
       "      <td>0.278754</td>\n",
       "      <td>0.339238</td>\n",
       "      <td>0.390776</td>\n",
       "      <td>0.444673</td>\n",
       "      <td>0.496308</td>\n",
       "      <td>0.543545</td>\n",
       "      <td>0.117617</td>\n",
       "      <td>0.255157</td>\n",
       "      <td>0.350500</td>\n",
       "      <td>0.410148</td>\n",
       "      <td>0.463029</td>\n",
       "      <td>0.521053</td>\n",
       "      <td>0.578518</td>\n",
       "      <td>0.632978</td>\n",
       "      <td>0.068252</td>\n",
       "      <td>0.265379</td>\n",
       "      <td>0.409952</td>\n",
       "      <td>0.528546</td>\n",
       "      <td>0.642893</td>\n",
       "      <td>0.757280</td>\n",
       "      <td>0.832089</td>\n",
       "      <td>0.773088</td>\n",
       "      <td>0.155092</td>\n",
       "      <td>0.344168</td>\n",
       "      <td>0.491433</td>\n",
       "      <td>0.622565</td>\n",
       "      <td>0.755260</td>\n",
       "      <td>0.893840</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram28</th>\n",
       "      <td>0.029033</td>\n",
       "      <td>0.143534</td>\n",
       "      <td>0.226180</td>\n",
       "      <td>0.274942</td>\n",
       "      <td>0.317640</td>\n",
       "      <td>0.364446</td>\n",
       "      <td>0.411714</td>\n",
       "      <td>0.459839</td>\n",
       "      <td>0.093677</td>\n",
       "      <td>0.205814</td>\n",
       "      <td>0.282751</td>\n",
       "      <td>0.331778</td>\n",
       "      <td>0.376384</td>\n",
       "      <td>0.427446</td>\n",
       "      <td>0.480486</td>\n",
       "      <td>0.535787</td>\n",
       "      <td>0.059140</td>\n",
       "      <td>0.213606</td>\n",
       "      <td>0.327128</td>\n",
       "      <td>0.425909</td>\n",
       "      <td>0.529783</td>\n",
       "      <td>0.647947</td>\n",
       "      <td>0.760841</td>\n",
       "      <td>0.833366</td>\n",
       "      <td>0.123074</td>\n",
       "      <td>0.273742</td>\n",
       "      <td>0.390952</td>\n",
       "      <td>0.500829</td>\n",
       "      <td>0.621013</td>\n",
       "      <td>0.761984</td>\n",
       "      <td>0.908296</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ngram11   ngram12   ngram13   ngram14   ngram15   ngram16   ngram17   ngram18   ngram21   ngram22   ngram23   ngram24   ngram25   ngram26   ngram27   ngram28   ngram11   ngram12   ngram13   ngram14   ngram15   ngram16   ngram17   ngram18   ngram21   ngram22   ngram23   ngram24   ngram25   ngram26   ngram27   ngram28\n",
       "ngram11  1.000000  0.779065  0.537676  0.432923  0.355230  0.297301  0.240264  0.208584 -0.583697 -0.399037 -0.185816 -0.105913 -0.052851 -0.021169  0.008396  0.020463  0.747008  0.504818  0.319161  0.228436  0.182614  0.145635  0.110902  0.084278 -0.585551 -0.284581 -0.090015 -0.015892  0.011320  0.025278  0.031009  0.029033\n",
       "ngram12  0.779065  1.000000  0.856481  0.751880  0.663182  0.584745  0.498815  0.444421 -0.398241  0.086422  0.275733  0.310127  0.320415  0.311363  0.294594  0.275611  0.638008  0.639218  0.543497  0.444672  0.378304  0.312699  0.246075  0.194360 -0.358341  0.028699  0.212071  0.237728  0.227962  0.204694  0.173930  0.143534\n",
       "ngram13  0.537676  0.856481  1.000000  0.951748  0.878298  0.796535  0.691158  0.620564 -0.184841  0.276217  0.595330  0.627975  0.615297  0.578430  0.521473  0.476293  0.467732  0.659145  0.688804  0.599281  0.520568  0.431504  0.341368  0.271136 -0.147329  0.259630  0.449512  0.437448  0.396486  0.339467  0.278754  0.226180\n",
       "ngram14  0.432923  0.751880  0.951748  1.000000  0.966365  0.901253  0.799729  0.724942 -0.104790  0.311459  0.629358  0.725453  0.732928  0.700569  0.637844  0.584367  0.390908  0.640118  0.739207  0.678397  0.602464  0.504312  0.401746  0.320386 -0.069602  0.328234  0.536447  0.531462  0.484485  0.414394  0.339238  0.274942\n",
       "ngram15  0.355230  0.663182  0.878298  0.966365  1.000000  0.968130  0.885447  0.813086 -0.050995  0.322956  0.618215  0.734604  0.789265  0.778309  0.725335  0.670659  0.328911  0.598983  0.750758  0.729618  0.666562  0.566543  0.455627  0.365480 -0.020449  0.352176  0.575992  0.592553  0.550527  0.475417  0.390776  0.317640\n",
       "ngram16  0.297301  0.584745  0.796535  0.901253  0.968130  1.000000  0.959836  0.899733 -0.015489  0.317167  0.584075  0.704915  0.780976  0.818662  0.796901  0.750420  0.281173  0.552684  0.732936  0.763050  0.724417  0.630067  0.514527  0.416834  0.011017  0.353666  0.580720  0.632146  0.606606  0.534517  0.444673  0.364446\n",
       "ngram17  0.240264  0.498815  0.691158  0.799729  0.885447  0.959836  1.000000  0.967820  0.017299  0.302811  0.528914  0.643909  0.729725  0.798709  0.837916  0.812553  0.233926  0.489433  0.678801  0.765281  0.762472  0.684664  0.571712  0.469438  0.039504  0.338731  0.553848  0.644387  0.644758  0.585256  0.496308  0.411714\n",
       "ngram18  0.208584  0.444421  0.620564  0.724942  0.813086  0.899733  0.967820  1.000000  0.031584  0.286067  0.486023  0.592977  0.677949  0.755623  0.816205  0.839149  0.208012  0.450295  0.635770  0.742784  0.775996  0.724802  0.626176  0.524838  0.051582  0.324581  0.526182  0.630340  0.659374  0.621999  0.543545  0.459839\n",
       "ngram21 -0.583697 -0.398241 -0.184841 -0.104790 -0.050995 -0.015489  0.017299  0.031584  1.000000  0.779095  0.538284  0.433050  0.355342  0.300212  0.246479  0.216714 -0.587573 -0.276336 -0.086629 -0.012057  0.017443  0.031841  0.039265  0.040367  0.745532  0.512542  0.322757  0.232239  0.187710  0.150757  0.117617  0.093677\n",
       "ngram22 -0.399037  0.086422  0.276217  0.311459  0.322956  0.317167  0.302811  0.286067  0.779095  1.000000  0.856390  0.751950  0.663005  0.587220  0.504957  0.452686 -0.361615  0.033870  0.213660  0.240645  0.233100  0.210134  0.180487  0.153431  0.639987  0.645165  0.547885  0.449070  0.383736  0.318714  0.255157  0.205814\n",
       "ngram23 -0.185816  0.275733  0.595330  0.629358  0.618215  0.584075  0.528914  0.486023  0.538284  0.856390  1.000000  0.951739  0.878185  0.798512  0.695808  0.626950 -0.151817  0.261561  0.449291  0.439320  0.400807  0.344377  0.284521  0.235157  0.472550  0.662663  0.692884  0.602603  0.525535  0.437137  0.350500  0.282751\n",
       "ngram24 -0.105913  0.310127  0.627975  0.725453  0.734604  0.704915  0.643909  0.592977  0.433050  0.751950  0.951739  1.000000  0.966582  0.903121  0.803575  0.730421 -0.074192  0.328356  0.534582  0.531856  0.487643  0.418530  0.344476  0.283623  0.395157  0.642599  0.743067  0.681761  0.606996  0.509172  0.410148  0.331778\n",
       "ngram25 -0.052851  0.320415  0.615297  0.732928  0.789265  0.780976  0.729725  0.677949  0.355342  0.663005  0.878185  0.966582  1.000000  0.968889  0.887303  0.816348 -0.025622  0.350455  0.572381  0.591228  0.552259  0.478506  0.395356  0.326165  0.333537  0.601265  0.754710  0.732932  0.670281  0.570322  0.463029  0.376384\n",
       "ngram26 -0.021169  0.311363  0.578430  0.700569  0.778309  0.818662  0.798709  0.755623  0.300212  0.587220  0.798512  0.903121  0.968889  1.000000  0.959418  0.900573  0.003115  0.349493  0.575153  0.628657  0.606508  0.536116  0.448322  0.372894  0.288085  0.556624  0.738264  0.766808  0.727501  0.633060  0.521053  0.427446\n",
       "ngram27  0.008396  0.294594  0.521473  0.637844  0.725335  0.796901  0.837916  0.816205  0.246479  0.504957  0.695808  0.803575  0.887303  0.959418  1.000000  0.968419  0.029808  0.332613  0.546918  0.639259  0.643105  0.585725  0.499283  0.420429  0.242333  0.494008  0.684921  0.770766  0.765576  0.689125  0.578518  0.480486\n",
       "ngram28  0.020463  0.275611  0.476293  0.584367  0.670659  0.750420  0.812553  0.839149  0.216714  0.452686  0.626950  0.730421  0.816348  0.900573  0.968419  1.000000  0.040929  0.316614  0.517282  0.622758  0.655112  0.619888  0.544349  0.467888  0.217572  0.455961  0.643164  0.750374  0.781948  0.732466  0.632978  0.535787\n",
       "ngram11  0.747008  0.638008  0.467732  0.390908  0.328911  0.281173  0.233926  0.208012 -0.587573 -0.361615 -0.151817 -0.074192 -0.025622  0.003115  0.029808  0.040929  1.000000  0.673114  0.422968  0.307175  0.243889  0.194377  0.148492  0.114063 -0.362638 -0.140705 -0.005521  0.053164  0.067011  0.071155  0.068252  0.059140\n",
       "ngram12  0.504818  0.639218  0.659145  0.640118  0.598983  0.552684  0.489433  0.450295 -0.276336  0.033870  0.261561  0.328356  0.350455  0.349493  0.332613  0.316614  0.673114  1.000000  0.810457  0.647921  0.537812  0.435738  0.339944  0.265548 -0.133694  0.447180  0.486668  0.441658  0.384973  0.324572  0.265379  0.213606\n",
       "ngram13  0.319161  0.543497  0.688804  0.739207  0.750758  0.732936  0.678801  0.635770 -0.086629  0.213660  0.449291  0.534582  0.572381  0.575153  0.546918  0.517282  0.422968  0.810457  1.000000  0.883911  0.755980  0.618973  0.485861  0.380600 -0.001784  0.486028  0.747582  0.701456  0.610307  0.508708  0.409952  0.327128\n",
       "ngram14  0.228436  0.444672  0.599281  0.678397  0.729618  0.763050  0.765281  0.742784 -0.012057  0.240645  0.439320  0.531856  0.591228  0.628657  0.639259  0.622758  0.307175  0.647921  0.883911  1.000000  0.912947  0.771815  0.617844  0.489828  0.056619  0.442644  0.703274  0.816733  0.753136  0.645844  0.528546  0.425909\n",
       "ngram15  0.182614  0.378304  0.520568  0.602464  0.666562  0.724417  0.762472  0.775996  0.017443  0.233100  0.400807  0.487643  0.552259  0.606508  0.643105  0.655112  0.243889  0.537812  0.755980  0.912947  1.000000  0.904727  0.753110  0.610329  0.072415  0.389280  0.615980  0.757959  0.821794  0.755147  0.642893  0.529783\n",
       "ngram16  0.145635  0.312699  0.431504  0.504312  0.566543  0.630067  0.684664  0.724802  0.031841  0.210134  0.344377  0.418530  0.478506  0.536116  0.585725  0.619888  0.194377  0.435738  0.618973  0.771815  0.904727  1.000000  0.895936  0.752604  0.077836  0.330974  0.516547  0.653611  0.759243  0.825593  0.757280  0.647947\n",
       "ngram17  0.110902  0.246075  0.341368  0.401746  0.455627  0.514527  0.571712  0.626176  0.039265  0.180487  0.284521  0.344476  0.395356  0.448322  0.499283  0.544349  0.148492  0.339944  0.485861  0.617844  0.753110  0.895936  1.000000  0.895440  0.075844  0.272378  0.417938  0.536762  0.648298  0.758492  0.832089  0.760841\n",
       "ngram18  0.084278  0.194360  0.271136  0.320386  0.365480  0.416834  0.469438  0.524838  0.040367  0.153431  0.235157  0.283623  0.326165  0.372894  0.420429  0.467888  0.114063  0.265548  0.380600  0.489828  0.610329  0.752604  0.895440  1.000000  0.069305  0.222844  0.337288  0.437232  0.540484  0.657618  0.773088  0.833366\n",
       "ngram21 -0.585551 -0.358341 -0.147329 -0.069602 -0.020449  0.011017  0.039504  0.051582  0.745532  0.639987  0.472550  0.395157  0.333537  0.288085  0.242333  0.217572 -0.362638 -0.133694 -0.001784  0.056619  0.072415  0.077836  0.075844  0.069305  1.000000  0.678910  0.426294  0.308831  0.247492  0.199271  0.155092  0.123074\n",
       "ngram22 -0.284581  0.028699  0.259630  0.328234  0.352176  0.353666  0.338731  0.324581  0.512542  0.645165  0.662663  0.642599  0.601265  0.556624  0.494008  0.455961 -0.140705  0.447180  0.486028  0.442644  0.389280  0.330974  0.272378  0.222844  0.678910  1.000000  0.809194  0.646118  0.537680  0.436905  0.344168  0.273742\n",
       "ngram23 -0.090015  0.212071  0.449512  0.536447  0.575992  0.580720  0.553848  0.526182  0.322757  0.547885  0.692884  0.743067  0.754710  0.738264  0.684921  0.643164 -0.005521  0.486668  0.747582  0.703274  0.615980  0.516547  0.417938  0.337288  0.426294  0.809194  1.000000  0.884300  0.757718  0.621615  0.491433  0.390952\n",
       "ngram24 -0.015892  0.237728  0.437448  0.531462  0.592553  0.632146  0.644387  0.630340  0.232239  0.449070  0.602603  0.681761  0.732932  0.766808  0.770766  0.750374  0.053164  0.441658  0.701456  0.816733  0.757959  0.653611  0.536762  0.437232  0.308831  0.646118  0.884300  1.000000  0.914514  0.773934  0.622565  0.500829\n",
       "ngram25  0.011320  0.227962  0.396486  0.484485  0.550527  0.606606  0.644758  0.659374  0.187710  0.383736  0.525535  0.606996  0.670281  0.727501  0.765576  0.781948  0.067011  0.384973  0.610307  0.753136  0.821794  0.759243  0.648298  0.540484  0.247492  0.537680  0.757718  0.914514  1.000000  0.904792  0.755260  0.621013\n",
       "ngram26  0.025278  0.204694  0.339467  0.414394  0.475417  0.534517  0.585256  0.621999  0.150757  0.318714  0.437137  0.509172  0.570322  0.633060  0.689125  0.732466  0.071155  0.324572  0.508708  0.645844  0.755147  0.825593  0.758492  0.657618  0.199271  0.436905  0.621615  0.773934  0.904792  1.000000  0.893840  0.761984\n",
       "ngram27  0.031009  0.173930  0.278754  0.339238  0.390776  0.444673  0.496308  0.543545  0.117617  0.255157  0.350500  0.410148  0.463029  0.521053  0.578518  0.632978  0.068252  0.265379  0.409952  0.528546  0.642893  0.757280  0.832089  0.773088  0.155092  0.344168  0.491433  0.622565  0.755260  0.893840  1.000000  0.908296\n",
       "ngram28  0.029033  0.143534  0.226180  0.274942  0.317640  0.364446  0.411714  0.459839  0.093677  0.205814  0.282751  0.331778  0.376384  0.427446  0.480486  0.535787  0.059140  0.213606  0.327128  0.425909  0.529783  0.647947  0.760841  0.833366  0.123074  0.273742  0.390952  0.500829  0.621013  0.761984  0.908296  1.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feas(save_data_char)\n",
    "test_feas(save_data_word)\n",
    "#test_feas()\n",
    "combine_feas = pd.concat([save_data_char, save_data_word], axis=1)\n",
    "test_feas(combine_feas)\n",
    "combine_feas.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_feas = [save_data_char, save_data_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "\n",
    "NUM_TOPICS = 300\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11 239 2 213</td>\n",
       "      <td>3 5 2 18 149 5 213 8 11 980 588 40 5 30 5 172 106</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>103 152 31 2 8 71 599 7 136 3142 7</td>\n",
       "      <td>819 133 136 211 2 32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2 114 181 10 23 20 9</td>\n",
       "      <td>3 5 201 13 20 2 120 9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>56 51 4</td>\n",
       "      <td>67 560 51 4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2 319 585</td>\n",
       "      <td>212 1033 13 15 2 9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                           question1                                          question2  label\n",
       "0   1                        11 239 2 213  3 5 2 18 149 5 213 8 11 980 588 40 5 30 5 172 106      1\n",
       "1   2  103 152 31 2 8 71 599 7 136 3142 7                               819 133 136 211 2 32      0\n",
       "2   3                2 114 181 10 23 20 9                              3 5 201 13 20 2 120 9      0\n",
       "3   4                             56 51 4                                        67 560 51 4      0\n",
       "4   5                           2 319 585                                 212 1033 13 15 2 9      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_word.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(train_data):\n",
    "    documents = list(train_data.iloc[:, 1])\n",
    "    documents.extend(list(train_data.iloc[:, 2]))\n",
    "    print documents[:10]\n",
    "    documents = [item.split(' ') for item in documents]\n",
    "    dictionary = Dictionary(documents)\n",
    "    corpus = [dictionary.doc2bow(document) for document in documents]\n",
    "    model = LdaMulticore(\n",
    "        corpus,\n",
    "        num_topics=NUM_TOPICS,\n",
    "        id2word=dictionary,\n",
    "        random_state=RANDOM_SEED,\n",
    "    )\n",
    "    return model, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_topic_distances(model, dictionary, pair):\n",
    "    q1_bow = dictionary.doc2bow(pair[0])\n",
    "    q2_bow = dictionary.doc2bow(pair[1])\n",
    "    \n",
    "    q1_topic_vec = np.array(model.get_document_topics(q1_bow, minimum_probability=0))[:, 1].reshape(1, -1)\n",
    "    q2_topic_vec = np.array(model.get_document_topics(q2_bow, minimum_probability=0))[:, 1].reshape(1, -1)\n",
    "    \n",
    "    return [\n",
    "        cosine_distances(q1_topic_vec, q2_topic_vec)[0][0],\n",
    "        euclidean_distances(q1_topic_vec, q2_topic_vec)[0][0],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_lda(csv_data):\n",
    "    cosine_lt = []\n",
    "    euclidean_lt = []\n",
    "    model, dictionary = build_model(csv_data)\n",
    "    for i in range(csv_data.shape[0]):\n",
    "        cosine_val, euclidean_val = compute_topic_distances(model, dictionary, (csv_data.iloc[i, 1].split(' '), csv_data.iloc[i, 2].split(' ')))\n",
    "        cosine_lt.append(cosine_val)\n",
    "        euclidean_lt.append(euclidean_val)\n",
    "    return cosine_lt, euclidean_lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1476 15 4 184 128 3 2 56 72 43 59', '159 31 13 10 3 2 14 95 65 87 10 199 123 195 10', '3 2 153 130 23 52 5 21 31 36 16', '76 84 258 226 68 94 6 2', '3 2 232 59 25 34', '3 2 70 19 52 13 21 29 19 16', '3 2 29 19 93 681', '6 2 70 19 236 57 36 226', '6 2 568 464 41 19 48 5 9 46 75 5 21 29 19 16', '3 2 26 17 4 13 21 33 25 56 72 143 291']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2f5ef9868734>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcosine_lt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meuclidean_lt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_lda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msave_data_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'cosine_distances'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcosine_lt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'euclidean_distances'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0meuclidean_lt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msave_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../lda_features_char.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcosine_lt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meuclidean_lt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_lda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_data' is not defined"
     ]
    }
   ],
   "source": [
    "cosine_lt, euclidean_lt = cal_lda(train_data_char)\n",
    "save_data_char = pd.DataFrame({'cosine_distances':cosine_lt, 'euclidean_distances':euclidean_lt})\n",
    "save_data.to_csv('../lda_features_char.csv', index=False)\n",
    "\n",
    "cosine_lt, euclidean_lt = cal_lda(train_data_word)\n",
    "save_data_word = pd.DataFrame({'cosine_distances':cosine_lt, 'euclidean_distances':euclidean_lt})\n",
    "save_data.to_csv('../lda_features_word.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  0.3482502133886111\n",
      "weights:  0.33651900941750956\n",
      "weights:  0.365155705182157\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cosine_distances</th>\n",
       "      <th>euclidean_distances</th>\n",
       "      <th>cosine_distances</th>\n",
       "      <th>euclidean_distances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cosine_distances</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914287</td>\n",
       "      <td>0.450425</td>\n",
       "      <td>0.422207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euclidean_distances</th>\n",
       "      <td>0.914287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419013</td>\n",
       "      <td>0.460359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosine_distances</th>\n",
       "      <td>0.450425</td>\n",
       "      <td>0.419013</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euclidean_distances</th>\n",
       "      <td>0.422207</td>\n",
       "      <td>0.460359</td>\n",
       "      <td>0.893685</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     cosine_distances  euclidean_distances  cosine_distances  euclidean_distances\n",
       "cosine_distances             1.000000             0.914287          0.450425             0.422207\n",
       "euclidean_distances          0.914287             1.000000          0.419013             0.460359\n",
       "cosine_distances             0.450425             0.419013          1.000000             0.893685\n",
       "euclidean_distances          0.422207             0.460359          0.893685             1.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feas(save_data_char)\n",
    "test_feas(save_data_word)\n",
    "combine_feas = pd.concat([save_data_char, save_data_word], axis=1)\n",
    "test_feas(combine_feas)\n",
    "combine_feas.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_feas = [save_data_char, save_data_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Simple Summary Statistics\n",
    "\n",
    "简单的统计类特征\n",
    "\n",
    "包括问题的最短长度，最长长度，长度差，长度比率，交集并集的比率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_difference_ratio(q1_tokens, q2_tokens):\n",
    "    return 1.0 * len(set(q1_tokens) ^ set(q2_tokens)) / (len(set(q1_tokens)) + len(set(q2_tokens)))\n",
    "\n",
    "def extract_tokenized_features(pair):\n",
    "    q1 = pair[0]\n",
    "    q2 = pair[1]\n",
    "    \n",
    "    shorter_token_length = min(len(q1), len(q2))\n",
    "    longer_token_length = max(len(q1), len(q2))\n",
    "    \n",
    "    return [\n",
    "        np.log(shorter_token_length + 1),\n",
    "        np.log(longer_token_length + 1),\n",
    "        np.log(abs(longer_token_length - shorter_token_length) + 1),\n",
    "        1.0 * shorter_token_length / longer_token_length,\n",
    "        word_difference_ratio(q1, q2),\n",
    "    ]\n",
    "def cal_summary(csv_data):\n",
    "    short_lt = []\n",
    "    long_lt = []\n",
    "    diff_lt = []\n",
    "    diff_ratio_lt = []\n",
    "    word_difference_ratio_lt = []\n",
    "    for i in range(csv_data.shape[0]):\n",
    "        a1,a2,a3,a4,a5 = extract_tokenized_features((csv_data.iloc[i, 1].split(' '), csv_data.iloc[i, 2].split(' ')))\n",
    "        short_lt.append(a1)\n",
    "        long_lt.append(a2)\n",
    "        diff_lt.append(a3)\n",
    "        diff_ratio_lt.append(a4)\n",
    "        word_difference_ratio_lt.append(a5)\n",
    "    return short_lt, long_lt, diff_lt, diff_ratio_lt, word_difference_ratio_lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_lt, long_lt, diff_lt, diff_ratio_lt, word_difference_ratio_lt = cal_summary(train_data_char)\n",
    "save_data_char = pd.DataFrame({'short_lt':short_lt,\n",
    "                          'long_lt':long_lt,\n",
    "                          'diff_lt':diff_lt,\n",
    "                          'diff_ratio_lt':diff_ratio_lt,\n",
    "                          'word_difference_ratio_lt':word_difference_ratio_lt})\n",
    "save_data.to_csv('../simsummary_features_char.csv', index=False)\n",
    "\n",
    "short_lt, long_lt, diff_lt, diff_ratio_lt, word_difference_ratio_lt = cal_summary(train_data_word)\n",
    "save_data_word = pd.DataFrame({'short_lt':short_lt,\n",
    "                          'long_lt':long_lt,\n",
    "                          'diff_lt':diff_lt,\n",
    "                          'diff_ratio_lt':diff_ratio_lt,\n",
    "                          'word_difference_ratio_lt':word_difference_ratio_lt})\n",
    "save_data.to_csv('../simsummary_features_word.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  0.4000280721454137\n",
      "weights:  0.3464224384659416\n",
      "weights:  0.40230203855221186\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff_lt</th>\n",
       "      <th>diff_ratio_lt</th>\n",
       "      <th>long_lt</th>\n",
       "      <th>short_lt</th>\n",
       "      <th>word_difference_ratio_lt</th>\n",
       "      <th>diff_lt</th>\n",
       "      <th>diff_ratio_lt</th>\n",
       "      <th>long_lt</th>\n",
       "      <th>short_lt</th>\n",
       "      <th>word_difference_ratio_lt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>diff_lt</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.930463</td>\n",
       "      <td>0.687793</td>\n",
       "      <td>0.078501</td>\n",
       "      <td>0.230256</td>\n",
       "      <td>0.787167</td>\n",
       "      <td>-0.725019</td>\n",
       "      <td>0.631898</td>\n",
       "      <td>0.128942</td>\n",
       "      <td>0.181086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diff_ratio_lt</th>\n",
       "      <td>-0.930463</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.465584</td>\n",
       "      <td>0.234474</td>\n",
       "      <td>-0.227166</td>\n",
       "      <td>-0.745388</td>\n",
       "      <td>0.800121</td>\n",
       "      <td>-0.423377</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>-0.174538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_lt</th>\n",
       "      <td>0.687793</td>\n",
       "      <td>-0.465584</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.743425</td>\n",
       "      <td>0.198276</td>\n",
       "      <td>0.641732</td>\n",
       "      <td>-0.384170</td>\n",
       "      <td>0.947680</td>\n",
       "      <td>0.719269</td>\n",
       "      <td>0.162716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_lt</th>\n",
       "      <td>0.078501</td>\n",
       "      <td>0.234474</td>\n",
       "      <td>0.743425</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041134</td>\n",
       "      <td>0.142547</td>\n",
       "      <td>0.186510</td>\n",
       "      <td>0.713867</td>\n",
       "      <td>0.923279</td>\n",
       "      <td>0.043541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_difference_ratio_lt</th>\n",
       "      <td>0.230256</td>\n",
       "      <td>-0.227166</td>\n",
       "      <td>0.198276</td>\n",
       "      <td>0.041134</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.220128</td>\n",
       "      <td>-0.201045</td>\n",
       "      <td>0.205721</td>\n",
       "      <td>0.063945</td>\n",
       "      <td>0.696595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diff_lt</th>\n",
       "      <td>0.787167</td>\n",
       "      <td>-0.745388</td>\n",
       "      <td>0.641732</td>\n",
       "      <td>0.142547</td>\n",
       "      <td>0.220128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.921698</td>\n",
       "      <td>0.711545</td>\n",
       "      <td>0.087698</td>\n",
       "      <td>0.209369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diff_ratio_lt</th>\n",
       "      <td>-0.725019</td>\n",
       "      <td>0.800121</td>\n",
       "      <td>-0.384170</td>\n",
       "      <td>0.186510</td>\n",
       "      <td>-0.201045</td>\n",
       "      <td>-0.921698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.447671</td>\n",
       "      <td>0.266287</td>\n",
       "      <td>-0.205842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_lt</th>\n",
       "      <td>0.631898</td>\n",
       "      <td>-0.423377</td>\n",
       "      <td>0.947680</td>\n",
       "      <td>0.713867</td>\n",
       "      <td>0.205721</td>\n",
       "      <td>0.711545</td>\n",
       "      <td>-0.447671</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.731866</td>\n",
       "      <td>0.169752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_lt</th>\n",
       "      <td>0.128942</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>0.719269</td>\n",
       "      <td>0.923279</td>\n",
       "      <td>0.063945</td>\n",
       "      <td>0.087698</td>\n",
       "      <td>0.266287</td>\n",
       "      <td>0.731866</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_difference_ratio_lt</th>\n",
       "      <td>0.181086</td>\n",
       "      <td>-0.174538</td>\n",
       "      <td>0.162716</td>\n",
       "      <td>0.043541</td>\n",
       "      <td>0.696595</td>\n",
       "      <td>0.209369</td>\n",
       "      <td>-0.205842</td>\n",
       "      <td>0.169752</td>\n",
       "      <td>0.021379</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           diff_lt  diff_ratio_lt   long_lt  short_lt  word_difference_ratio_lt   diff_lt  diff_ratio_lt   long_lt  short_lt  word_difference_ratio_lt\n",
       "diff_lt                   1.000000      -0.930463  0.687793  0.078501                  0.230256  0.787167      -0.725019  0.631898  0.128942                  0.181086\n",
       "diff_ratio_lt            -0.930463       1.000000 -0.465584  0.234474                 -0.227166 -0.745388       0.800121 -0.423377  0.164557                 -0.174538\n",
       "long_lt                   0.687793      -0.465584  1.000000  0.743425                  0.198276  0.641732      -0.384170  0.947680  0.719269                  0.162716\n",
       "short_lt                  0.078501       0.234474  0.743425  1.000000                  0.041134  0.142547       0.186510  0.713867  0.923279                  0.043541\n",
       "word_difference_ratio_lt  0.230256      -0.227166  0.198276  0.041134                  1.000000  0.220128      -0.201045  0.205721  0.063945                  0.696595\n",
       "diff_lt                   0.787167      -0.745388  0.641732  0.142547                  0.220128  1.000000      -0.921698  0.711545  0.087698                  0.209369\n",
       "diff_ratio_lt            -0.725019       0.800121 -0.384170  0.186510                 -0.201045 -0.921698       1.000000 -0.447671  0.266287                 -0.205842\n",
       "long_lt                   0.631898      -0.423377  0.947680  0.713867                  0.205721  0.711545      -0.447671  1.000000  0.731866                  0.169752\n",
       "short_lt                  0.128942       0.164557  0.719269  0.923279                  0.063945  0.087698       0.266287  0.731866  1.000000                  0.021379\n",
       "word_difference_ratio_lt  0.181086      -0.174538  0.162716  0.043541                  0.696595  0.209369      -0.205842  0.169752  0.021379                  1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feas(save_data_char)\n",
    "test_feas(save_data_word)\n",
    "combine_feas = pd.concat([save_data_char, save_data_word], axis=1)\n",
    "test_feas(combine_feas)\n",
    "combine_feas.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simsum_feas = [save_data_char, save_data_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Distances\n",
    "\n",
    "Create TF-IDF vectors from question texts and compute vector distances between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.textsklearn  import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "\n",
    "# vectorizer = TfidfVectorizer(\n",
    "#     encoding='utf-8',\n",
    "#     analyzer='word',\n",
    "#     strip_accents='unicode',\n",
    "#     ngram_range=(1, 1),\n",
    "#     lowercase=True,\n",
    "#     norm='l2',\n",
    "#     use_idf=True,\n",
    "#     smooth_idf=True,\n",
    "#     sublinear_tf=True,\n",
    "# )\n",
    "\n",
    "def load_tfidf_dict(file_path):\n",
    "    source_obj = open(file_path, 'r')\n",
    "    tfidf_dt = {}\n",
    "    for line in source_obj:\n",
    "        word, tfidf_val = line.strip().split('&|&')\n",
    "        word = word.decode('UTF-8')\n",
    "        tfidf_dt[word] = float(tfidf_val)\n",
    "    return tfidf_dt\n",
    "\n",
    "def load_word_vec(file_path):\n",
    "    source_obj = open(file_path, 'r')\n",
    "    word_vec_dt = {}\n",
    "    for i, line in enumerate(source_obj):\n",
    "        if i == 0 and 'word2vec' in file_path:\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        line_lt = line.split()\n",
    "        word = line_lt[0].decode('UTF-8')\n",
    "        vec_list = [float(x) for x in line_lt[1:]]\n",
    "        word_vec_dt[word] = vec_list\n",
    "    return word_vec_dt\n",
    "\n",
    "def get_sentence_vector(word_vec_dict, input_string_x1, tfidf_dict):\n",
    "    vec_sentence = 0.0\n",
    "    len_vec = len(word_vec_dict['花呗'.decode('UTF-8')])\n",
    "    for word in input_string_x1:\n",
    "        word_vec = word_vec_dict.get(word)\n",
    "        word_tfidf = tfidf_dict.get(word)\n",
    "        if word_vec is None or word_tfidf is None:\n",
    "            continue\n",
    "        vec_sentence += np.multiply(word_vec, word_tfidf)\n",
    "    vec_sentence = vec_sentence / (np.sqrt(np.sum(np.power(vec_sentence, 2))))\n",
    "    return vec_sentence\n",
    "\n",
    "def cos_distance_bag_tfidf(input_string_x1, input_string_x2, word_vec_dict, tfidf_dict):\n",
    "    sentence_vec1 = get_sentence_vector(word_vec_dict, input_string_x1, tfidf_dict)\n",
    "    sentence_vec2 = get_sentence_vector(word_vec_dict, input_string_x2, tfidf_dict)\n",
    "    numerator = np.sum(np.multiply(sentence_vec1, sentence_vec2))\n",
    "    denominator = np.sqrt(np.sum(np.power(sentence_vec1, 2))) * np.sqrt(np.sum(np.power(sentence_vec2, 2)))\n",
    "    cos_distance = float(numerator)/float(denominator)\n",
    "    manhat_distance = np.sum(np.abs(np.subtract(sentence_vec1, sentence_vec2)))\n",
    "    if np.isnan(manhat_distance):manhat_distance = 300\n",
    "    manhat_distance = np.log(manhat_distance + 0.000001)/5.0\n",
    "    \n",
    "    canberra_distance = np.sum(np.abs(sentence_vec1 - sentence_vec2) / np.abs(sentence_vec2 + sentence_vec1))\n",
    "    if np.isnan(canberra_distance):canberra_distance = 300\n",
    "    canberra_distance = np.log(canberra_distance + 0.000001)/5.0\n",
    "    \n",
    "    minkow_distance = np.power(np.sum(np.power(np.abs(sentence_vec1-sentence_vec2), 3)), 0.333333)\n",
    "    if np.isnan(minkow_distance):minkow_distance = 300\n",
    "    minkow_distance = np.log(minkow_distance + 0.000001)/5.0\n",
    "    \n",
    "    euclidean_distance = np.sqrt(np.sum(np.power(sentence_vec1-sentence_vec2, 2)))\n",
    "    if np.isnan(euclidean_distance):euclidean_distance = 300\n",
    "    euclidean_distance = np.log(euclidean_distance + 0.000001)/5.0\n",
    "    return cos_distance, manhat_distance, minkow_distance, euclidean_distance\n",
    "\n",
    "tfidf_dict = load_tfidf_dict('../data/aux/sim_tfidf.txt')\n",
    "word_vec_dict = load_word_vec('../data/aux/word2vec.txt')\n",
    "fasttext_vect_dict = load_word_vec('../data/aux/fasttext.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ori_data.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.add_word('花呗')\n",
    "jieba.add_word('借呗')\n",
    "jieba.add_word('收钱码')\n",
    "jieba.add_word('收款码')\n",
    "def cal_tfidf(csv_data):\n",
    "    cos_distance_lt = []\n",
    "    manhat_distance_lt = []\n",
    "    minkow_distance_lt = []\n",
    "    euclidean_distance_lt = []\n",
    "    word_difference_ratio_lt = []\n",
    "    for i in range(csv_data.shape[0]):\n",
    "        id, ques1, ques2, label = list(csv_data.iloc[i,:])\n",
    "        ques1 = ques1.decode('UTF-8')\n",
    "        ques2 = ques2.decode('UTF-8')\n",
    "        ques_lt1 = jieba.lcut(ques1)\n",
    "        ques_lt2 = jieba.lcut(ques2)\n",
    "        cos_distance, manhat_distance, minkow_distance, euclidean_distance = cos_distance_bag_tfidf(\n",
    "            ques_lt1, ques_lt2, word_vec_dict, tfidf_dict)\n",
    "        cos_distance_lt.append(cos_distance)\n",
    "        manhat_distance_lt.append(manhat_distance)\n",
    "        minkow_distance_lt.append(minkow_distance)\n",
    "        euclidean_distance_lt.append(euclidean_distance)\n",
    "    return cos_distance_lt, manhat_distance_lt, minkow_distance_lt, euclidean_distance_lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_distance_lt, manhat_distance_lt, minkow_distance_lt, euclidean_distance_lt = cal_tfidf(train_ori_data)\n",
    "save_data_word = pd.DataFrame({'cosine_distances':cos_distance_lt, 'manhat_distances':manhat_distance_lt,\n",
    "                         'minkow_distances':minkow_distance_lt, 'euclidean_distances':euclidean_distance_lt})\n",
    "save_data.to_csv('../tfidf_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feas(save_data_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feas = pd.concat([ngram_feas[0],ngram_feas[1], lda_feas[0],lda_feas[1], simsum_feas[0], simsum_feas[1], save_data_word], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  0.41826050537484294\n"
     ]
    }
   ],
   "source": [
    "y = train_ori_data.iloc[:, 3]\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def test_feas_all(X):\n",
    "    Xt, Xv, yt, yv = train_test_split(X, y)\n",
    "    clf = RandomForestClassifier(max_features= 'sqrt' ,n_estimators=100, oob_score = True, class_weight={0: 1.,1: 5,})\n",
    "    clf.fit(Xt, yt)\n",
    "    f1 = f1_score(yv, clf.predict(Xv))\n",
    "    print 'weights: ', f1\n",
    "test_feas(all_feas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenchi/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/chenchi/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight={0: 1.0, 1: 5},\n",
       "            criterion='gini', max_depth=None, max_features='sqrt',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=50, n_jobs=-1, oob_score=True, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [200, 700], 'max_features': ['auto', 'sqrt', 'log2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True, class_weight={0: 1.,1: 5,}) \n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 700],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(all_feas, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'sqrt', 'n_estimators': 700}\n"
     ]
    }
   ],
   "source": [
    "print CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
